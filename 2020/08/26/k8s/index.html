<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Kubernetes集群搭建 | huisebug</title><meta name="keywords" content="k8s,hpav2,hpa"><meta name="author" content="huisebug"><meta name="copyright" content="huisebug"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="这是一个k8s集群，从零开始搭建服务器环境，keepalived+haproxy实现集群高可用VIP；glusterfs搭建实现可持续存储；k8s1.12版本集群；efk日志系统；prometheus-operator告警系统；HPA v2横向pod扩容；k8s1.11+以上的版本部署基本没什么区别，无非就是优化了一些参数的配置和增加一些功能，舍弃一些api，此文档适用后续发布的其他版本的部署，提">
<meta property="og:type" content="article">
<meta property="og:title" content="Kubernetes集群搭建">
<meta property="og:url" content="https://huisebug.github.io/2020/08/26/k8s/index.html">
<meta property="og:site_name" content="huisebug">
<meta property="og:description" content="这是一个k8s集群，从零开始搭建服务器环境，keepalived+haproxy实现集群高可用VIP；glusterfs搭建实现可持续存储；k8s1.12版本集群；efk日志系统；prometheus-operator告警系统；HPA v2横向pod扩容；k8s1.11+以上的版本部署基本没什么区别，无非就是优化了一些参数的配置和增加一些功能，舍弃一些api，此文档适用后续发布的其他版本的部署，提">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://huisebug.github.io/img/head.jpg">
<meta property="article:published_time" content="2020-08-26T05:04:01.000Z">
<meta property="article:modified_time" content="2023-04-20T05:40:06.665Z">
<meta property="article:author" content="huisebug">
<meta property="article:tag" content="k8s">
<meta property="article:tag" content="hpav2">
<meta property="article:tag" content="hpa">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://huisebug.github.io/img/head.jpg"><link rel="shortcut icon" href="/img/linuxqie.jpg"><link rel="canonical" href="https://huisebug.github.io/2020/08/26/k8s/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: huisebug","link":"链接: ","source":"来源: huisebug","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Kubernetes集群搭建',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-04-20 13:40:06'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="huisebug" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/linuxqie.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">65</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/download/"><i class="fa-fw fas fa-folder-open"></i><span> Download</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/head.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">huisebug</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/download/"><i class="fa-fw fas fa-folder-open"></i><span> Download</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Kubernetes集群搭建</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-08-26T05:04:01.000Z" title="发表于 2020-08-26 13:04:01">2020-08-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-04-20T05:40:06.665Z" title="更新于 2023-04-20 13:40:06">2023-04-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Kubernetes集群搭建"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>这是一个k8s集群，从零开始搭建服务器环境，keepalived+haproxy实现集群高可用VIP；glusterfs搭建实现可持续存储；k8s1.12版本集群；efk日志系统；prometheus-operator告警系统；HPA v2横向pod扩容；k8s1.11+以上的版本部署基本没什么区别，无非就是优化了一些参数的配置和增加一些功能，舍弃一些api，此文档适用后续发布的其他版本的部署，提供一些部署思路。</p>
<span id="more"></span>



<h1 id="硬件环境准备"><a href="#硬件环境准备" class="headerlink" title="硬件环境准备"></a>硬件环境准备</h1><ul>
<li>  服务器三台centos7.6</li>
<li>  每台服务器两块硬盘，一块作为服务器硬盘，一块作为glusterFS硬盘</li>
<li>  关闭firewalld防火墙和selinux</li>
<li>  IP地址：192.168.137.10—12/24；主机名api、node1、node2.huisebug.com</li>
<li>  使用阿里云yum源</li>
<li>  下面的操作是之前使用k8s1.12.4搭建的，其中的参数同样适用于k8s.12.*,因为hpa的原因替换为了k8s1.11.8。</li>
<li>  k8s版本1.11.8（2019年3月1号发布）；</li>
<li>  hpa在如下版本会会存在hpa无法获取内存的使用情况的bug，所以不推荐使用这些版本，其他版本未测试。如果你在使用了1.11.*版本后切换到了如下版本，是因为数据在etcd还没更新，一段时间后就会提示获取不到<blockquote>
<p>  k8s1.13.3<br>  k8s1.12.4<br>  k8s1.13.4<br>  k8s1.12.5<br>  k8s1.12.6</p>
</blockquote>
</li>
<li>  总结出k8s1.12+版本的HPA是无法获取内存使用情况，进一步跟进官方，k8s1.11.*版本是不支持autoscaling/v2beta2.</li>
</ul>
<h1 id="进行系统配置并开启IPVS"><a href="#进行系统配置并开启IPVS" class="headerlink" title="进行系统配置并开启IPVS"></a>进行系统配置并开启IPVS</h1><ul>
<li>  关闭防火墙、selinux</li>
<li>  关闭系统的Swap，Kubernetes 1.8开始要求。</li>
<li>  关闭linux swap空间的swappiness</li>
<li>  配置L2网桥在转发包时会被iptables的FORWARD规则所过滤，该配置被CNI插件需要，更多信息请参考<a target="_blank" rel="noopener" href="https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/#network-plugin-requirements">NetworkPluginRequirements</a></li>
<li>  开启IPVS，将会后续应用到kube-proxy中去</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 所有主机：基本系统配置</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 关闭Selinux/firewalld</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config</span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 关闭交换分区</span></span><br><span class="line">swapoff -a</span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 增加个谷歌dns，方便访问外网</span></span><br><span class="line">echo &quot;nameserver 8.8.8.8&quot; &gt;&gt; /etc/resolv.conf</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置网桥包经IPTables，core文件生成路径</span></span><br><span class="line">echo &quot;&quot;&quot;</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">vm.overcommit_memory=1</span><br><span class="line">vm.panic_on_oom=0</span><br><span class="line">&quot;&quot;&quot; &gt; /etc/sysctl.conf</span><br><span class="line">modprobe br_netfilter</span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure>
<p>从Linux内核3.18-rc1开始，你必须使用modprobe br_netfilter来启用bridge-netfilter。，不然会提示以下错误<br><img src="/2020/08/26/k8s/media/10033b726854f2231957b8048571e073.png"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 所有主机：基本系统配置</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 同步时间</span></span><br><span class="line">yum install -y ntpdate</span><br><span class="line">ntpdate -u ntp.api.bz</span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装内核组件</span></span><br><span class="line">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm ;yum --enablerepo=elrepo-kernel install kernel-lt-devel kernel-lt -y</span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查默认内核版本高于4.1，否则请调整默认启动参数</span></span><br><span class="line">grub2-editenv list</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 确认内核版本</span></span><br><span class="line">uname -a</span><br><span class="line">注解：一般centos7的内核是3.10.当你升级后重启服务器，显示的内核还是之前的，因为你没切换到新内核，需要在主机启动页面选择，如果是云服务器，是看不到选择界面的，这时候就需要将新安装的内核设定为操作系统的默认内核，或者说如何将新版本的内核设置为重启后的默认内核？ </span><br><span class="line">仅需两步，之后重启即可。</span><br><span class="line">grub2-set-default 0</span><br><span class="line">grub2-mkconfig -o /etc/grub2.cfg</span><br><span class="line">reboot</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 重启后再次执行以下命令，确认内核版本</span></span><br><span class="line">uname –a</span><br><span class="line"><span class="meta">#</span><span class="bash"> 确认内核高于4.1后，开启IPVS</span></span><br><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">ipvs_modules=&quot;ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4&quot;</span><br><span class="line">for kernel_module in \$&#123;ipvs_modules&#125;; do</span><br><span class="line"> /sbin/modinfo -F filename \$&#123;kernel_module&#125; &gt; /dev/null 2&gt;&amp;1</span><br><span class="line"> if [ \$? -eq 0 ]; then</span><br><span class="line"> /sbin/modprobe \$&#123;kernel_module&#125;</span><br><span class="line"> fi</span><br><span class="line">done</span><br><span class="line">EOF</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 授权并执行</span></span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs</span><br></pre></td></tr></table></figure>

<p>Kubernetes要求集群中所有机器具有不同的Mac地址、产品uuid、Hostname。可以使用如下命令查看Mac和uuid</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 所有主机：检查UUID和Mac</span></span><br><span class="line">cat /sys/class/dmi/id/product_uuid</span><br><span class="line">ip link</span><br></pre></td></tr></table></figure>

<h1 id="GlusterFS"><a href="#GlusterFS" class="headerlink" title="GlusterFS"></a>GlusterFS</h1><p>搭建glusterfs来作为可持续存储k8s的CSI（可以跳过不部署）</p>
<h2 id="安装glusterfs"><a href="#安装glusterfs" class="headerlink" title="安装glusterfs"></a>安装glusterfs</h2><p><font color="red" size="3">三台服务器都要执行</font></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 先安装 gluster 源</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> yum install centos-release-gluster -y</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装 glusterfs 组件（这里包含了server和client）</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建 glusterfs服务运行目录</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir /opt/glusterd</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改 glusterd 目录，将/var/lib改成/opt</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sed -i &amp;<span class="comment">#39;s/var/lib/opt/g&amp;#39; /etc/glusterfs/glusterd.vol</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动 glusterfs（为挂载提供服务）</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl start glusterd.service</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置开机启动</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl <span class="built_in">enable</span> glusterd.service</span></span><br><span class="line"><span class="meta">#</span><span class="bash">查看状态</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl status glusterd.service</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="配置-glusterfs"><a href="#配置-glusterfs" class="headerlink" title="配置 glusterfs"></a>配置 glusterfs</h2><p><font color="red" size="3">三台服务器都要执行</font></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置本地解析文件hosts</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vi /etc/hosts</span></span><br><span class="line">192.168.137.10 api.huisebug.com</span><br><span class="line">192.168.137.11 node1.huisebug.com</span><br><span class="line">192.168.137.12 node2.huisebug.com</span><br><span class="line"><span class="meta">#</span><span class="bash"> 开放端口（24007是gluster服务运行所需的端口号）如果关闭了防火墙就省略此步操作。</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> iptables -I INPUT -p tcp --dport 24007 -j ACCEPT</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建存储目录</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir /opt/gfs_data</span></span><br><span class="line"></span><br><span class="line">为了方便管理可使用一块新硬盘新建一个分区将其挂载到 /opt/gfs_data</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看硬盘</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> fdisk -l /dev/sdb</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 建立分区并格式化为xfs类型</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> fdisk /dev/sdb</span></span><br><span class="line">1.  n 建立新分区</span><br><span class="line">2.  p 建立主分区</span><br><span class="line">3.  1 分区号为1</span><br><span class="line">4.  一直回车默认，将整块磁盘建立为一个分区，使用全部空间</span><br><span class="line">5.  w 保存退出</span><br><span class="line"><span class="meta">#</span><span class="bash"> 使分区生效</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> partprobe /dev/sdb</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 强制格式化为xfs</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkfs.xfs -f /dev/sdb1</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 手动将/dev/sdb1分区挂载到/opt/gfs_data</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mount /dev/sdb1 /opt/gfs_data</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 开机自动挂载</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&quot;/dev/sdb1 /opt/gfs_data xfs defaults 0 0&quot;</span> &gt;&gt; /etc/fstab</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看挂载状态</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> df –hT</span></span><br></pre></td></tr></table></figure>

<h2 id="添加节点到集群"><a href="#添加节点到集群" class="headerlink" title="添加节点到集群"></a>添加节点到集群</h2><p>三台服务器都安装好服务并成功启动后</p>
<p>执行probe操作即将三台服务器的gluster服务建立集群，选择其中任意一台执行probe操作，将会建立一个集群，执行完成后在任意一台执行以下命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gluster peer status</span><br></pre></td></tr></table></figure>
<p>将会看到其他两台的信息<br>执⾏操作的本机不需要 probe本机，并且只需执行一次,此处我在api.huisebug.com上操作。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gluster peer probe node1.huisebug.com</span><br><span class="line">gluster peer probe node2.huisebug.com</span><br></pre></td></tr></table></figure>
<p>查看状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gluster peer status </span><br></pre></td></tr></table></figure>

<h2 id="volume-类型"><a href="#volume-类型" class="headerlink" title="volume 类型"></a>volume 类型</h2><p>GlusterFS中的volume的模式有很多中，包括以下⼏种：</p>
<ul>
<li>  分布卷（默认模式）：即DHT, 也叫 分布卷: 将⽂件已hash算法随机分布到⼀台服务器节点中存储。</li>
<li>  复制模式：即AFR, 创建volume 时带 replica x 数量: 将⽂件复制到replica x个节点中。</li>
<li>  条带模式：即Striped, 创建volume 时带 stripe x 数量：将⽂件切割成数据块，分别存储到 stripe x 个节点中 ( 类似raid 0 )。</li>
<li>  分布式条带模式：最少需要4台服务器才能创建。 创建volume 时stripe 2 server = 4个节点： 是DHT 与 Striped 的组合型。</li>
<li>  分布式复制模式：最少需要4台服务器才能创建。 创建volume 时replica 2 server =4 个节点：是DHT 与 AFR 的组合型。</li>
<li>  条带复制卷模式：最少需要4台服务器才能创建。 创建volume 时stripe 2 replica 2 server = 4 个节点： 是 Striped 与 AFR 的组合型。</li>
<li>  三种模式混合： ⾄少需要8台 服务器才能创建。 stripe 2 replica 2 ,每4个节点组成⼀个组。<br>可参考以下链接<br><a target="_blank" rel="noopener" href="http://www.cnblogs.com/jicki/p/5801712.html">http://www.cnblogs.com/jicki/p/5801712.html</a></li>
</ul>
<p><font color="red" size="5">为了项目要求，我这里建立复制模式的volume</font></p>
<h2 id="建立复制卷"><a href="#建立复制卷" class="headerlink" title="建立复制卷"></a>建立复制卷</h2><p>现在整个gluster集群是一个整体，任意节点执行都可以</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 建立复制卷，卷名为test-volume</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume create test-volume replica 3 \</span></span><br><span class="line"><span class="bash">api.huisebug.com:/opt/gfs_data \</span></span><br><span class="line"><span class="bash">node1.huisebug.com:/opt/gfs_data \</span></span><br><span class="line"><span class="bash">node2.huisebug.com:/opt/gfs_data \</span></span><br><span class="line"><span class="bash">force</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 开启复制卷</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume start test-volume</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看volume状态</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume info</span></span><br><span class="line">Volume Name: test-volume &lt;br&gt;Type: Replicate &lt;br&gt;Volume ID: b52e2b36-0a88-43c5-b596-5ec0e01ca529 &lt;br&gt;Status: Started &lt;br&gt;Snapshot Count: 0 &lt;br&gt;Number of Bricks: 1 x 3 = 3&lt;br&gt; Transport-type: tcp &lt;br&gt;Bricks: &lt;br&gt;Brick1: api.huisebug.com:/opt/gfs_data &lt;br&gt;Brick2: node1.huisebug.com:/opt/gfs_data &lt;br&gt;Brick3: node2.huisebug.com:/opt/gfs_data &lt;br&gt;Options Reconfigured: &lt;br&gt;transport.address-family: inet &lt;br&gt;nfs.disable: on &lt;br&gt;performance.client-io-threads: off </span><br></pre></td></tr></table></figure>


<h2 id="配额限制"><a href="#配额限制" class="headerlink" title="配额限制"></a>配额限制</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开启 指定 volume 的配额</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume quota test-volume <span class="built_in">enable</span></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 限制 指定 volume 的配额，我们这里的硬盘是50G，所以分配给他10G</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume quota test-volume limit-usage / 10GB</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 cache ⼤⼩, 默认32MB,这个千万别设置太大了，不然会导致挂载不上</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume <span class="built_in">set</span> test-volume performance.cache-size 160MB</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 io 线程, 太⼤会导致进程崩溃</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume <span class="built_in">set</span> test-volume performance.io-thread-count 16</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 ⽹络检测时间, 默认42s</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume <span class="built_in">set</span> test-volume network.ping-timeout 10</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 写缓冲区的⼤⼩, 默认1M</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume <span class="built_in">set</span> test-volume performance.write-behind-window-size 1024MB</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 回写 (写数据时间，先写入缓存内，再写入硬盘)</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume <span class="built_in">set</span> test-volume performance.write-behind on</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置好了voleme的配额后要重新启动卷</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume stop test-volume</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume start test-volume</span></span><br></pre></td></tr></table></figure>

<h2 id="挂载使用"><a href="#挂载使用" class="headerlink" title="挂载使用"></a>挂载使用</h2><p><font color="red" size="3">三台服务器都要执行</font></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 建立使用服务器的目录</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p /opt/gfs_datause</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 挂载关联</span></span><br><span class="line">mount –t 指定挂载的卷类型 主机地址:卷名 挂载到本地的哪个目录（主机地址输入哪台服务器的地址都可以）</span><br><span class="line"><span class="meta">$</span><span class="bash"> mount -t glusterfs api.huisebug.com:test-volume /opt/gfs_datause</span></span><br><span class="line">如果挂载失败，查看日志文件/var/log/glusterfs/opt-gfs_datause.log解决问题所在，我遇到是我把gluster volume set test-volume performance.cache-size 160MB设置为了4GB，导致无法挂载，然后我修改为了160MB，就可以了</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">查看挂载情况，如果要持续使用记得设置自动挂载</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> df -hT</span></span><br><span class="line">文件系统 类型 容量 已用 可用 已用% 挂载点 </span><br><span class="line">api.huisebug.com:test-volume fuse.glusterfs 10G 0 10G 0% /opt/gfs_datause </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">自动挂载</span></span><br><span class="line">echo &quot;api.huisebug.com:test-volume /opt/gfs_datause glusterfs defaults 0 0&quot; &gt;&gt; /etc/fstab</span><br></pre></td></tr></table></figure>

<p><font color="red" size="3">记得执行node1和node2上述操作</font></p>
<h2 id="测试同步效果"><a href="#测试同步效果" class="headerlink" title="测试同步效果"></a>测试同步效果</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@api gfs_datause]# touch &#123;1..10&#125; </span><br><span class="line">[root@api gfs_datause]# ls </span><br><span class="line">1 10 2 3 4 5 6 7 8 9 </span><br><span class="line"></span><br><span class="line">[root@node1 opt]# ls gfs_datause/</span><br><span class="line">1 10 2 3 4 5 6 7 8 9 </span><br><span class="line"></span><br><span class="line">[root@node2 gfs_datause]# ls</span><br><span class="line">1 10 2 3 4 5 6 7 8 9 </span><br></pre></td></tr></table></figure>

<p>GlusterFS搭建完毕！！！</p>
<h1 id="Keepalived"><a href="#Keepalived" class="headerlink" title="Keepalived"></a>Keepalived</h1><p>整个集群的高可用VIP</p>
<p><font color="red" size="3">三台服务器都要执行</font></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 安装keepalived和ipvs、socat</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> yum install -y socat keepalived ipvsadm</span></span><br></pre></td></tr></table></figure>

<h2 id="启动Keepalived服务"><a href="#启动Keepalived服务" class="headerlink" title="启动Keepalived服务"></a>启动Keepalived服务</h2><p>修改配置文件keepalived.conf为如下三台服务器的，这里我们暂时先让VIP运行起来，后续监听端口和进程后续再增加。</p>
<p>api</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim /etc/keepalived/keepalived.conf</span> </span><br><span class="line"></span><br><span class="line">! Configuration File for keepalived </span><br><span class="line">global_defs &#123;</span><br><span class="line"> router_id vip1 </span><br><span class="line">&#125; </span><br><span class="line">vrrp_instance VI_1 &#123; </span><br><span class="line"><span class="meta">#</span><span class="bash">三台配置此处均是BACKUP</span> </span><br><span class="line">state BACKUP </span><br><span class="line">interface ens33 </span><br><span class="line">virtual_router_id 55 </span><br><span class="line"><span class="meta">#</span><span class="bash">优先级</span> </span><br><span class="line">priority 95 </span><br><span class="line">advert_int 1 </span><br><span class="line">authentication &#123; </span><br><span class="line"> auth_type PASS </span><br><span class="line"> auth_pass 123456 </span><br><span class="line">&#125; </span><br><span class="line">virtual_ipaddress &#123;</span><br><span class="line"> 192.168.137.13 </span><br><span class="line">&#125; </span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>node1</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim /etc/keepalived/keepalived.conf</span> </span><br><span class="line"></span><br><span class="line">! Configuration File for keepalived </span><br><span class="line">global_defs &#123;</span><br><span class="line"> router_id vip2</span><br><span class="line"> &#125; </span><br><span class="line">vrrp_instance VI_1 &#123; </span><br><span class="line"><span class="meta">#</span><span class="bash">三台配置此处均是BACKUP</span> </span><br><span class="line">state BACKUP</span><br><span class="line">interface ens33 </span><br><span class="line">virtual_router_id 55 </span><br><span class="line"><span class="meta">#</span><span class="bash">优先级</span> </span><br><span class="line">priority 90 </span><br><span class="line">advert_int 1 </span><br><span class="line">authentication &#123;</span><br><span class="line"> auth_type PASS </span><br><span class="line"> auth_pass 123456 </span><br><span class="line">&#125; </span><br><span class="line">virtual_ipaddress &#123; </span><br><span class="line">192.168.137.13</span><br><span class="line">&#125;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>node2</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim /etc/keepalived/keepalived.conf</span> </span><br><span class="line"></span><br><span class="line">! Configuration File for keepalived </span><br><span class="line">global_defs &#123;</span><br><span class="line"> router_id vip3</span><br><span class="line"> &#125;</span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line"><span class="meta"> #</span><span class="bash">三台配置此处均是BACKUP</span> </span><br><span class="line">state BACKUP </span><br><span class="line">interface ens33 </span><br><span class="line">virtual_router_id 55 </span><br><span class="line"><span class="meta">#</span><span class="bash">优先级</span> </span><br><span class="line">priority 85 </span><br><span class="line">advert_int 1 </span><br><span class="line">authentication &#123;</span><br><span class="line"> auth_type PASS</span><br><span class="line"> auth_pass 123456 </span><br><span class="line">&#125; </span><br><span class="line">virtual_ipaddress &#123;</span><br><span class="line"> 192.168.137.13 </span><br><span class="line">&#125;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>

<p>重启服务，自启</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> systemctl restart keepalived</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl <span class="built_in">enable</span> keepalived</span></span><br></pre></td></tr></table></figure>
<p>在优先级最高的那台查看VIP地址是否建立</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip addr</span></span><br></pre></td></tr></table></figure>
<p>验证可以使用新建一个终端来验证是否会连接到优先级高的那台服务器</p>
<h1 id="证书"><a href="#证书" class="headerlink" title="证书"></a>证书</h1><p>Kubernetes系统的各组件需要使⽤TLS证书对通信进⾏加密，本⽂档使⽤CloudFlare的PKI⼯具集cfssl来⽣成Certificate Authority (CA)和其它证书；⽣成的CA证书和秘钥⽂件如下：</p>
<ul>
<li>  ca-key.pem</li>
<li>  ca.pem</li>
<li>  kubernetes-key.pem</li>
<li>  kubernetes.pem</li>
<li>  kube-proxy.pem</li>
<li>  kube-proxy-key.pem</li>
<li>  admin.pem</li>
<li>  admin-key.pem</li>
</ul>
<p>使⽤证书的组件如下：</p>
<ul>
<li>  etcd：使⽤ ca.pem、kubernetes-key.pem、kubernetes.pem；</li>
<li>  kube-apiserver：使⽤ ca.pem、kubernetes-key.pem、kubernetes.pem；</li>
<li>  kubelet：使⽤ ca.pem；</li>
<li>  kube-proxy：使⽤ ca.pem、kube-proxy-key.pem、kubeproxy.pem；</li>
<li>  kubectl：使⽤ ca.pem、admin-key.pem、admin.pem；</li>
<li>  kube-controller-manager：使⽤ ca-key.pem、ca.pem</li>
</ul>
<p>注意：以下操作都在 <strong>api</strong> 节点即 <strong>192.168.137.10</strong>这台主机上执⾏，证书只需要创建⼀次即可，以后在向集群中添加新节点时只要将**/etc/kubernetes/**⽬录下的证书拷⻉到新节点上即可。</p>
<h2 id="安装cfssl"><a href="#安装cfssl" class="headerlink" title="安装cfssl"></a>安装cfssl</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64</span><br><span class="line">chmod +x cfssl_linux-amd64</span><br><span class="line">mv cfssl_linux-amd64 /usr/local/bin/cfssl</span><br><span class="line">wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64</span><br><span class="line">chmod +x cfssljson_linux-amd64</span><br><span class="line">mv cfssljson_linux-amd64 /usr/local/bin/cfssljson</span><br><span class="line">wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64</span><br><span class="line">chmod +x cfssl-certinfo_linux-amd64</span><br><span class="line">mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo</span><br><span class="line">export PATH=/usr/local/bin:$PATH</span><br></pre></td></tr></table></figure>

<h2 id="创建ca配置文件"><a href="#创建ca配置文件" class="headerlink" title="创建ca配置文件"></a>创建ca配置文件</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir /root/ssl</span><br><span class="line">cd /root/ssl</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下面的操作是将模板文件复制过来，可以不操作，直接建立。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cfssl print-defaults config &gt; config.json</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cfssl print-defaults csr &gt; csr.json</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cat config.json &gt; ca-config.json</span></span><br></pre></td></tr></table></figure>

<p>根据config.json⽂件的格式创建如下的ca-config.json⽂件<br>过期时间设置成了 87600h(10年)<br>修改 ca-config.json内容为如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">vim ca-config.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;signing&quot;: &#123;</span><br><span class="line">        &quot;default&quot;: &#123;</span><br><span class="line">            &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;profiles&quot;: &#123;</span><br><span class="line">            &quot;kubernetes&quot;: &#123;</span><br><span class="line">                &quot;usages&quot;: [</span><br><span class="line">                    &quot;signing&quot;,</span><br><span class="line">                    &quot;key encipherment&quot;,</span><br><span class="line">                    &quot;server auth&quot;,</span><br><span class="line">                    &quot;client auth&quot;</span><br><span class="line">                ],</span><br><span class="line">                &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>字段说明:</p>
<ul>
<li>  ca-config.json ：可以定义多个profiles，分别指定不同的过期时间、使⽤场景等参数；后续在签名证书时使⽤某个profile；</li>
<li>  signing ：表示该证书可⽤于签名其它证书；⽣成的 ca.pem 证书中CA=TRUE ；</li>
<li>  server auth ：表示client可以⽤该 CA 对server提供的证书进⾏验证；</li>
<li>  client auth ：表示server可以⽤该CA对client提供的证书进⾏验证；</li>
</ul>
<h2 id="创建CA证书"><a href="#创建CA证书" class="headerlink" title="创建CA证书"></a>创建CA证书</h2><h3 id="创建-CA-证书签名请求"><a href="#创建-CA-证书签名请求" class="headerlink" title="创建 CA 证书签名请求"></a>创建 CA 证书签名请求</h3><p>创建 ca-csr.json ⽂件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">vim ca-csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>  “CN”： Common Name ，kube-apiserver 从证书中提取该字段作为请求的⽤户名 (User Name)；浏览器使⽤该字段验证⽹站是否合法；</li>
<li>  “O”： Organization ，kube-apiserver 从证书中提取该字段作为请求⽤户所属的组(Group)；</li>
</ul>
<h3 id="⽣成-CA-证书和私钥ca-key-pem-ca-pem"><a href="#⽣成-CA-证书和私钥ca-key-pem-ca-pem" class="headerlink" title="⽣成 CA 证书和私钥ca-key.pem ca.pem"></a>⽣成 CA 证书和私钥ca-key.pem ca.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert -initca ca-csr.json | cfssljson -bare ca</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls</span></span><br><span class="line">ca-config.json ca.csr ca-csr.json ca-key.pem ca.pem config.json csr.json</span><br></pre></td></tr></table></figure>

<h2 id="创建-kubernetes-证书"><a href="#创建-kubernetes-证书" class="headerlink" title="创建 kubernetes 证书"></a>创建 kubernetes 证书</h2><h3 id="创建-kubernetes-证书签名请求⽂件-kubernetes-csr-json"><a href="#创建-kubernetes-证书签名请求⽂件-kubernetes-csr-json" class="headerlink" title="创建 kubernetes 证书签名请求⽂件 kubernetes-csr.json"></a>创建 kubernetes 证书签名请求⽂件 kubernetes-csr.json</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">cat ca-csr.json &gt; kubernetes-csr.json</span><br><span class="line">vim kubernetes-csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">        &quot;127.0.0.1&quot;,</span><br><span class="line">        &quot;192.168.137.10&quot;,</span><br><span class="line">        &quot;192.168.137.11&quot;,</span><br><span class="line">        &quot;192.168.137.12&quot;,</span><br><span class="line">        &quot;192.168.137.13&quot;,</span><br><span class="line">        &quot;10.254.0.1&quot;,</span><br><span class="line">        &quot;kubernetes&quot;,</span><br><span class="line">        &quot;kubernetes.default&quot;,</span><br><span class="line">        &quot;kubernetes.default.svc&quot;,</span><br><span class="line">        &quot;kubernetes.default.svc.cluster&quot;,</span><br><span class="line">        &quot;kubernetes.default.svc.cluster.local&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>  如果 hosts 字段不为空则需要指定授权使⽤该证书的 <strong>IP</strong>或域名列表，由于该证书后续被 etcd 集群和 kubernetes master集群使⽤，所以上⾯分别指定了 etcd 集群、 kubernetes master集群的主机 IP 和<strong>kubernetes</strong> 服务的集群 <strong>IP</strong>（⼀般是 kube-apiserver 指定的service-cluster-ip-range ⽹段的第⼀个IP，如 10.254.0.1。</li>
<li>  hosts中的内容可以为空，即使按照上⾯的置，向集群中增加新节点后也不需要重新⽣成证书。</li>
</ul>
<h3 id="⽣成-kubernetes-证书和私钥kubernetes-key-pem-kubernetes-pem"><a href="#⽣成-kubernetes-证书和私钥kubernetes-key-pem-kubernetes-pem" class="headerlink" title="⽣成 kubernetes 证书和私钥kubernetes-key.pem kubernetes.pem"></a>⽣成 kubernetes 证书和私钥kubernetes-key.pem kubernetes.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls kubernetes*</span></span><br><span class="line">kubernetes.csr kubernetes-csr.json kubernetes-key.pem kubernetes.pem</span><br></pre></td></tr></table></figure>
<h2 id="创建admin证书"><a href="#创建admin证书" class="headerlink" title="创建admin证书"></a>创建admin证书</h2><h3 id="创建-admin-证书签名请求⽂件-admin-csr-json-："><a href="#创建-admin-证书签名请求⽂件-admin-csr-json-：" class="headerlink" title="创建 admin 证书签名请求⽂件 admin-csr.json ："></a>创建 admin 证书签名请求⽂件 admin-csr.json ：</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cat ca-csr.json &gt; admin-csr.json</span><br><span class="line">vim admin-csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;admin&quot;,</span><br><span class="line">    &quot;hosts&quot;:[],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;system:masters&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>  后续 kube-apiserver 使⽤ RBAC 对客户端(如 kubelet 、 kubeproxy、 Pod)请求进⾏授权；</li>
<li>  kube-apiserver 预定义了⼀些 RBAC 使⽤的 RoleBindings ，如cluster-admin 将Group system:masters 与 Role cluster-admin绑定，该 Role 授予了调⽤kube-apiserver 的所有 <strong>API</strong>的权限；</li>
<li>  OU 指定该证书的 Group 为 system:masters ， kubelet 使⽤该证书访问kube-apiserver 时 ，由于证书被 CA签名，所以认证通过，同时由于证书⽤户组为经过预授权的 system:masters，所以被授予访问所有 API 的权限；</li>
</ul>
<h3 id="⽣成-admin-证书和私钥admin-key-pem-admin-pem"><a href="#⽣成-admin-证书和私钥admin-key-pem-admin-pem" class="headerlink" title="⽣成 admin 证书和私钥admin-key.pem admin.pem"></a>⽣成 admin 证书和私钥admin-key.pem admin.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls admin*</span></span><br><span class="line">admin.csr admin-csr.json admin-key.pem admin.pem</span><br></pre></td></tr></table></figure>
<h2 id="创建-kube-controller-manager-证书"><a href="#创建-kube-controller-manager-证书" class="headerlink" title="创建 kube-controller-manager 证书"></a>创建 kube-controller-manager 证书</h2><h3 id="创建-kube-controller-manager-证书签名请求⽂件-kube-controller-manager-csr-json"><a href="#创建-kube-controller-manager-证书签名请求⽂件-kube-controller-manager-csr-json" class="headerlink" title="创建 kube-controller-manager 证书签名请求⽂件 kube-controller-manager-csr.json"></a>创建 kube-controller-manager 证书签名请求⽂件 kube-controller-manager-csr.json</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim kube-controller-manager-csr.json</span></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;system:kube-controller-manager&quot;,</span><br><span class="line">    &quot;hosts&quot;: [],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;system:kube-controller-manager&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="⽣成kube-controller-manager证书和私钥kube-controller-manager-key-pem-kube-controller-manager-pem"><a href="#⽣成kube-controller-manager证书和私钥kube-controller-manager-key-pem-kube-controller-manager-pem" class="headerlink" title="⽣成kube-controller-manager证书和私钥kube-controller-manager-key.pem kube-controller-manager.pem"></a>⽣成kube-controller-manager证书和私钥kube-controller-manager-key.pem kube-controller-manager.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls kube-controller-manager*</span></span><br><span class="line">kube-controller-manager.csr kube-controller-manager-csr.json kube-controller-manager-key.pem kube-controller-manager.pem</span><br></pre></td></tr></table></figure>
<h2 id="创建-kube-scheduler-证书"><a href="#创建-kube-scheduler-证书" class="headerlink" title="创建 kube-scheduler 证书"></a>创建 kube-scheduler 证书</h2><h3 id="创建kube-scheduler-证书签名请求⽂件-kube-scheduler-csr-json"><a href="#创建kube-scheduler-证书签名请求⽂件-kube-scheduler-csr-json" class="headerlink" title="创建kube-scheduler 证书签名请求⽂件 kube-scheduler-csr.json"></a>创建kube-scheduler 证书签名请求⽂件 kube-scheduler-csr.json</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim kube-scheduler-csr.json</span></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;:&quot;system:kube-scheduler&quot;,</span><br><span class="line">    &quot;hosts&quot;: [],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;system:kube-scheduler&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="⽣成kube-scheduler证书和私钥kube-scheduler-key-pem-kube-scheduler-pem"><a href="#⽣成kube-scheduler证书和私钥kube-scheduler-key-pem-kube-scheduler-pem" class="headerlink" title="⽣成kube-scheduler证书和私钥kube-scheduler-key.pem kube-scheduler.pem"></a>⽣成kube-scheduler证书和私钥kube-scheduler-key.pem kube-scheduler.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls kube-scheduler*</span></span><br><span class="line">kube-scheduler.csr kube-scheduler-csr.json kube-scheduler-key.pem kube-scheduler.pem</span><br></pre></td></tr></table></figure>
<h2 id="创建-front-proxy-证书"><a href="#创建-front-proxy-证书" class="headerlink" title="创建 front-proxy 证书"></a>创建 front-proxy 证书</h2><h3 id="创建-front-proxy-证书签名请求⽂件front-proxy-ca-csr-json"><a href="#创建-front-proxy-证书签名请求⽂件front-proxy-ca-csr-json" class="headerlink" title="创建 front-proxy 证书签名请求⽂件front-proxy-ca-csr.json"></a>创建 front-proxy 证书签名请求⽂件<strong>front-proxy-ca-csr.json</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim front-proxy-ca-csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="生成-front-proxy-ca证书和私钥front-proxy-ca-key-pem-front-proxy-ca-pem"><a href="#生成-front-proxy-ca证书和私钥front-proxy-ca-key-pem-front-proxy-ca-pem" class="headerlink" title="生成 front-proxy-ca证书和私钥front-proxy-ca-key.pem  front-proxy-ca.pem"></a>生成 front-proxy-ca证书和私钥front-proxy-ca-key.pem  front-proxy-ca.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-ca</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls front-proxy-ca*</span></span><br><span class="line">front-proxy-ca.csr front-proxy-ca-csr.json front-proxy-ca-key.pem front-proxy-ca.pem</span><br></pre></td></tr></table></figure>
<h3 id="创建-front-proxy-client-证书签名请求⽂件-front-proxy-client-csr-json"><a href="#创建-front-proxy-client-证书签名请求⽂件-front-proxy-client-csr-json" class="headerlink" title="创建 front-proxy-client 证书签名请求⽂件 front-proxy-client-csr.json"></a>创建 front-proxy-client 证书签名请求⽂件 front-proxy-client-csr.json</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim front-proxy-client-csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;front-proxy-client&quot;,</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="⽣成front-proxy-client证书和私钥front-proxy-client-key-pem-front-proxy-client-pem"><a href="#⽣成front-proxy-client证书和私钥front-proxy-client-key-pem-front-proxy-client-pem" class="headerlink" title="⽣成front-proxy-client证书和私钥front-proxy-client-key.pem  front-proxy-client.pem"></a>⽣成front-proxy-client证书和私钥front-proxy-client-key.pem  front-proxy-client.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert \</span></span><br><span class="line"><span class="bash">-ca=front-proxy-ca.pem \</span></span><br><span class="line"><span class="bash">-ca-key=front-proxy-ca-key.pem \</span></span><br><span class="line"><span class="bash">-config=ca-config.json \</span></span><br><span class="line"><span class="bash">-profile=kubernetes \</span></span><br><span class="line"><span class="bash">front-proxy-client-csr.json | cfssljson -bare front-proxy-client</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls front-proxy-client*</span></span><br><span class="line">front-proxy-client.csr front-proxy-client-csr.json front-proxy-client-key.pem front-proxy-client.pem</span><br></pre></td></tr></table></figure>
<h2 id="校验证书"><a href="#校验证书" class="headerlink" title="校验证书"></a>校验证书</h2><p>以 kubernetes 证书为例</p>
<h3 id="使⽤-opsnssl-命令"><a href="#使⽤-opsnssl-命令" class="headerlink" title="使⽤ opsnssl 命令"></a>使⽤ opsnssl 命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl x509 -noout -text -in kubernetes.pem</span><br></pre></td></tr></table></figure>
<ul>
<li>  确认 Issuer 字段的内容和 ca-csr.json ⼀致；</li>
<li>  确认 Subject 字段的内容和 kubernetes-csr.json ⼀致；</li>
<li>  确认 X509v3 Subject Alternative Name 字段的内容和 kubernetescsr.json ⼀致；</li>
<li>  确认 X509v3 Key Usage、Extended Key Usage 字段的内容和 caconfig.json 中kubernetes profile ⼀致；</li>
</ul>
<h3 id="使⽤-cfssl-certinfo-命令"><a href="#使⽤-cfssl-certinfo-命令" class="headerlink" title="使⽤ cfssl-certinfo 命令"></a>使⽤ cfssl-certinfo 命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cfssl-certinfo -cert kubernetes.pem</span><br></pre></td></tr></table></figure>
<h2 id="分发证书"><a href="#分发证书" class="headerlink" title="分发证书"></a>分发证书</h2><p><font color="red" size="3">三台服务器都要执行</font></p>
<p>将⽣成的证书和秘钥⽂件（后缀名为 .pem ）拷⻉到所有机器的/etc/kubernetes/ssl<br>⽬录下备⽤；具体复制方法自己操作</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/kubernetes/ssl</span><br><span class="line">cp -rf *.pem /etc/kubernetes/ssl</span><br></pre></td></tr></table></figure>

<h1 id="Haproxy"><a href="#Haproxy" class="headerlink" title="Haproxy"></a>Haproxy</h1><p>1.结合keepalived保证整个集群的HA（高可用）</p>
<p>2.使用 keepalived 和 haproxy 实现 kube-apiserver 高可用的步骤：</p>
<ul>
<li>  keepalived 提供 kube-apiserver 对外服务的 VIP；</li>
<li>  haproxy 监听 VIP，后端连接所有 kube-apiserver实例，提供健康检查和负载均衡功能；</li>
<li>  运行 keepalived 和 haproxy 的节点称为 LB 节点。由于 keepalived是一主多备运行模式，故至少两个 LB 节点。复用 master 节点的三台机器，haproxy监听的端口(9443) 需要与 kube-apiserver 的端口6443不同，避免冲突。</li>
<li>  keepalived 在运行过程中周期检查本机的 haproxy 进程状态，如果检测到 haproxy进程异常，则触发重新选主的过程，VIP 将飘移到新选出来的主节点，从而实现 VIP的高可用。</li>
<li>  所有组件（如kubectl、kube-apiserver、kube-controller-manager、kube-scheduler、kubelet等）都通过VIP和haproxy监听的9443 端口访问kube-apiserver服务。</li>
</ul>
<p><font color="red" size="3">三台服务器都要执行</font></p>
<h2 id="安装haproxy"><a href="#安装haproxy" class="headerlink" title="安装haproxy"></a>安装haproxy</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> yum -y install haproxy</span></span><br></pre></td></tr></table></figure>
<p>修改haproxy配置文件如下，这里将9443端口（这个端口将代理到各k8s集群api端口6443）和haproxy进程绑定在一起，并定义后端服务器。并且开启100端口监听haproxy进程状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">haproxy 配置文件：</span></span><br><span class="line"><span class="meta">cat&gt;</span><span class="bash">haproxy.cfg&lt;&lt;<span class="string">EOF</span></span></span><br><span class="line">global</span><br><span class="line">    log /dev/log    local0</span><br><span class="line">    log /dev/log    local1 notice</span><br><span class="line">    chroot /var/lib/haproxy</span><br><span class="line">    stats socket /run/haproxy/admin.sock mode 660 level admin</span><br><span class="line">    stats timeout 30s</span><br><span class="line">    user haproxy</span><br><span class="line">    group haproxy</span><br><span class="line">    daemon</span><br><span class="line">    nbproc 1</span><br><span class="line">    </span><br><span class="line">defaults</span><br><span class="line">    log     global</span><br><span class="line">    timeout connect 5000</span><br><span class="line">    timeout client  10m</span><br><span class="line">    timeout server  10m</span><br><span class="line"></span><br><span class="line">listen  admin_stats</span><br><span class="line">    bind 0.0.0.0:100</span><br><span class="line">    mode http</span><br><span class="line">    log 127.0.0.1 local0 err</span><br><span class="line">    stats refresh 30s </span><br><span class="line">    stats uri /status</span><br><span class="line">    stats realm welcome login\ Haproxy</span><br><span class="line">    stats auth admin:123456</span><br><span class="line">    stats hide-version</span><br><span class="line">    stats admin if TRUE</span><br><span class="line"></span><br><span class="line"> listen kube-master</span><br><span class="line">     bind 0.0.0.0:9443</span><br><span class="line">     mode tcp</span><br><span class="line">     option tcplog</span><br><span class="line">     balance source</span><br><span class="line">     server api 192.168.137.10:6443 check inter 2000 fall 2 rise 2 weight 1</span><br><span class="line">     server node1 192.168.137.11:6443 check inter 2000 fall 2 rise 2 weight 1</span><br><span class="line">     server node2 192.168.137.12:6443 check inter 2000 fall 2 rise 2 weight 1</span><br><span class="line">EOF</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>建立haproxy的工作目录并启动服务，开机自启，工作目录/run/haproxy重启服务器会丢失，所以将其加入到随系统启动而建立</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 赋予执行权限</span></span><br><span class="line">chmod +x /etc/rc.d/rc.local</span><br><span class="line">echo &quot;mkdir -p /run/haproxy&quot; &gt;&gt; /etc/rc.local </span><br><span class="line">mkdir -p /run/haproxy </span><br><span class="line">systemctl start haproxy </span><br><span class="line">systemctl enable haproxy</span><br></pre></td></tr></table></figure>

<p>查看是否监听9443端口。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">netstat -lntp | grep 9443</span><br><span class="line">tcp 0 0 0.0.0.0:9443 0.0.0.0:* LISTEN 10019/haproxy</span><br></pre></td></tr></table></figure>
<h2 id="增加keepalived配置，使其嗅探haproxy的状态"><a href="#增加keepalived配置，使其嗅探haproxy的状态" class="headerlink" title="增加keepalived配置，使其嗅探haproxy的状态"></a>增加keepalived配置，使其嗅探haproxy的状态</h2><p><font color="red" size="3">三台服务器都要执行</font></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vrrp_script check-haproxy &#123;</span><br><span class="line">    script &quot;killall -0 haproxy&quot;</span><br><span class="line">    interval 1</span><br><span class="line">    weight -20</span><br><span class="line">&#125;</span><br><span class="line">并且在VIP下面配置track_script以对应上面的脚本</span><br><span class="line">  virtual_ipaddress &#123;</span><br><span class="line">         192.168.137.13</span><br><span class="line">     &#125;</span><br><span class="line">   track_script &#123;</span><br><span class="line">     check-haproxy</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>使用 killall -0 haproxy 命令检查所在节点的 haproxy<br>进程是否正常。如果异常则将权重减少（-20）,从而触发重新选主过程；</p>
<p>例如api节点配置<br><img src="/2020/08/26/k8s/media/07aa73fd92ffd1f9e62dbe8c48c64de2.png"></p>
<p>重启服务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart keepalived</span><br></pre></td></tr></table></figure>
<p>验证HA效果，用户名和密码是之前开启的100端口中定义的admin:123456<br><img src="/2020/08/26/k8s/media/5f9ad336cd3c31746b5f7a08ac951d0a.png"><br><img src="/2020/08/26/k8s/media/bfbddf798678dc16cf6abd1a4f1514b0.png"></p>
<h1 id="kubectl"><a href="#kubectl" class="headerlink" title="kubectl"></a>kubectl</h1><p>kubectl作为k8s集群的客户端工具，首先需要安装来生成集群配置文件kubeconfig<br>kubernetes二进制下载地址，为了快速下载，我们可以在机器上下载后再上传到其他主机<br>下载地址：<br><a target="_blank" rel="noopener" href="https://dl.k8s.io/v1.12.4/kubernetes-server-linux-amd64.tar.gz">https://dl.k8s.io/v1.12.4/kubernetes-server-linux-amd64.tar.gz</a></p>
<p>因为我们要做HA集群，所以这里三台服务器都会作为api服务器，所以我们直接将所有二进制程序放到指定地方</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget https://dl.k8s.io/v1.12.4/kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">tar zxf kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">cd kubernetes</span><br><span class="line">cp -rf server/bin/&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet&#125; /usr/local/bin/</span><br></pre></td></tr></table></figure>

<h2 id="创建-kubectl所需kubeconfig文件"><a href="#创建-kubectl所需kubeconfig文件" class="headerlink" title="创建 kubectl所需kubeconfig文件"></a>创建 kubectl所需kubeconfig文件</h2><p>主要是用于kubectl命令和kubelet服务进行获取apiserver信息并且集群角色cluster-admin与自建用户admin进行绑定</p>
<p>kubeconfig.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export KUBE_APISERVER=&quot;https://192.168.137.13:9443&quot;</span><br><span class="line">kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/ssl/ca.pem --embed-certs=true --server=$&#123;KUBE_APISERVER&#125;</span><br><span class="line">kubectl config set-credentials admin --client-certificate=/etc/kubernetes/ssl/admin.pem --embed-certs=true --client-key=/etc/kubernetes/ssl/admin-key.pem</span><br><span class="line">kubectl config set-context kubernetes --cluster=kubernetes --user=admin</span><br><span class="line">kubectl config use-context kubernetes</span><br></pre></td></tr></table></figure>

<ul>
<li>  KUBEAPISERVER变量中定义的是VIP地址。</li>
<li>  admin.pem证书OU字段值为 system:masters,kube-apiserver预定义的RoleBinding cluster-admin将Group system:masters与 Role cluster-admin绑定，该 Role 授予了调⽤ kube-apiserver相关 API 的权限；</li>
<li>  ⽣成的 kubeconfig 被保存到 ~/.kube/config ⽂件;</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source kubeconfig.sh</span><br></pre></td></tr></table></figure>
<h1 id="k8s集群建立前预备配置文件"><a href="#k8s集群建立前预备配置文件" class="headerlink" title="k8s集群建立前预备配置文件"></a>k8s集群建立前预备配置文件</h1><h2 id="创建-TLS-Bootstrapping-token文件"><a href="#创建-TLS-Bootstrapping-token文件" class="headerlink" title="创建 TLS Bootstrapping token文件"></a>创建 TLS Bootstrapping token文件</h2><h3 id="Token-auth-file"><a href="#Token-auth-file" class="headerlink" title="Token auth file"></a>Token auth file</h3><p>Token可以是任意的包涵128 bit的字符串，可以使⽤安全的随机数发⽣器⽣成</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x |tr -d &#x27; &#x27;)</span><br></pre></td></tr></table></figure>
<p>建立token.csv</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; token.csv &lt;&lt; EOF</span><br><span class="line"><span class="meta">$</span><span class="bash">&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,<span class="string">&quot;system:kubelet-bootstrap&quot;</span></span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat token.csv</span><br><span class="line">b156d4d29fe7a73e554a145fc996e3c6,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;</span><br></pre></td></tr></table></figure>
<p>注意：在进⾏后续操作前请检查 <strong>token.csv</strong>⽂件，确认其中的**${BOOTSTRAP_TOKEN}** 环境变量已经被真实的值替换。</p>
<p><strong>BOOTSTRAP_TOKEN</strong> 将被写⼊到 kube-apiserver 使⽤的 token.csv ⽂件和 kubelet使⽤的 bootstrap.kubeconfig ⽂件，如果后续重新⽣成了BOOTSTRAP_TOKEN，则需要：</p>
<ol>
<li>更新 token.csv ⽂件，分发到所有机器 (master 和 node）的/etc/kubernetes/⽬录下，分发到node节点上⾮必需；</li>
<li>重新⽣成 bootstrap.kubeconfig ⽂件，分发到所有 node 机器的/etc/kubernetes/⽬录下；</li>
<li>重启 kube-apiserver 和 kubelet 进程；</li>
<li>重新 approve kubelet 的 csr 请求；</li>
</ol>
<p>!!!注意，需要将token.csv文件复制到另外两台服务器，不要重新生成token.csv文件，文件目录是之前的ssl文件的上一级目录/etc/kubernetes</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp token.csv /etc/kubernetes/</span><br></pre></td></tr></table></figure>
<h2 id="创建服务需要的kubeconfig文件"><a href="#创建服务需要的kubeconfig文件" class="headerlink" title="创建服务需要的kubeconfig文件"></a>创建服务需要的kubeconfig文件</h2><p>在开启了 TLS 的集群中，每当与集群交互的时候少不了的是身份认证，使用kubeconfig（即证书） 和 token 两种认证方式是最简单也最通用的认证方式。</p>
<p><font color="red" size="6">总之kubeconfig就是为访问集群所作的配置。</font></p>
<h3 id="创建-kubelet服务所需-bootstrapping-kubeconfig文件"><a href="#创建-kubelet服务所需-bootstrapping-kubeconfig文件" class="headerlink" title="创建 kubelet服务所需 bootstrapping.kubeconfig文件"></a>创建 kubelet服务所需 bootstrapping.kubeconfig文件</h3><p>主要是用于生成的bootstrap.kubeconfig提供给kubelet进行node注册<br>bootstrap.kubeconfig.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/kubernetes</span><br><span class="line">export KUBE_APISERVER=&quot;https://192.168.137.13:9443&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置集群参数</span></span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">--certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">--embed-certs=true \</span><br><span class="line">--server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">--kubeconfig=bootstrap.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置客户端认证参数</span></span><br><span class="line">kubectl config set-credentials kubelet-bootstrap \</span><br><span class="line">--token=$&#123;BOOTSTRAP_TOKEN&#125; \</span><br><span class="line">--kubeconfig=bootstrap.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置上下文参数</span></span><br><span class="line">kubectl config set-context default \</span><br><span class="line">--cluster=kubernetes \</span><br><span class="line">--user=kubelet-bootstrap \</span><br><span class="line">--kubeconfig=bootstrap.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置默认上下文</span></span><br><span class="line">kubectl config use-context default --kubeconfig=bootstrap.kubeconfig</span><br></pre></td></tr></table></figure>

<ul>
<li>  上述也用到了变量BOOTSTRAP_TOKEN，所以kubeconfig生成后也是复制到其他服务</li>
<li>  –embed-certs 为 true 时表示将 certificate-authority 证书写⼊到⽣成的bootstrap.kubeconfig⽂件中；</li>
<li>  设置客户端认证参数时没有指定秘钥和证书，后续由 kubeapiserver⾃动⽣成；</li>
</ul>
<h3 id="创建-kubelet服务所需-kubelet-kubeconfig文件"><a href="#创建-kubelet服务所需-kubelet-kubeconfig文件" class="headerlink" title="创建 kubelet服务所需 kubelet.kubeconfig文件"></a>创建 kubelet服务所需 kubelet.kubeconfig文件</h3><p>这里我们直接使用/root/.kube/config文件，也可以自己建立kubelet证书后来生成</p>
<h3 id="创建-kube-controller-manager服务所需kube-controller-manager-kubeconfig-⽂件"><a href="#创建-kube-controller-manager服务所需kube-controller-manager-kubeconfig-⽂件" class="headerlink" title="创建 kube-controller-manager服务所需kube-controller-manager.kubeconfig ⽂件"></a>创建 kube-controller-manager服务所需kube-controller-manager.kubeconfig ⽂件</h3><p>kube-controller-manager.kubeconfig.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/kubernetes</span><br><span class="line">export KUBE_APISERVER=&quot;https://192.168.137.13:9443&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置集群参数</span></span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">--certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">--embed-certs=true \</span><br><span class="line">--server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">--kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置客户端认证参数</span></span><br><span class="line">kubectl config set-credentials system:kube-controller-manager \</span><br><span class="line">--client-certificate=/etc/kubernetes/ssl/kube-controller-manager.pem \</span><br><span class="line">--client-key=/etc/kubernetes/ssl/kube-controller-manager-key.pem \</span><br><span class="line">--embed-certs=true \</span><br><span class="line">--kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置上下文参数</span></span><br><span class="line">kubectl config set-context system:kube-controller-manager@kubernetes \</span><br><span class="line">--cluster=kubernetes \</span><br><span class="line">--user=system:kube-controller-manager \</span><br><span class="line">--kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置默认上下文</span></span><br><span class="line">kubectl config use-context system:kube-controller-manager@kubernetes --kubeconfig=kube-controller-manager.kubeconfig</span><br></pre></td></tr></table></figure>

<h3 id="创建-kube-scheduler服务所需kube-scheduler-kubeconfig-⽂件"><a href="#创建-kube-scheduler服务所需kube-scheduler-kubeconfig-⽂件" class="headerlink" title="创建 kube-scheduler服务所需kube-scheduler.kubeconfig ⽂件"></a>创建 kube-scheduler服务所需kube-scheduler.kubeconfig ⽂件</h3><p>kube-scheduler.kubeconfig.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/kubernetes</span><br><span class="line">export KUBE_APISERVER=&quot;https://192.168.137.13:9443&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置集群参数</span></span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">--certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">--embed-certs=true \</span><br><span class="line">--server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">--kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置客户端认证参数</span></span><br><span class="line">kubectl config set-credentials system:kube-scheduler \</span><br><span class="line">--client-certificate=/etc/kubernetes/ssl/kube-scheduler.pem \</span><br><span class="line">--client-key=/etc/kubernetes/ssl/kube-scheduler-key.pem \</span><br><span class="line">--embed-certs=true \</span><br><span class="line">--kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置上下文参数</span></span><br><span class="line">kubectl config set-context system:kube-scheduler@kubernetes \</span><br><span class="line">--cluster=kubernetes \</span><br><span class="line">--user=system:kube-scheduler \</span><br><span class="line">--kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置默认上下文</span></span><br><span class="line">kubectl config use-context system:kube-scheduler@kubernetes --kubeconfig=kube-scheduler.kubeconfig</span><br></pre></td></tr></table></figure>

<h3 id="执行脚本文件生成kubeconfig文件"><a href="#执行脚本文件生成kubeconfig文件" class="headerlink" title="执行脚本文件生成kubeconfig文件"></a>执行脚本文件生成kubeconfig文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">source /root/bootstrap.kubeconfig.sh</span><br><span class="line">source /root/kube-controller-manager.kubeconfig.sh</span><br><span class="line">source /root/kube-scheduler.kubeconfig.sh</span><br></pre></td></tr></table></figure>
<h3 id="分发-kubeconfig-⽂件"><a href="#分发-kubeconfig-⽂件" class="headerlink" title="分发 kubeconfig ⽂件"></a>分发 kubeconfig ⽂件</h3><p>将 kubeconfig ⽂件分发到其他服务器的 /etc/kubernetes/ ⽬录，具体怎么分发自己操作</p>
<h1 id="Etcd3-3-4"><a href="#Etcd3-3-4" class="headerlink" title="Etcd3.3.4"></a>Etcd3.3.4</h1><p><font color="red" size="3">三台服务器都要执行</font></p>
<h2 id="TLS-认证⽂件"><a href="#TLS-认证⽂件" class="headerlink" title="TLS 认证⽂件"></a>TLS 认证⽂件</h2><p>需要为 etcd 集群创建加密通信的 TLS 证书，这⾥复⽤以前创建的kubernetes 证书</p>
<p>ca.pem kubernetes-key.pem kubernetes.pem</p>
<p>kubernetes 证书的 hosts 字段列表中包含上⾯三台服务器的IP，否则后续证书校验会失败；</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> wget https://github.com/coreos/etcd/releases/download/v3.3.4/etcd-v3.3.4-linux-amd64.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar zxf etcd-v3.3.4-linux-amd64.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mv etcd-v3.3.4-linux-amd64/etcd* /usr/<span class="built_in">local</span>/bin/</span></span><br></pre></td></tr></table></figure>
<h2 id="ectd服务services配置文件"><a href="#ectd服务services配置文件" class="headerlink" title="ectd服务services配置文件"></a>ectd服务services配置文件</h2><p>/usr/lib/systemd/system/etcd.service</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line"></span><br><span class="line">Description=Etcd Server</span><br><span class="line"></span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">After=network-online.target</span><br><span class="line"></span><br><span class="line">Wants=network-online.target</span><br><span class="line"></span><br><span class="line">Documentation=https://github.com/coreos</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line"></span><br><span class="line">Type=notify</span><br><span class="line"></span><br><span class="line">WorkingDirectory=/var/lib/etcd/</span><br><span class="line"></span><br><span class="line">EnvironmentFile=-/etc/etcd/etcd.conf</span><br><span class="line"></span><br><span class="line">ExecStart=/usr/local/bin/etcd </span><br><span class="line"></span><br><span class="line">--name $&#123;ETCD_NAME&#125; </span><br><span class="line"></span><br><span class="line">--cert-file=/etc/kubernetes/ssl/kubernetes.pem </span><br><span class="line"></span><br><span class="line">--key-file=/etc/kubernetes/ssl/kubernetes-key.pem </span><br><span class="line"></span><br><span class="line">--peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem </span><br><span class="line"></span><br><span class="line">--peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem </span><br><span class="line"></span><br><span class="line">--trusted-ca-file=/etc/kubernetes/ssl/ca.pem </span><br><span class="line"></span><br><span class="line">--peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem </span><br><span class="line"></span><br><span class="line">--initial-advertise-peer-urls $&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; </span><br><span class="line"></span><br><span class="line">--listen-peer-urls $&#123;ETCD_LISTEN_PEER_URLS&#125; </span><br><span class="line"></span><br><span class="line">--listen-client-urls $&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 </span><br><span class="line"></span><br><span class="line">--advertise-client-urls $&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; </span><br><span class="line"></span><br><span class="line">--initial-cluster-token $&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; </span><br><span class="line"></span><br><span class="line">--initial-cluster-infra1=https://192.168.137.10:2380,infra2=https://192.168.137.11:2380,infra3=https://192.168.137.12:2380 </span><br><span class="line"></span><br><span class="line">--initial-cluster-state new </span><br><span class="line"></span><br><span class="line">--data-dir=$&#123;ETCD_DATA_DIR&#125;</span><br><span class="line"></span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">RestartSec=5</span><br><span class="line"></span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line"></span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>  指定 etcd 的⼯作⽬录为 /var/lib/etcd ，数据⽬录为/var/lib/etcd，需在启动服务前创建这两个⽬录；</li>
<li>  为了保证通信安全，需要指定 etcd 的公私钥(cert-file和key-file)、Peers通信的公私钥和 CA证书(peer-cert-file、peer-key-file、peertrusted-ca-file)、客户端的CA证书（trusted-ca-file)；</li>
<li>  创建 kubernetes.pem 证书时使⽤的 kubernetes-csr.json ⽂件的hosts字段包含所有 <strong>etcd</strong> 节点的<strong>IP</strong>，否则证书校验会出错；</li>
<li>  –initial-cluster-state 值为 new 时， –name 的参数值必须位于–initial-cluster 列表中；<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /var/lib/etcd</span><br><span class="line">mkdir /etc/etcd</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="环境变量配置⽂件"><a href="#环境变量配置⽂件" class="headerlink" title="环境变量配置⽂件"></a>环境变量配置⽂件</h2><p>/etc/etcd/etcd.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> [member]</span></span><br><span class="line">ETCD_NAME=infra1</span><br><span class="line">ETCD_DATA_DIR=&quot;/var/lib/etcd&quot;</span><br><span class="line">ETCD_LISTEN_PEER_URLS=&quot;https://192.168.137.10:2380&quot;</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.137.10:2379&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash">[cluster]</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.137.10:2380&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=https://192.168.137.10:2379</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这是192.168.137.10节点的配置，其他两个etcd节点只要将上⾯的IP地址改成相应节点的IP地址即可。ETCD_NAME换成对应节点的infra1/2/3。<br>/etc/etcd/etcd.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> [member]</span></span><br><span class="line">ETCD_NAME=infra2</span><br><span class="line">ETCD_DATA_DIR=&quot;/var/lib/etcd&quot;</span><br><span class="line">ETCD_LISTEN_PEER_URLS=&quot;https://192.168.137.11:2380&quot;</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.137.11:2379&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash">[cluster]</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.137.11:2380&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=https://192.168.137.11:2379</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> [member]</span></span><br><span class="line">ETCD_NAME=infra3</span><br><span class="line">ETCD_DATA_DIR=&quot;/var/lib/etcd&quot;</span><br><span class="line">ETCD_LISTEN_PEER_URLS=&quot;https://192.168.137.12:2380&quot;</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.137.12:2379&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash">[cluster]</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.137.12:2380&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.137.12:2379&quot;</span><br></pre></td></tr></table></figure>

<h2 id="启动etcd服务"><a href="#启动etcd服务" class="headerlink" title="启动etcd服务"></a>启动etcd服务</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable etcd</span><br><span class="line">systemctl start etcd</span><br></pre></td></tr></table></figure>
<p>验证</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">etcdctl \</span><br><span class="line">--ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">--cert-file=/etc/kubernetes/ssl/kubernetes.pem \</span><br><span class="line">--key-file=/etc/kubernetes/ssl/kubernetes-key.pem \</span><br><span class="line">cluster-health</span><br></pre></td></tr></table></figure>

<p><font color="red" size="6">结果最后⼀⾏为 cluster is healthy 时表示集群服务正常。</font></p>
<h1 id="kube-apiserver"><a href="#kube-apiserver" class="headerlink" title="kube-apiserver"></a>kube-apiserver</h1><h2 id="创建-kube-apiserver的服务配置⽂件"><a href="#创建-kube-apiserver的服务配置⽂件" class="headerlink" title="创建 kube-apiserver的服务配置⽂件"></a>创建 kube-apiserver的服务配置⽂件</h2><p>/usr/lib/systemd/system/kube-apiserver.service</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes API Service</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=network.target</span><br><span class="line">After=etcd.service</span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/apiserver</span><br><span class="line">ExecStart=/usr/local/bin/kube-apiserver \</span><br><span class="line"><span class="meta">$</span><span class="bash">KUBE_API_ARGS</span></span><br><span class="line">Restart=on-failure</span><br><span class="line">Type=notify</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>

<h2 id="apiserver服务配置⽂件-etc-kubernetes-apiserver-内容"><a href="#apiserver服务配置⽂件-etc-kubernetes-apiserver-内容" class="headerlink" title="apiserver服务配置⽂件 /etc/kubernetes/apiserver 内容"></a>apiserver服务配置⽂件 /etc/kubernetes/apiserver 内容</h2><p>此处是主机名api的服务器配置，另外两台服务器修改其中的IP地址即可。<br>/etc/kubernetes/apiserver</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">KUBE_API_ARGS=&quot;--advertise-address=192.168.137.10 \</span><br><span class="line">    --allow-privileged=true \</span><br><span class="line">    --client-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">    --disable-admission-plugins=PersistentVolumeLabel \</span><br><span class="line">--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \</span><br><span class="line">--authorization-mode=RBAC,Node \</span><br><span class="line">--token-auth-file=/etc/kubernetes/token.csv \</span><br><span class="line">    --enable-bootstrap-token-auth=true \</span><br><span class="line">    --etcd-cafile=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">    --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \</span><br><span class="line">    --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \</span><br><span class="line">    --etcd-servers=https://192.168.137.10:2379,https://192.168.137.11:2379,https://192.168.137.12:2379 \</span><br><span class="line">    --insecure-port=0 \</span><br><span class="line">    --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \</span><br><span class="line">    --secure-port=6443 \</span><br><span class="line">    --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \</span><br><span class="line">    --service-cluster-ip-range=10.254.0.0/16 \</span><br><span class="line">    --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem  \</span><br><span class="line">    --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \</span><br><span class="line">    --kubelet-client-certificate=/etc/kubernetes/ssl/admin.pem \</span><br><span class="line">    --kubelet-client-key=/etc/kubernetes/ssl/admin-key.pem\</span><br><span class="line">    --proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.pem \</span><br><span class="line">    --proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client-key.pem \</span><br><span class="line">    --requestheader-allowed-names=front-proxy-client \</span><br><span class="line">    --requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem \</span><br><span class="line">    --requestheader-extra-headers-prefix=X-Remote-Extra- \</span><br><span class="line">    --requestheader-group-headers=X-Remote-Group \</span><br><span class="line">    --requestheader-username-headers=X-Remote-User \</span><br><span class="line">    --v=2 \</span><br><span class="line">    --logtostderr=true \</span><br><span class="line">    --audit-log-maxage=30  \</span><br><span class="line">     --audit-log-maxbackup=3  \</span><br><span class="line">     --audit-log-maxsize=100  \</span><br><span class="line">     --audit-log-path=/var/log/kubernetes/audit.log  \</span><br><span class="line">     --audit-policy-file=/etc/kubernetes/audit-policy.yml  \</span><br><span class="line">     --experimental-encryption-provider-config=/etc/kubernetes/encryption.yaml  \</span><br><span class="line">     --event-ttl=1h&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>  –authorization-mode=Node,RBAC： 开启 Node 和 RBAC授权模式，拒绝未授权的请求；</li>
<li>  –enable-admission-plugins：启用 ServiceAccount 和 NodeRestriction；</li>
<li>  –service-account-key-file：签名 ServiceAccount Token的公钥文件，kube-controller-manager 的 –service-account-private-key-file指定私钥文件，两者配对使用；</li>
<li>  –tls-*-file：指定 apiserver 使用的证书、私钥和 CA 文件。–client-ca-file用于验证 client (kue-controller-manager、kube-scheduler、kubelet、kube-proxy等)请求所带的证书；</li>
<li>  –kubelet-client-certificate、–kubelet-client-key：如果指定，则使用 https访问 kubelet APIs；需要为 kubernete 用户定义 RBAC 规则，否则无权访问 kubeletAPI；</li>
<li>  –service-cluster-ip-range： 指定 Service Cluster IP 地址段；</li>
<li>  –service-node-port-range： 指定 NodePort 的端口范围；</li>
<li>  –runtime-config=api/all=true： 启用所有版本的 APIs，如autoscaling/v2alpha1；</li>
<li>  –enable-bootstrap-token-auth：启用 kubelet bootstrap 的 token 认证；</li>
<li>  –kubelet-client-certificate=/etc/kubernetes/ssl/admin.pem这里我为什么不重新生成kubelet的证书呢，因为后面安装kubelet的时候使用的kubeconfig就是将admin用户生成给kubectl使用的kubeconfig（即.kube/config文件）复制给他使用了，所以这里直接使用admin证书，当然你也可以去生成kubelet证书。</li>
<li>  –kubelet-client-key=/etc/kubernetes/ssl/admin-key.pem，同上述</li>
<li>–proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.pem<br>  –proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client-key.pem<br>  –requestheader-allowed-names=front-proxy-client<br>  –requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem<br>  kube-proxy客户端使用证书。 如果不指定这些文件，将会无法exec进去pod和无法logs –f 查看pod 日志</li>
<li>–requestheader-allowed-names=front-proxy-client<br>  –requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem<br>  –requestheader-extra-headers-prefix=X-Remote-Extra-<br>  –requestheader-group-headers=X-Remote-Group<br>  –requestheader-username-headers=X-Remote-User<br>  用于支持metrics-server，后续将会讲解。</li>
<li>  –audit-policy-file=/etc/kubernetes/audit-policy.yml ,文件内容如下<br>参考地址：<a target="_blank" rel="noopener" href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/etc/kubernetes/audit-policy.yml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/etc/kubernetes/audit-policy.yml</a></li>
<li>–experimental-encryption-provider-config：启用加密特性；文件生成如下<br>参考地址：<a target="_blank" rel="noopener" href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/etc/kubernetes/encryption.yaml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/etc/kubernetes/encryption.yaml</a><br>  生成 EncryptionConfig 所需的加密 key<br>文件下载地址：<a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/etc/kubernetes/encryption-source.yaml">https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/etc/kubernetes/encryption-source.yaml</a><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">下载后将其更名为encryption.yaml</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mv encryption-source.yaml encryption.yaml</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ENCRYPT_SECRET=$( head -c 32 /dev/urandom | base64 )</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sed -ri <span class="string">&quot;/secret:/s#(: ).+#\1<span class="variable">$&#123;ENCRYPT_SECRET&#125;</span>#&quot;</span> encryption.yaml</span></span><br><span class="line"> </span><br></pre></td></tr></table></figure></li>
<li>  node1的apiserver服务配置文件只需要修改此处–advertise-address=192.168.137.11即可；</li>
<li>  node2的apiserver服务配置文件只需要修改此处–advertise-address=192.168.137.12即可；</li>
</ul>
<h2 id="启动kube-apiserver"><a href="#启动kube-apiserver" class="headerlink" title="启动kube-apiserver"></a>启动kube-apiserver</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-apiserver</span><br><span class="line">systemctl start kube-apiserver</span><br><span class="line">systemctl status kube-apiserver</span><br></pre></td></tr></table></figure>
<h1 id="kube-controller-manager"><a href="#kube-controller-manager" class="headerlink" title="kube-controller-manager"></a>kube-controller-manager</h1><p><font color="red" size="3">三台服务器都要执行</font></p>
<h2 id="创建-kube-controller-manager服务配置⽂件"><a href="#创建-kube-controller-manager服务配置⽂件" class="headerlink" title="创建 kube-controller-manager服务配置⽂件"></a>创建 kube-controller-manager服务配置⽂件</h2><p>/usr/lib/systemd/system/kube-controller-manager.service</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line"></span><br><span class="line">Description=Kubernetes Controller Manager</span><br><span class="line"></span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line"></span><br><span class="line">EnvironmentFile=-/etc/kubernetes/controller-manager</span><br><span class="line"></span><br><span class="line">ExecStart=/usr/local/bin/kube-controller-manager </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash">KUBE_CONTROLLER_MANAGER_ARGS</span></span><br><span class="line"></span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line"></span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>

<h2 id="controller-manager服务配置⽂件-etc-kubernetes-controller-manager"><a href="#controller-manager服务配置⽂件-etc-kubernetes-controller-manager" class="headerlink" title="controller-manager服务配置⽂件 /etc/kubernetes/controller-manager "></a>controller-manager服务配置⽂件 /etc/kubernetes/controller-manager </h2><p>/etc/kubernetes/controller-manager</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">KUBE_CONTROLLER_MANAGER_ARGS=&quot;--address=0.0.0.0 \</span><br><span class="line">    --allocate-node-cidrs=true \</span><br><span class="line">    --cluster-cidr=10.10.0.0/16 \</span><br><span class="line">    --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">    --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  \</span><br><span class="line">    --controllers=*,bootstrapsigner,tokencleaner \</span><br><span class="line">    --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span><br><span class="line">    --leader-elect=true \</span><br><span class="line">    --node-cidr-mask-size=24 \</span><br><span class="line">    --root-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">    --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \</span><br><span class="line">--use-service-account-credentials=true &quot;</span><br></pre></td></tr></table></figure>

<ul>
<li>  –address=0.0.0.0此处为了方便后续的prometheus-operator访问服务端口设置为所有网卡可以访问，之前设置的是127.0.0.1</li>
<li>  –allocate-node-cidrs=true，您的Kubernetes控制器管理器配置为分配pod CIDR（即通过传递–allocate-node-cidrs=true给控制器管理器）</li>
<li>  –cluster-cidr=10.10.0.0/16，您的Kubernetes控制器管理器已经提供了一个cluster-cidr（即通过传递–cluster-cidr=10.10.0.0/16，默认情况下清单需要）。</li>
</ul>
<h2 id="启动-kube-controller-manager"><a href="#启动-kube-controller-manager" class="headerlink" title="启动 kube-controller-manager"></a>启动 kube-controller-manager</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-controller-manager</span><br><span class="line">systemctl start kube-controller-manager</span><br><span class="line">systemctl status kube-controller-manager</span><br></pre></td></tr></table></figure>
<h1 id="kube-scheduler"><a href="#kube-scheduler" class="headerlink" title="kube-scheduler"></a>kube-scheduler</h1><p><font color="red" size="3">三台服务器都要执行</font></p>
<h2 id="创建-kube-scheduler服务配置⽂件"><a href="#创建-kube-scheduler服务配置⽂件" class="headerlink" title="创建 kube-scheduler服务配置⽂件"></a>创建 kube-scheduler服务配置⽂件</h2><p>/usr/lib/systemd/system/kube-scheduler.service</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line"></span><br><span class="line">Description=Kubernetes Scheduler Plugin</span><br><span class="line"></span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line"></span><br><span class="line">EnvironmentFile=-/etc/kubernetes/scheduler</span><br><span class="line"></span><br><span class="line">ExecStart=/usr/local/bin/kube-scheduler </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash">KUBE_SCHEDULER_ARGS</span></span><br><span class="line"></span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line"></span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line"></span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>

<h2 id="scheduler服务配置⽂件-etc-kubernetes-scheduler"><a href="#scheduler服务配置⽂件-etc-kubernetes-scheduler" class="headerlink" title="scheduler服务配置⽂件 /etc/kubernetes/scheduler "></a>scheduler服务配置⽂件 /etc/kubernetes/scheduler </h2><p>/etc/kubernetes/scheduler</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">KUBE_SCHEDULER_ARGS=&quot;--address=0.0.0.0 \</span><br><span class="line">    --leader-elect=true \</span><br><span class="line">--kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>  –address=0.0.0.0此处为了方便后续的prometheus-operator访问服务端口设置为所有网卡可以访问，之前设置的是127.0.0.1</li>
</ul>
<h2 id="启动-kube-scheduler"><a href="#启动-kube-scheduler" class="headerlink" title="启动 kube-scheduler"></a>启动 kube-scheduler</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-scheduler</span><br><span class="line">systemctl start kube-scheduler</span><br><span class="line">systemctl status kube-scheduler</span><br></pre></td></tr></table></figure>
<h1 id="至此Master三大组件安装完成"><a href="#至此Master三大组件安装完成" class="headerlink" title="至此Master三大组件安装完成"></a>至此Master三大组件安装完成</h1><p>查看运行状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for i in kube-apiserver kube-controller-manager kube-scheduler; do systemctl restart $i ; done</span><br><span class="line">for i in kube-apiserver kube-controller-manager kube-scheduler; do systemctl status $i -l ; done</span><br></pre></td></tr></table></figure>
<p>验证集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl cluster-info</span></span><br><span class="line">Kubernetes master is running at https://192.168.137.13:9443</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get cs</span></span><br><span class="line">NAME STATUS MESSAGE ERROR</span><br><span class="line">scheduler Healthy ok</span><br><span class="line">controller-manager Healthy ok</span><br><span class="line">etcd-0 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br><span class="line">etcd-1 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br><span class="line">etcd-2 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h1><p>Docker从1.13版本开始调整了默认的防火墙规则，禁用了iptables filter表中FOWARD链，这样会引起Kubernetes集群中跨Node的Pod无法通信，因此docker安装完成后，还需要手动修改iptables规则。</p>
<p><font color="red" size="3">三台服务器都要执行</font></p>
<h2 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line">yum makecache fast</span><br><span class="line">yum install -y docker-ce</span><br></pre></td></tr></table></figure>
<h2 id="编辑systemctl的Docker启动文件"><a href="#编辑systemctl的Docker启动文件" class="headerlink" title="编辑systemctl的Docker启动文件"></a>编辑systemctl的Docker启动文件</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &quot;13i ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT&quot; /usr/lib/systemd/system/docker.service</span><br></pre></td></tr></table></figure>
<h2 id="修改docker参数"><a href="#修改docker参数" class="headerlink" title="修改docker参数"></a>修改docker参数</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/docker/</span><br><span class="line"><span class="meta">cat&gt;</span><span class="bash">/etc/docker/daemon.json&lt;&lt;<span class="string">EOF</span></span></span><br><span class="line">&#123;</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https://fz5yth0r.mirror.aliyuncs.com&quot;],</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;storage-opts&quot;: [</span><br><span class="line">    &quot;overlay2.override_kernel_check=true&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;100m&quot;,</span><br><span class="line">    &quot;max-file&quot;: &quot;3&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="启动docker"><a href="#启动docker" class="headerlink" title="启动docker"></a>启动docker</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable docker</span><br><span class="line">systemctl start docker</span><br></pre></td></tr></table></figure>

<h1 id="建立不加密的docker私有仓库方式"><a href="#建立不加密的docker私有仓库方式" class="headerlink" title="建立不加密的docker私有仓库方式"></a>建立不加密的docker私有仓库方式</h1><h2 id="建立最简单的私库"><a href="#建立最简单的私库" class="headerlink" title="建立最简单的私库"></a>建立最简单的私库</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --restart=always --name dockerregistry \</span><br><span class="line"> -p 5000:5000 \</span><br><span class="line"> -v /huisebug/dockerimagestorehouse/registry:/var/lib/registry \</span><br><span class="line"> -v /huisebug/dockerimagestorehouse/config.yml:/etc/docker/registry/config.yml  registry</span><br></pre></td></tr></table></figure>
<p>config.yml内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">version: 0.1</span><br><span class="line">log:</span><br><span class="line">  fields:</span><br><span class="line">    service: registry</span><br><span class="line">storage:</span><br><span class="line">  delete:</span><br><span class="line">    enabled: true</span><br><span class="line">  cache:</span><br><span class="line">    blobdescriptor: inmemory</span><br><span class="line">  filesystem:</span><br><span class="line">    rootdirectory: /var/lib/registry</span><br><span class="line">http:</span><br><span class="line">  addr: :5000</span><br><span class="line">  headers:</span><br><span class="line">    X-Content-Type-Options: [nosniff]</span><br><span class="line">health:</span><br><span class="line">  storagedriver:</span><br><span class="line">    enabled: true</span><br><span class="line">    interval: 10s</span><br><span class="line">    threshold: 3</span><br></pre></td></tr></table></figure>
<p>更改docker的service文件增加–insecure-registry 192.168.137.10:5000<br>ExecStart=/usr/bin/dockerd –insecure-registry 192.168.137.10:5000</p>
<p>重启docker</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>

<h1 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h1><p><font color="red" size="3">三台服务器都要执行</font></p>
<h2 id="创建-kubelet服务配置⽂件"><a href="#创建-kubelet服务配置⽂件" class="headerlink" title="创建 kubelet服务配置⽂件"></a>创建 kubelet服务配置⽂件</h2><p> /usr/lib/systemd/system/kubelet.service</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line"></span><br><span class="line">Description=Kubernetes Kubelet Server</span><br><span class="line"></span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">After=docker.service</span><br><span class="line"></span><br><span class="line">Requires=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line"></span><br><span class="line">WorkingDirectory=/var/lib/kubelet</span><br><span class="line"></span><br><span class="line">EnvironmentFile=-/etc/kubernetes/kubelet</span><br><span class="line"></span><br><span class="line">ExecStart=/usr/local/bin/kubelet \</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash">KUBELET_ARGS</span></span><br><span class="line"></span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line"></span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>创建工作目录，需要手动创建</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /var/lib/kubelet</span><br></pre></td></tr></table></figure>

<h2 id="kubelet服务配置文件-etc-kubernetes-kubelet"><a href="#kubelet服务配置文件-etc-kubernetes-kubelet" class="headerlink" title="kubelet服务配置文件/etc/kubernetes/kubelet"></a>kubelet服务配置文件/etc/kubernetes/kubelet</h2><p>/etc/kubernetes/kubelet</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">KUBELET_ARGS=&quot;--bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><br><span class="line">--kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><br><span class="line">--config=/var/lib/kubelet/config.yaml \</span><br><span class="line">--cni-bin-dir=/opt/cni/bin \</span><br><span class="line">--cni-conf-dir=/etc/cni/net.d \</span><br><span class="line">--network-plugin=cni&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>  bootstrap.kubeconfig之前我们已经建立了。kubelet 使⽤该⽂件中的⽤户名和 token向 kube-apiserver 发送 TLS Bootstrapping 请求；</li>
<li>  –kubeconfig=/etc/kubernetes/kubelet.kubeconfig 中指定的 kubelet.kubeconfig⽂件在第⼀次启动kubelet之前并不存在，请看下⽂，当通过CSR请求后会⾃动⽣成kubelet.kubeconfig ⽂件，如果你的节点上已经⽣成了 ~/.kube/config⽂件，你可以将该⽂件拷⻉到该路径下，并重命名为 kubelet.kubeconfig，所有node节点可以共⽤同⼀个kubelet.kubeconfig⽂件，这样新添加的节点就不需要再创建CSR请求就能⾃动添加到kubernetes集群中。同样，在任意能够访问到kubernetes集群的主机上使⽤kubectl –kubeconfig 命令操作集群时，只要使⽤ ~/.kube/config⽂件就可以通过权限认证，因为这⾥⾯已经有认证信息并认为你是admin⽤户，对集群拥有所有权限。<br>执行以下命令<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp -rf /root/.kube/config /etc/kubernetes/kubelet.kubeconfig</span><br></pre></td></tr></table></figure></li>
</ul>
<p><font color="red" size="3">并且分发到所有安装kubelet的服务器</font></p>
<ul>
<li>  –config=/var/lib/kubelet/config.yaml中声明了kubelet服务的端口信息等，参考地址：<a target="_blank" rel="noopener" href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/var/lib/kubelet/config.yaml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/var/lib/kubelet/config.yaml</a></li>
<li>–cni-bin-dir=/opt/cni/bin<br> –cni-conf-dir=/etc/cni/net.d<br> –network-plugin=cni，你有一个Kubernetes集群配置为使用CNI网络插件（即通过传递–network-plugin=cni给kubelet）</li>
</ul>
<p>建立文件目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /etc/kubernetes/manifests</span><br><span class="line">echo &quot; mkdir /etc/kubernetes/manifests&quot; &gt;&gt; /etc/rc.local</span><br></pre></td></tr></table></figure>

<p><font color="red" size="6">!!!注意</font></p>
<ul>
<li>  hostPort不适用于CNI</li>
<li>  使用<em>hostPort</em>和CNI插件的组合将导致Kubernetes静默忽略<em>hostPort</em>属性。</li>
</ul>
<h2 id="启动kubelet"><a href="#启动kubelet" class="headerlink" title="启动kubelet"></a>启动kubelet</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kubelet</span><br><span class="line">systemctl start kubelet</span><br><span class="line">systemctl status kubelet</span><br></pre></td></tr></table></figure>
<h2 id="检查node状态"><a href="#检查node状态" class="headerlink" title="检查node状态"></a>检查node状态</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node</span></span><br><span class="line">NAME STATUS ROLES AGE VERSION</span><br><span class="line">api.huisebug.com NotReady &lt;none&gt; 6m8s v1.12.4</span><br><span class="line">node1.huisebug.com NotReady &lt;none&gt; 4s v1.12.4</span><br><span class="line">node2.huisebug.com NotReady &lt;none&gt; 1s v1.12.4</span><br></pre></td></tr></table></figure>
<p>可以看到状态为NotReady 因为缺少cni配置文件而导致kubelet服务（systemctl status kubelet -l）在日志中提示找不到配置而无法变为Ready</p>
<p>我们可以将/etc/cni/net.d/下的配置文件暂时建立10-calico.conflist，使状态变为Ready。后续再做修改或者删除。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p /etc/cni/net.d/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vim /etc/cni/net.d/10-calico.conflist</span></span><br></pre></td></tr></table></figure>
<p>参考下载地址：<br><a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/etc/cni/net.d/10-calico.conflist">https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/etc/cni/net.d/10-calico.conflist</a></p>
<p>再次查看状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node</span></span><br><span class="line">NAME STATUS ROLES AGE VERSION</span><br><span class="line">api.huisebug.com Ready &lt;none&gt; 34m v1.12.4</span><br><span class="line">node1.huisebug.com Ready &lt;none&gt; 28m v1.12.4</span><br><span class="line">node2.huisebug.com Ready &lt;none&gt; 28m v1.12.4</span><br></pre></td></tr></table></figure>

<h1 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h1><p>这里我们采用daemonsets方式建立</p>
<p>这里的代理模式使用的是ipvs，不再使用iptables，并且ipvs依赖于nf_conntrack_ipv4，所以需要将服务器的内核目录挂载到pod中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 建立serviceaccount</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl -n kube-system create serviceaccount kube-proxy</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 建立集群角色绑定</span></span><br><span class="line">kubectl create clusterrolebinding system:kube-proxy \</span><br><span class="line">--clusterrole system:node-proxier \</span><br><span class="line">--serviceaccount kube-system:kube-proxy</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 建立daemonsets方式建立的pod和需要的参数configmap，参考地址:</span></span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/kube-proxy/kube-proxy.yaml</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 建立集群的第一个pod，会需要基础pod镜像，每台服务器执行一遍。拉取基础镜像。</span></span><br><span class="line">docker pull huisebug/sec_re:pause3.1 &amp;&amp; docker tag huisebug/sec_re:pause3.1 k8s.gcr.io/pause:3.1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看状态</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod --all-namespaces -o wide</span></span><br><span class="line">![](media/3bf05a168dbd6d583e0c0d5dbb65b226.png)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">通过ipvsadm查看 proxy 规则</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ipvsadm -ln</span></span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> RemoteAddress:Port Forward Weight ActiveConn InActConn</span></span><br><span class="line">TCP 10.254.0.1:443 rr</span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> 192.168.137.10:6443 Masq 1 0 0</span></span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> 192.168.137.11:6443 Masq 1 0 0</span></span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> 192.168.137.12:6443 Masq 1 0 0</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 确认使用ipvs模式</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> curl localhost:10249/proxyMode</span></span><br><span class="line">ipvs</span><br></pre></td></tr></table></figure>
<h1 id="至此Node三大组件安装完成"><a href="#至此Node三大组件安装完成" class="headerlink" title="至此Node三大组件安装完成"></a>至此Node三大组件安装完成</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">查看状态</span><br><span class="line">for i in etcd kube-apiserver kube-controller-manager kube-scheduler kubelet docker; do systemctl status $i -l; done</span><br><span class="line"></span><br><span class="line">重启所有二进制安装的</span><br><span class="line">for i in etcd kube-apiserver kube-controller-manager kube-scheduler kubelet docker; do systemctl restart $i; done</span><br></pre></td></tr></table></figure>

<h1 id="Calico"><a href="#Calico" class="headerlink" title="Calico"></a>Calico</h1><h2 id="环境要求，原打算是使用3-4-0版本，奈何失败了。"><a href="#环境要求，原打算是使用3-4-0版本，奈何失败了。" class="headerlink" title="环境要求，原打算是使用3.4.0版本，奈何失败了。"></a>环境要求，原打算是使用3.4.0版本，奈何失败了。</h2><ul>
<li>  您的Kubernetes控制器管理器配置为分配pod CIDR（即通过传递–allocate-node-cidrs=true给控制器管理器）</li>
<li>  您的Kubernetes控制器管理器已经提供了一个cluster-cidr（即通过传递–cluster-cidr=10.10.0.0/16，默认情况下清单需要）。</li>
<li>  你有一个Kubernetes集群配置为使用CNI网络插件（即通过传递–network-plugin=cni给kubelet）<blockquote>
<p>  我犯了一个错误就是将cluster-cidr和cluster-range-ip配置成相同了，修改后如果出现pod无法建立，<br>  那么就将etcd的数据目录清空，然后重新启动所有服务，让apiserver重新请求etcd并写入数据。</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /var/lib/etcd/*</span><br><span class="line">for i in etcd kube-apiserver kube-controller-manager kube-scheduler kubelet docker; do systemctl restart $i ; done</span><br></pre></td></tr></table></figure>
参考我的yaml文件,calico3.1.3<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export interface=ens33</span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Calico/calico3.1.3node-ctl.yaml</span><br></pre></td></tr></table></figure></li>
<li>  记得上面要将cid地址修改为你的cidr地址</li>
<li>  记得查看interface名称，系统不同网卡名称也不同</li>
</ul>
<p>执行后检测pod建立情况<br><img src="/2020/08/26/k8s/media/6ffc2f19054e181ed137799a29bd742e.png"><br>检测网卡情况<br><img src="/2020/08/26/k8s/media/e94fcb31b73adab699fc0580db9a037b.png"><br>会发现缺少cali*开头的网卡名称，那是我们现在还没建立过不使用hostNetwork: true的pod，我们这里使用建立一个普通的nginx pod</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f https://kubernetes.io/docs/user-guide/nginx-deployment.yaml --record</span><br></pre></td></tr></table></figure>
<p>再次查看网卡信息<br><img src="/2020/08/26/k8s/media/27573e66d33decbf6a61f9f972f9ce11.png"></p>
<h2 id="验证不同节点的容器之间能否ping通"><a href="#验证不同节点的容器之间能否ping通" class="headerlink" title="验证不同节点的容器之间能否ping通"></a>验证不同节点的容器之间能否ping通</h2><p><img src="/2020/08/26/k8s/media/c45fba5757f980bff79e58f73ff0456c.png"></p>
<h2 id="验证能否访问外网"><a href="#验证能否访问外网" class="headerlink" title="验证能否访问外网"></a>验证能否访问外网</h2><p>建立一个测试工具pod，busybox需要一个持续输出，这里我将网关地址赋予它</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl run busybox --image=busybox --command -- ping 192.168.137.1</span><br><span class="line">kubectl exec -it busybox-759d8dbd98-tf9hd ping 61.139.2.69</span><br><span class="line">kubectl exec -it busybox-759d8dbd98-tf9hd nslookup baidu.com 61.139.2.69</span><br></pre></td></tr></table></figure>
<p><img src="/2020/08/26/k8s/media/dc12719300e4ac7400b7b62617a2ce72.png"></p>
<p>至此calico安装完成。</p>
<h2 id="问题汇总"><a href="#问题汇总" class="headerlink" title="问题汇总"></a>问题汇总</h2><p>如果kubectl get node一直是notready，</p>
<p>执行下面步骤</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Calico/calico3.1.3node-ctl.yaml</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">三台服务器都执行</span></span><br><span class="line">rm -rf /etc/cni/net.d/*</span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Calico/calico3.1.3node-ctl.yaml</span><br></pre></td></tr></table></figure>

<h1 id="Coredns"><a href="#Coredns" class="headerlink" title="Coredns"></a>Coredns</h1><p>参考地址,记得将其中的集群ip地址修改为你在kubelet中定义的地址</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Coredns/coredns.yaml</span><br></pre></td></tr></table></figure>
<h2 id="验证效果"><a href="#验证效果" class="headerlink" title="验证效果"></a>验证效果</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">查看是否成功建立coredns pod</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod --all-namespaces -o wide 查看是否成功建立coredns pod</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc</span></span><br><span class="line">NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</span><br><span class="line">kubernetes ClusterIP 10.254.0.1 &lt;none&gt; 443/TCP 16m</span><br><span class="line"></span><br><span class="line">可以看到default命名空间下现在就一个service</span><br><span class="line">将之前建立的nginx-deployment暴露为service</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl expose deploy nginx-deployment</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc</span></span><br><span class="line">NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</span><br><span class="line">kubernetes ClusterIP 10.254.0.1 &lt;none&gt; 443/TCP 16m</span><br><span class="line">nginx-deployment ClusterIP 10.254.136.63 &lt;none&gt; 80/TCP 3s</span><br></pre></td></tr></table></figure>
<p>测试能否解析成对应的IP地址</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -it nginx-deployment-5c689d88bb-2mzx4 ping nginx-deployment</span><br></pre></td></tr></table></figure>
<p><img src="/2020/08/26/k8s/media/82e7d16fd70b1d760463183eeafd14c6.png"></p>
<p>测试不同命名空间</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc --all-namespaces</span><br></pre></td></tr></table></figure>
<p><img src="/2020/08/26/k8s/media/0080be720975ed9265171ad7919a5b22.png"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -it nginx-deployment-5c689d88bb-2mzx4 ping calico-typha.kube-system</span><br></pre></td></tr></table></figure>
<p><img src="/2020/08/26/k8s/media/172614abc7d8086efdaa1aa3e170edbb.png"></p>
<p>至此，coredns安装完成。</p>
<h1 id="再次验证整个集群的proxy负载机制效果"><a href="#再次验证整个集群的proxy负载机制效果" class="headerlink" title="再次验证整个集群的proxy负载机制效果"></a>再次验证整个集群的proxy负载机制效果</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -ln</span><br></pre></td></tr></table></figure>
<p><img src="/2020/08/26/k8s/media/23eea17b92a059fc23388c2672365cb2.png"></p>
<h1 id="Helm"><a href="#Helm" class="headerlink" title="Helm"></a>Helm</h1><p>相当于centos的yum，Ubuntu的apt-get命令</p>
<p>此处安装的helm是2.12.2版本，之前的2.6版本在安装后续的chart会出现：</p>
<p>Error: parse error in *** function “genCA” not defined 错误</p>
<h2 id="Helm-client安装"><a href="#Helm-client安装" class="headerlink" title="Helm client安装"></a>Helm client安装</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 下载</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> wget https://storage.googleapis.com/kubernetes-helm/helm-v2.12.2-linux-amd64.tar.gz</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 解包并将二进制文件helm拷贝到/usr/<span class="built_in">local</span>/bin目录下</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar -zxvf helm-v2.12.2-linux-amd64.tar.gz &amp;&amp; mv linux-amd64/helm /usr/<span class="built_in">local</span>/bin/helm</span></span><br></pre></td></tr></table></figure>
<h2 id="安装socat"><a href="#安装socat" class="headerlink" title="安装socat"></a>安装socat</h2><p><font color="red" size="3">用于端口转发，在准备初始环境安装keepalived已经安装，必须在所有Node服务器安装</font></p>
<h2 id="Helm-server安装"><a href="#Helm-server安装" class="headerlink" title="Helm server安装"></a>Helm server安装</h2><p>创建tiller的 serviceaccount 和 clusterrolebinding</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl create serviceaccount --namespace kube-system tiller</span><br><span class="line">kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller</span><br></pre></td></tr></table></figure>
<p>然后安装helm服务端tiller<br>这里因为一些原因，使用的是阿里云的镜像</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.12.2 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</span><br></pre></td></tr></table></figure>
<p>修改serviceAccount ：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl patch deploy -n kube-system tiller-deploy -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;serviceAccount&quot;:&quot;tiller&quot;&#125;&#125;&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure>
<p>检查是否安装成功，等待一段时间后。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> helm version</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/08/26/k8s/media/f858112ba11661d4a205598f9c0a426f.png"></p>
<h1 id="Traefik"><a href="#Traefik" class="headerlink" title="Traefik"></a>Traefik</h1><p>k8s集群中的http反向代理服务</p>
<p>直接下载官方的charts，然后找到traefik就可以执行安装了</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> &lt;https://github.com/helm/charts.git&gt;</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> charts/stable/traefik</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 执行默认配置，显然是不符合我这里的需求。</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> helm install .</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 列出helm安装的资源类型</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> helm ls</span></span><br><span class="line">NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE</span><br><span class="line">gilded-skunk 1 Fri Jan 18 10:36:42 2019 DEPLOYED traefik-1.59.0 1.7.6 default</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">删除helm安装的资源类型</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> helm delete gilded-skunk --purge</span></span><br><span class="line">release &quot;gilded-skunk&quot; deleted</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装我想要的一些功能</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认的template目录下的deployment.yaml文件没有开启hostNetwork: <span class="literal">true</span>，执行下面语句开启（主要是我这儿环境配置了hostPort不生效，（因为CNI插件的原因）配置hostNetwork就生效）</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /root/charts/stable/traefik/templates</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> Lnum=$(sed -n <span class="string">&#x27;/spec/=&#x27;</span> deployment.yaml |sed -n <span class="string">&quot;2&quot;</span>p)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下面的格式为sed -ie 单引单引双引变量名双引单引a六个空格hostNetwork: <span class="literal">true</span>单引 deployment.yaml</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sed -ie &amp;<span class="comment">#39;&amp;#39;&amp;#34;$Lnum&amp;#34;&amp;#39;a&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;hostNetwork: true&amp;#39;  deployment.yaml</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">中间是6个空格，不能少也不能多</span></span><br></pre></td></tr></table></figure>

<p>也可以参考我已经修改好的charts，参考地址：<br><a target="_blank" rel="noopener" href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/Traefik">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/Traefik</a><br>参考地址仅做参考，因为官方charts是不断更新的，或许后续会不断添加新功能，例如后续可能会添加支持hostNetwork: true功能，推荐还是根据官方charts来设置。</p>
<h2 id="安装traefik"><a href="#安装traefik" class="headerlink" title="安装traefik"></a>安装traefik</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> helm delete traefik --purge</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> helm install --name traefik --namespace kube-system \</span></span><br><span class="line"><span class="bash">--<span class="built_in">set</span> replicas=3,cpuLimit=1000m,memoryLimit=1Gi,rbac.enabled=<span class="literal">true</span>,\</span></span><br><span class="line"><span class="bash">dashboard.enabled=<span class="literal">true</span>,dashboard.domain=traefik.huisebug.com,\</span></span><br><span class="line"><span class="bash">metrics.prometheus.enabled=<span class="literal">true</span> \</span></span><br><span class="line"><span class="bash">/root/charts/stable/traefik</span></span><br></pre></td></tr></table></figure>
<p>确保服务已经运行<br><img src="/2020/08/26/k8s/media/9e90faf47ed6c8b79f1a213d8f1865ad.png"><br><img src="/2020/08/26/k8s/media/12dffb5c85ff5fea46833c01e4a7a070.png"></p>
<p>重启服务器后如果traefik没有正确启动，那就删除后重新建立。</p>
<h2 id="验证效果-1"><a href="#验证效果-1" class="headerlink" title="验证效果"></a>验证效果</h2><p>之前我们验证coredns时候将一个nginx暴露为service，现在建立一个ingress来对应这个service</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc --all-namespaces</span><br></pre></td></tr></table></figure>
<p><img src="/2020/08/26/k8s/media/76bd44213cfe4012bf900395b9497aa7.png"></p>
<p>参考yaml地址</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Traefik/nginx-ingress.yaml</span><br></pre></td></tr></table></figure>
<p>在本地解析文件中追加域名解析<br>192.168.137.10 traefik.huisebug.com nginx-deployment.huisebug.com</p>
<p>访问验证效果<br><img src="/2020/08/26/k8s/media/1089f58ff64fe2e1987d9d72af840686.png"><br><img src="/2020/08/26/k8s/media/bacc338b37613e8ee8c525253c1c6a42.png"></p>
<p>至此，traefik安装完成</p>
<h1 id="Dashboard"><a href="#Dashboard" class="headerlink" title="Dashboard"></a>Dashboard</h1><p>会额外建立一个名称为anonymous-dashboard-proxy的 Cluster Role(Binding)来让system:anonymous这个匿名使用者能够通过 API Server 来 proxy 到 KubernetesDashboard,而这个 RBAC规则仅能够存取services/proxy资源,以及https:kubernetes-dashboard:资源名称同时在1.7 版本以后的 Dashboard 将不再提供所有权限,因此需要建立一个 service account来绑定 cluster-admin role</p>
<p>具体yaml参考</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Dashboard/dashboard.yaml</span><br></pre></td></tr></table></figure>

<h2 id="访问验证"><a href="#访问验证" class="headerlink" title="访问验证"></a>访问验证</h2><ul>
<li>  方式一：可以通过浏览器读取Dashboard</li>
<li>  方式二：ingress既然上述我们已经搭建了traefik，所以我们建立一个ingress来访问。</li>
</ul>
<h3 id="直接https访问方式"><a href="#直接https访问方式" class="headerlink" title="直接https访问方式"></a>直接https访问方式</h3><h4 id="导⼊证书"><a href="#导⼊证书" class="headerlink" title="导⼊证书"></a>导⼊证书</h4><p>将⽣成的admin.pem证书转换格式（/etc/kubernetes/ssl目录下）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/kubernetes/ssl</span><br><span class="line">openssl pkcs12 -export -in admin.pem -out admin.p12 -inkey admin-key.pem</span><br></pre></td></tr></table></figure>
<p>将⽣成的 admin.p12<br>证书导⼊的你的电脑的浏览器，导出的时候记住你设置的密码，导⼊的时候还要⽤到。</p>
<h4 id="访问"><a href="#访问" class="headerlink" title="访问"></a>访问</h4><p>这里不能写成VIP+代理端口来跳转，我们三台服务器都安装了api-server，可以任意如下一个访问</p>
<p><a target="_blank" rel="noopener" href="https://192.168.137.10:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/">https://192.168.137.10:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</a></p>
<p><a target="_blank" rel="noopener" href="https://192.168.137.11:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/">https://192.168.137.11:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</a></p>
<p><a target="_blank" rel="noopener" href="https://192.168.137.12:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/">https://192.168.137.12:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</a></p>
<p>成功向浏览器导入证书后访问成功效果</p>
<p><img src="/2020/08/26/k8s/media/778d54e157730e8122dfc3f5ae727361.png"></p>
<p>使用令牌（token）进行访问</p>
<p>令牌获取。</p>
<p>$ kubectl -n kube-system describe secrets | sed -rn &#34;/sdashboard-token-/,/^token/{/^token/s#S+s+##p}&#34;</p>
<p><img src="/2020/08/26/k8s/media/49791fb732caf260cf373180c5f08891.png"></p>
<p>将token值复制到令牌一栏即可成功访问dashboard</p>
<p><img src="/2020/08/26/k8s/media/de528414d705dc0816e623fb1c5725d4.png"></p>
<p><img src="/2020/08/26/k8s/media/1eeb76fe6060a353e0fd0548d47b94e4.png"></p>
<h3 id="ingress方式"><a href="#ingress方式" class="headerlink" title="ingress方式"></a>ingress方式</h3><p>参考yaml文件地址</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Dashboard/kubernetes-dashboard-ingress.yaml</span><br></pre></td></tr></table></figure>

<p>这里我们需要开启traefik的https功能，并且开启ssl.insecureSkipVerify=true跳过验证SSL连接上的证书，如果不开启此处，就无法使用ingress进行访问。此处是相当于使用了https连接，没有进行证书验证。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">helm delete traefik --purge</span><br><span class="line">helm install --name traefik --namespace kube-system \</span><br><span class="line">--set imageTag=1.6.5,replicas=3,\</span><br><span class="line">cpuLimit=1000m,memoryLimit=1Gi,rbac.enabled=true,\</span><br><span class="line">dashboard.enabled=true,dashboard.domain=traefik.huisebug.com,\</span><br><span class="line">metrics.prometheus.enabled=true,\</span><br><span class="line">ssl.enabled=true,ssl.insecureSkipVerify=true \</span><br><span class="line">/root/charts/stable/traefik</span><br></pre></td></tr></table></figure>
<p>如果出现traefik出现问题就降低配置中的traefik镜像版本。</p>
<h4 id="验证访问效果"><a href="#验证访问效果" class="headerlink" title="验证访问效果"></a>验证访问效果</h4><p><img src="/2020/08/26/k8s/media/1a75a341166a1a3c3fc59aff8d3b53c1.png"></p>
<h1 id="Scope监控"><a href="#Scope监控" class="headerlink" title="Scope监控"></a>Scope监控</h1><p>用于监控整个k8s集群的网络TOP</p>
<h2 id="安装scope"><a href="#安装scope" class="headerlink" title="安装scope"></a>安装scope</h2><p>直接使用官方yaml</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f &quot;https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d &#x27;\n&#x27;)&quot;</span><br></pre></td></tr></table></figure>


<p>注意上面的namespace是weave</p>
<h2 id="暴露访问两种方式"><a href="#暴露访问两种方式" class="headerlink" title="暴露访问两种方式"></a>暴露访问两种方式</h2><h3 id="Service的nodeport方式"><a href="#Service的nodeport方式" class="headerlink" title="Service的nodeport方式"></a>Service的nodeport方式</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">wget https://cloud.weave.works/k8s/scope.yaml</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改service的值</span></span><br><span class="line">    spec:</span><br><span class="line">      type: NodePort</span><br><span class="line">      ports:</span><br><span class="line">        - name: app</span><br><span class="line">          port: 80</span><br><span class="line">          protocol: TCP</span><br><span class="line">          targetPort: 4040</span><br><span class="line">          nodePort: 30040</span><br></pre></td></tr></table></figure>

<p>注意：Nodeport只能暴露30000到32767，不然会报错如下：</p>
<p>The Service “weave-scope-app” is invalid: spec.ports[0].nodePort: Invalid value:<br>4040: provided port is not in the valid range. The range of valid ports is<br>30000-32767</p>
<h3 id="Traefik代理"><a href="#Traefik代理" class="headerlink" title="Traefik代理"></a>Traefik代理</h3><p>编写一个ingress</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">参考yaml地址</span><br><span class="line">kubectl create -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Scope/scope-ingress.yaml</span><br></pre></td></tr></table></figure>
<h2 id="验证效果-2"><a href="#验证效果-2" class="headerlink" title="验证效果"></a>验证效果</h2><p><img src="/2020/08/26/k8s/media/3be2bb4c074a23be29d02276b0ce7390.png"></p>
<h1 id="EFK"><a href="#EFK" class="headerlink" title="EFK"></a>EFK</h1><p>此栏目已经迁移到<a href="https://huisebug.github.io/2019/05/08/gluster-heketi-efk/">gluster-heketi-efk</a></p>
<h1 id="Prometheus-operator"><a href="#Prometheus-operator" class="headerlink" title="Prometheus-operator "></a>Prometheus-operator </h1><p>此栏目已经迁移到<a href="https://huisebug.github.io/2019/08/27/Prometheus-Operator/">Prometheus-operator</a> </p>
<h1 id="Nginx-ingress"><a href="#Nginx-ingress" class="headerlink" title="Nginx-ingress"></a>Nginx-ingress</h1><p>同样的是整个集群的反向代理，可支持四层代理</p>
<p>此处安装是建立在prometheus-operator的基础上的。</p>
<p>直接下载官方的charts，然后找到traefik就可以执行安装了</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone &lt;https://github.com/helm/charts.git&gt;</span><br></pre></td></tr></table></figure>
<p>详细参数介绍请参考官方地址<br><a target="_blank" rel="noopener" href="https://github.com/helm/charts/tree/master/stable/nginx-ingress#configuration">https://github.com/helm/charts/tree/master/stable/nginx-ingress#configuration</a></p>
<p>具体的镜像下载</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker pull mirrorgooglecontainers/defaultbackend:1.4</span><br><span class="line">docker tag mirrorgooglecontainers/defaultbackend:1.4 k8s.gcr.io/defaultbackend:1.4</span><br><span class="line"></span><br><span class="line">docker pull huisebug/sec_re:nginx-ingress-controller-0.21.0</span><br><span class="line">docker tag huisebug/sec_re:nginx-ingress-controller-0.21.0 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0</span><br></pre></td></tr></table></figure>
<h2 id="安装nginx-ingress"><a href="#安装nginx-ingress" class="headerlink" title="安装nginx-ingress"></a>安装nginx-ingress</h2><p>首先需要删除traefik</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">helm delete traefik --purge</span><br><span class="line">helm delete nginx-ingress --purge</span><br></pre></td></tr></table></figure>
<p>官方的可配置参数各种坑不断，不推荐用命令行参数，直接修改values.yaml比较方便。下面是我修改的参数值：</p>
<ul>
<li>  部署类型为daemonset</li>
<li>  开启hostNetwork功能</li>
<li>  资源配额</li>
<li>  更改service type： LoadBalancer为ClusterIP</li>
<li>  rbac开启</li>
<li>  serviceAccount名称为nginx-ingress-controller</li>
<li>  开启metrics</li>
<li>  开启servicemonitor，设置namespace为monitoring便于prometheus-operator监控</li>
</ul>
<p>部署<br>参考我的valuesdaemonset.yaml文件地址：<br><a target="_blank" rel="noopener" href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/nginx-ingress/valuesdaemonset.yaml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/nginx-ingress/valuesdaemonset.yaml</a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helm delete nginx-ingress --purge</span><br><span class="line">helm install --name nginx-ingress --namespace kube-system /root/charts/stable/nginx-ingress </span><br><span class="line">-f /root/charts/stable/nginx-ingress/valuesdaemonset.yaml</span><br></pre></td></tr></table></figure>
<h2 id="验证效果-3"><a href="#验证效果-3" class="headerlink" title="验证效果"></a>验证效果</h2><p><img src="/2020/08/26/k8s/media/341628f03517a9fa7797699c65852ce4.png"></p>
<p>访问prometheus查看是否添加了nginx-ingress-controller的metrics</p>
<p><img src="/2020/08/26/k8s/media/28a8d1fc8edc5fad645b1dada6f8cc4d.png"></p>
<p>如何验证默认后端（nginx-ingress-default-backend）效果呢？</p>
<p>首先我们在本地hosts文件中建立一个整个k8s集群中没有定义ingress，不存在的域名并指向nginx-ingress-controller，访问测试即可</p>
<p><img src="/2020/08/26/k8s/media/58ed9d31362103f188537195c5a8dda3.png"></p>
<p>访问测试</p>
<p><img src="/2020/08/26/k8s/media/b6e69420019de9c1fcdc79edb8fc1af3.png"></p>
<h6 id><a href="#" class="headerlink" title></a></h6><p>也可以参考我已经修改好的charts，参考地址：<br><a target="_blank" rel="noopener" href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/nginx-ingress">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/nginx-ingress</a></p>
<h6 id="-1"><a href="#-1" class="headerlink" title></a></h6><h2 id="nginx-ingress-controller-nginx功能"><a href="#nginx-ingress-controller-nginx功能" class="headerlink" title="nginx-ingress-controller nginx功能"></a>nginx-ingress-controller nginx功能</h2><p>nginx-ingress-controller 并不像traefik一样提供WEB界面功能</p>
<p>nginx-ingress-controller-ingress.yaml<br>参考地址：<br><a target="_blank" rel="noopener" href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/nginx-ingress/ingress.yaml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/nginx-ingress/ingress.yaml</a></p>
<p>验证效果，这个返回的页面是nginx-ingress-controller返回的，并不是后端返回的</p>
<p><img src="/2020/08/26/k8s/media/c8bc815bef8e22e91dbb8e2779559c28.png"></p>
<h2 id="nginx-ingress-controller的四层代理"><a href="#nginx-ingress-controller的四层代理" class="headerlink" title="nginx-ingress-controller的四层代理"></a>nginx-ingress-controller的四层代理</h2><p>nginx从1.9.0开始，新增加了一个stream模块，用来实现四层协议的转发、代理或者负载均衡等。可以配置TCP或者UDP来实现这个功能</p>
<h3 id="代理集群的coredns"><a href="#代理集群的coredns" class="headerlink" title="代理集群的coredns"></a>代理集群的coredns</h3><p>只需要在values.yaml文件中增加UDP协议端口：coredns的service名：service_IP</p>
<p><img src="/2020/08/26/k8s/media/0c70598ca20e83a5f36f4444db4bdec4.png"></p>
<p>重新建立nginx-ingress-controller后，查看是否成功开启UDP 53端口</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -lnup | grep 53</span><br></pre></td></tr></table></figure>
<p><img src="/2020/08/26/k8s/media/821127b3b80f30705b270f49349d209f.png"></p>
<p>集群完整的域名如何获取？</p>
<p>我们使用一个pod去ping另一个pod的service名称，kube-dns会解析出完整的域名，就可以修改其中的service名和对应的namespace即可</p>
<p><img src="/2020/08/26/k8s/media/82e7d16fd70b1d760463183eeafd14c6.png"></p>
<p>验证效果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nslookup -qt=A kubernetes.default.svc.cluster.local 192.168.137.13</span><br><span class="line">nslookup -qt=A kube-dns.kube-system.svc.cluster.local 192.168.137.13</span><br><span class="line">nslookup -qt=A prometheus-k8s.monitoring.svc.cluster.local 192.168.137.13</span><br></pre></td></tr></table></figure>
<p>windows CMD验证</p>
<p><img src="/2020/08/26/k8s/media/7671a9d3a5beb90673783537c32d0cf7.png"></p>
<p>linux 验证</p>
<p><img src="/2020/08/26/k8s/media/e08988c2ffa569bce75b7972fcb2fac8.png"></p>
<h1 id="Metrics-server"><a href="#Metrics-server" class="headerlink" title="Metrics-server"></a>Metrics-server</h1><p>kubernetes Metrics Server是资源使用数据的集群范围聚合器，是Heapster的后继者。度量服务器通过汇集来自kubernetes.summary_api的数据来收集节点和pod的CPU和内存使用情况。摘要API是一种内存高效的API，用于将数据从Kubelet / cAdvisor传递到度量服务器。</p>
<p>从 v1.8 开始，资源使用情况的度量（如容器的 CPU 和内存使用）可以通过 Metrics API获取。注意：</p>
<ul>
<li>  Metrics API 只可以查询当前的度量数据，并不保存历史数据</li>
<li>  Metrics API URI 为 /apis/metrics.k8s.io/，在 k8s.io/metrics 维护</li>
<li>  必须部署 metrics-server 才能使用该 API，metrics-server 通过调用 Kubelet Summary API 获取数据</li>
<li>  在新版本的kubernetes中 Pod CPU使用率不在来源于heapster,而是来自于metrics-server</li>
<li>  支持metrics-server必须在api-server中添加如下参数</li>
</ul>
<p>设置apiserver相关参数</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">--requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem </span><br><span class="line">--proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.pem </span><br><span class="line">--proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client-key.pem </span><br><span class="line">--requestheader-allowed-names=aggregator </span><br><span class="line">--requestheader-group-headers=X-Remote-Group </span><br><span class="line">--requestheader-extra-headers-prefix=X-Remote-Extra- </span><br><span class="line">--requestheader-username-headers=X-Remote-User </span><br></pre></td></tr></table></figure>
<h2 id="部署metrics-server"><a href="#部署metrics-server" class="headerlink" title="部署metrics-server"></a>部署metrics-server</h2><p>此处将metrics-server放到prometheus-operator后面是因为在prometheus-operator建立的时候就建立了一个以prometheus-adapter为支持的<strong>api：metrics.k8s.io</strong>，这个api建立后就可以使用kubectl top命令了，但是prometheus-operator方式建立的<strong>api：metrics.k8s.io</strong>仅仅支持kubectltop pod，并不支持kubectl top node。<br>为了后续的<strong>api：custom.metrics.k8s.io</strong>使用prometheus-adapter来作服务支撑和使用metrics-server来建立<strong>api：metrics.k8s.io</strong>，我们需要移除prometheus-operator以prometheus-adapter为支持的<strong>api：metrics.k8s.io</strong></p>
<p>将prometheus-adapter的所有yaml文件归档到一个文件目录中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir prometheus-adapter</span><br><span class="line">mv prometheus-adapter-* prometheus-adapter/</span><br><span class="line">kubectl delete -f prometheus-adapter/</span><br></pre></td></tr></table></figure>
<p><img src="/2020/08/26/k8s/media/1eb6a6f5a131ff6287126ff61259575d.png"></p>
<p>然后部署metrics-server，具体部署yaml参考地址</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Metrics-server1.12/Metrics-server1.12.yaml</span><br></pre></td></tr></table></figure>
<h2 id="验证效果-4"><a href="#验证效果-4" class="headerlink" title="验证效果"></a>验证效果</h2><p>获取命令。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl api-versions | grep metrics.k8s.io</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl top pod --all-namespaces</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl top node</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/08/26/k8s/media/12031d891104780e561b9742041e8b3f.png"></p>
<p>查看nodes metrics：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get --raw &quot;/apis/metrics.k8s.io/v1beta1/nodes&quot; | jq .</span><br></pre></td></tr></table></figure>
<p><img src="/2020/08/26/k8s/media/d213f882d29ff203408d0021dcbc860c.png"></p>
<h1 id="HPA"><a href="#HPA" class="headerlink" title="HPA"></a>HPA</h1><p>此栏目已经迁移到<a href="https://huisebug.github.io/2019/08/28/k8s-hpa/">k8s-HPA</a> </p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">huisebug</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://huisebug.github.io/2020/08/26/k8s/">https://huisebug.github.io/2020/08/26/k8s/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://huisebug.github.io" target="_blank">huisebug</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/k8s/">k8s</a><a class="post-meta__tags" href="/tags/hpav2/">hpav2</a><a class="post-meta__tags" href="/tags/hpa/">hpa</a></div><div class="post_share"><div class="social-share" data-image="/img/head.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button button--animated"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/12/15/prometheus-operator%E5%91%8A%E8%AD%A6%E5%9C%BA%E6%99%AF/"><img class="prev-cover" src="/img/head.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Prometheus-Operator告警场景</div></div></a></div><div class="next-post pull-right"><a href="/2020/06/03/jenkins-CICD-k8s/"><img class="next-cover" src="/img/jenkins+k8s.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Jenkins持续集成到Kubernetes集群</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2019/08/28/k8s-hpa/" title="Kubernetes-hpav2横向自动扩容"><img class="cover" src="/img/head.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-08-28</div><div class="title">Kubernetes-hpav2横向自动扩容</div></div></a></div><div><a href="/2018/09/06/k8s常见方法及错误解决/" title="持续更新-20190306-k8s常见方法及错误解决"><img class="cover" src="/img/head.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-09-06</div><div class="title">持续更新-20190306-k8s常见方法及错误解决</div></div></a></div><div><a href="/2021/07/06/kata容器的一些分享/" title="kata容器的一些分享"><img class="cover" src="/img/kata.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-07-06</div><div class="title">kata容器的一些分享</div></div></a></div><div><a href="/2020/12/15/k8s高版本服务部署yaml/" title="kubernetes集群高版本常见服务部署yaml"><img class="cover" src="/img/head.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-15</div><div class="title">kubernetes集群高版本常见服务部署yaml</div></div></a></div><div><a href="/2020/12/15/prometheus-operator告警场景/" title="Prometheus-Operator告警场景"><img class="cover" src="/img/head.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-15</div><div class="title">Prometheus-Operator告警场景</div></div></a></div><div><a href="/2019/08/27/Prometheus-Operator/" title="Prometheus-Operator监控k8s"><img class="cover" src="/img/prometheus.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-08-27</div><div class="title">Prometheus-Operator监控k8s</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/linuxqie.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">huisebug</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">65</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/huisebug"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/huisebug" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:huisebug@aliyun.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">贯彻容器搬砖化 提供有偿技术支援 请联系QQ-1139873783</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A1%AC%E4%BB%B6%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="toc-number">1.</span> <span class="toc-text">硬件环境准备</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%BF%9B%E8%A1%8C%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE%E5%B9%B6%E5%BC%80%E5%90%AFIPVS"><span class="toc-number">2.</span> <span class="toc-text">进行系统配置并开启IPVS</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#GlusterFS"><span class="toc-number">3.</span> <span class="toc-text">GlusterFS</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85glusterfs"><span class="toc-number">3.1.</span> <span class="toc-text">安装glusterfs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE-glusterfs"><span class="toc-number">3.2.</span> <span class="toc-text">配置 glusterfs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0%E8%8A%82%E7%82%B9%E5%88%B0%E9%9B%86%E7%BE%A4"><span class="toc-number">3.3.</span> <span class="toc-text">添加节点到集群</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#volume-%E7%B1%BB%E5%9E%8B"><span class="toc-number">3.4.</span> <span class="toc-text">volume 类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B%E5%A4%8D%E5%88%B6%E5%8D%B7"><span class="toc-number">3.5.</span> <span class="toc-text">建立复制卷</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%85%8D%E9%A2%9D%E9%99%90%E5%88%B6"><span class="toc-number">3.6.</span> <span class="toc-text">配额限制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8C%82%E8%BD%BD%E4%BD%BF%E7%94%A8"><span class="toc-number">3.7.</span> <span class="toc-text">挂载使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E5%90%8C%E6%AD%A5%E6%95%88%E6%9E%9C"><span class="toc-number">3.8.</span> <span class="toc-text">测试同步效果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Keepalived"><span class="toc-number">4.</span> <span class="toc-text">Keepalived</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8Keepalived%E6%9C%8D%E5%8A%A1"><span class="toc-number">4.1.</span> <span class="toc-text">启动Keepalived服务</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%81%E4%B9%A6"><span class="toc-number">5.</span> <span class="toc-text">证书</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85cfssl"><span class="toc-number">5.1.</span> <span class="toc-text">安装cfssl</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAca%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">5.2.</span> <span class="toc-text">创建ca配置文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BACA%E8%AF%81%E4%B9%A6"><span class="toc-number">5.3.</span> <span class="toc-text">创建CA证书</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-CA-%E8%AF%81%E4%B9%A6%E7%AD%BE%E5%90%8D%E8%AF%B7%E6%B1%82"><span class="toc-number">5.3.1.</span> <span class="toc-text">创建 CA 证书签名请求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%BD%A3%E6%88%90-CA-%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%81%E9%92%A5ca-key-pem-ca-pem"><span class="toc-number">5.3.2.</span> <span class="toc-text">⽣成 CA 证书和私钥ca-key.pem ca.pem</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-kubernetes-%E8%AF%81%E4%B9%A6"><span class="toc-number">5.4.</span> <span class="toc-text">创建 kubernetes 证书</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-kubernetes-%E8%AF%81%E4%B9%A6%E7%AD%BE%E5%90%8D%E8%AF%B7%E6%B1%82%E2%BD%82%E4%BB%B6-kubernetes-csr-json"><span class="toc-number">5.4.1.</span> <span class="toc-text">创建 kubernetes 证书签名请求⽂件 kubernetes-csr.json</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%BD%A3%E6%88%90-kubernetes-%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%81%E9%92%A5kubernetes-key-pem-kubernetes-pem"><span class="toc-number">5.4.2.</span> <span class="toc-text">⽣成 kubernetes 证书和私钥kubernetes-key.pem kubernetes.pem</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAadmin%E8%AF%81%E4%B9%A6"><span class="toc-number">5.5.</span> <span class="toc-text">创建admin证书</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-admin-%E8%AF%81%E4%B9%A6%E7%AD%BE%E5%90%8D%E8%AF%B7%E6%B1%82%E2%BD%82%E4%BB%B6-admin-csr-json-%EF%BC%9A"><span class="toc-number">5.5.1.</span> <span class="toc-text">创建 admin 证书签名请求⽂件 admin-csr.json ：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%BD%A3%E6%88%90-admin-%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%81%E9%92%A5admin-key-pem-admin-pem"><span class="toc-number">5.5.2.</span> <span class="toc-text">⽣成 admin 证书和私钥admin-key.pem admin.pem</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-kube-controller-manager-%E8%AF%81%E4%B9%A6"><span class="toc-number">5.6.</span> <span class="toc-text">创建 kube-controller-manager 证书</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-kube-controller-manager-%E8%AF%81%E4%B9%A6%E7%AD%BE%E5%90%8D%E8%AF%B7%E6%B1%82%E2%BD%82%E4%BB%B6-kube-controller-manager-csr-json"><span class="toc-number">5.6.1.</span> <span class="toc-text">创建 kube-controller-manager 证书签名请求⽂件 kube-controller-manager-csr.json</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%BD%A3%E6%88%90kube-controller-manager%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%81%E9%92%A5kube-controller-manager-key-pem-kube-controller-manager-pem"><span class="toc-number">5.6.2.</span> <span class="toc-text">⽣成kube-controller-manager证书和私钥kube-controller-manager-key.pem kube-controller-manager.pem</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-kube-scheduler-%E8%AF%81%E4%B9%A6"><span class="toc-number">5.7.</span> <span class="toc-text">创建 kube-scheduler 证书</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAkube-scheduler-%E8%AF%81%E4%B9%A6%E7%AD%BE%E5%90%8D%E8%AF%B7%E6%B1%82%E2%BD%82%E4%BB%B6-kube-scheduler-csr-json"><span class="toc-number">5.7.1.</span> <span class="toc-text">创建kube-scheduler 证书签名请求⽂件 kube-scheduler-csr.json</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%BD%A3%E6%88%90kube-scheduler%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%81%E9%92%A5kube-scheduler-key-pem-kube-scheduler-pem"><span class="toc-number">5.7.2.</span> <span class="toc-text">⽣成kube-scheduler证书和私钥kube-scheduler-key.pem kube-scheduler.pem</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-front-proxy-%E8%AF%81%E4%B9%A6"><span class="toc-number">5.8.</span> <span class="toc-text">创建 front-proxy 证书</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-front-proxy-%E8%AF%81%E4%B9%A6%E7%AD%BE%E5%90%8D%E8%AF%B7%E6%B1%82%E2%BD%82%E4%BB%B6front-proxy-ca-csr-json"><span class="toc-number">5.8.1.</span> <span class="toc-text">创建 front-proxy 证书签名请求⽂件front-proxy-ca-csr.json</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90-front-proxy-ca%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%81%E9%92%A5front-proxy-ca-key-pem-front-proxy-ca-pem"><span class="toc-number">5.8.2.</span> <span class="toc-text">生成 front-proxy-ca证书和私钥front-proxy-ca-key.pem  front-proxy-ca.pem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-front-proxy-client-%E8%AF%81%E4%B9%A6%E7%AD%BE%E5%90%8D%E8%AF%B7%E6%B1%82%E2%BD%82%E4%BB%B6-front-proxy-client-csr-json"><span class="toc-number">5.8.3.</span> <span class="toc-text">创建 front-proxy-client 证书签名请求⽂件 front-proxy-client-csr.json</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%BD%A3%E6%88%90front-proxy-client%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%81%E9%92%A5front-proxy-client-key-pem-front-proxy-client-pem"><span class="toc-number">5.8.4.</span> <span class="toc-text">⽣成front-proxy-client证书和私钥front-proxy-client-key.pem  front-proxy-client.pem</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%A1%E9%AA%8C%E8%AF%81%E4%B9%A6"><span class="toc-number">5.9.</span> <span class="toc-text">校验证书</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E2%BD%A4-opsnssl-%E5%91%BD%E4%BB%A4"><span class="toc-number">5.9.1.</span> <span class="toc-text">使⽤ opsnssl 命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E2%BD%A4-cfssl-certinfo-%E5%91%BD%E4%BB%A4"><span class="toc-number">5.9.2.</span> <span class="toc-text">使⽤ cfssl-certinfo 命令</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%8F%91%E8%AF%81%E4%B9%A6"><span class="toc-number">5.10.</span> <span class="toc-text">分发证书</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Haproxy"><span class="toc-number">6.</span> <span class="toc-text">Haproxy</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85haproxy"><span class="toc-number">6.1.</span> <span class="toc-text">安装haproxy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A2%9E%E5%8A%A0keepalived%E9%85%8D%E7%BD%AE%EF%BC%8C%E4%BD%BF%E5%85%B6%E5%97%85%E6%8E%A2haproxy%E7%9A%84%E7%8A%B6%E6%80%81"><span class="toc-number">6.2.</span> <span class="toc-text">增加keepalived配置，使其嗅探haproxy的状态</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#kubectl"><span class="toc-number">7.</span> <span class="toc-text">kubectl</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-kubectl%E6%89%80%E9%9C%80kubeconfig%E6%96%87%E4%BB%B6"><span class="toc-number">7.1.</span> <span class="toc-text">创建 kubectl所需kubeconfig文件</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#k8s%E9%9B%86%E7%BE%A4%E5%BB%BA%E7%AB%8B%E5%89%8D%E9%A2%84%E5%A4%87%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">8.</span> <span class="toc-text">k8s集群建立前预备配置文件</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-TLS-Bootstrapping-token%E6%96%87%E4%BB%B6"><span class="toc-number">8.1.</span> <span class="toc-text">创建 TLS Bootstrapping token文件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Token-auth-file"><span class="toc-number">8.1.1.</span> <span class="toc-text">Token auth file</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E6%9C%8D%E5%8A%A1%E9%9C%80%E8%A6%81%E7%9A%84kubeconfig%E6%96%87%E4%BB%B6"><span class="toc-number">8.2.</span> <span class="toc-text">创建服务需要的kubeconfig文件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-kubelet%E6%9C%8D%E5%8A%A1%E6%89%80%E9%9C%80-bootstrapping-kubeconfig%E6%96%87%E4%BB%B6"><span class="toc-number">8.2.1.</span> <span class="toc-text">创建 kubelet服务所需 bootstrapping.kubeconfig文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-kubelet%E6%9C%8D%E5%8A%A1%E6%89%80%E9%9C%80-kubelet-kubeconfig%E6%96%87%E4%BB%B6"><span class="toc-number">8.2.2.</span> <span class="toc-text">创建 kubelet服务所需 kubelet.kubeconfig文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-kube-controller-manager%E6%9C%8D%E5%8A%A1%E6%89%80%E9%9C%80kube-controller-manager-kubeconfig-%E2%BD%82%E4%BB%B6"><span class="toc-number">8.2.3.</span> <span class="toc-text">创建 kube-controller-manager服务所需kube-controller-manager.kubeconfig ⽂件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-kube-scheduler%E6%9C%8D%E5%8A%A1%E6%89%80%E9%9C%80kube-scheduler-kubeconfig-%E2%BD%82%E4%BB%B6"><span class="toc-number">8.2.4.</span> <span class="toc-text">创建 kube-scheduler服务所需kube-scheduler.kubeconfig ⽂件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC%E6%96%87%E4%BB%B6%E7%94%9F%E6%88%90kubeconfig%E6%96%87%E4%BB%B6"><span class="toc-number">8.2.5.</span> <span class="toc-text">执行脚本文件生成kubeconfig文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%8F%91-kubeconfig-%E2%BD%82%E4%BB%B6"><span class="toc-number">8.2.6.</span> <span class="toc-text">分发 kubeconfig ⽂件</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Etcd3-3-4"><span class="toc-number">9.</span> <span class="toc-text">Etcd3.3.4</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#TLS-%E8%AE%A4%E8%AF%81%E2%BD%82%E4%BB%B6"><span class="toc-number">9.1.</span> <span class="toc-text">TLS 认证⽂件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ectd%E6%9C%8D%E5%8A%A1services%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">9.2.</span> <span class="toc-text">ectd服务services配置文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE%E2%BD%82%E4%BB%B6"><span class="toc-number">9.3.</span> <span class="toc-text">环境变量配置⽂件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8etcd%E6%9C%8D%E5%8A%A1"><span class="toc-number">9.4.</span> <span class="toc-text">启动etcd服务</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#kube-apiserver"><span class="toc-number">10.</span> <span class="toc-text">kube-apiserver</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-kube-apiserver%E7%9A%84%E6%9C%8D%E5%8A%A1%E9%85%8D%E7%BD%AE%E2%BD%82%E4%BB%B6"><span class="toc-number">10.1.</span> <span class="toc-text">创建 kube-apiserver的服务配置⽂件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#apiserver%E6%9C%8D%E5%8A%A1%E9%85%8D%E7%BD%AE%E2%BD%82%E4%BB%B6-etc-kubernetes-apiserver-%E5%86%85%E5%AE%B9"><span class="toc-number">10.2.</span> <span class="toc-text">apiserver服务配置⽂件 &#x2F;etc&#x2F;kubernetes&#x2F;apiserver 内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8kube-apiserver"><span class="toc-number">10.3.</span> <span class="toc-text">启动kube-apiserver</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#kube-controller-manager"><span class="toc-number">11.</span> <span class="toc-text">kube-controller-manager</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-kube-controller-manager%E6%9C%8D%E5%8A%A1%E9%85%8D%E7%BD%AE%E2%BD%82%E4%BB%B6"><span class="toc-number">11.1.</span> <span class="toc-text">创建 kube-controller-manager服务配置⽂件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#controller-manager%E6%9C%8D%E5%8A%A1%E9%85%8D%E7%BD%AE%E2%BD%82%E4%BB%B6-etc-kubernetes-controller-manager"><span class="toc-number">11.2.</span> <span class="toc-text">controller-manager服务配置⽂件 &#x2F;etc&#x2F;kubernetes&#x2F;controller-manager </span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8-kube-controller-manager"><span class="toc-number">11.3.</span> <span class="toc-text">启动 kube-controller-manager</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#kube-scheduler"><span class="toc-number">12.</span> <span class="toc-text">kube-scheduler</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-kube-scheduler%E6%9C%8D%E5%8A%A1%E9%85%8D%E7%BD%AE%E2%BD%82%E4%BB%B6"><span class="toc-number">12.1.</span> <span class="toc-text">创建 kube-scheduler服务配置⽂件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#scheduler%E6%9C%8D%E5%8A%A1%E9%85%8D%E7%BD%AE%E2%BD%82%E4%BB%B6-etc-kubernetes-scheduler"><span class="toc-number">12.2.</span> <span class="toc-text">scheduler服务配置⽂件 &#x2F;etc&#x2F;kubernetes&#x2F;scheduler </span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8-kube-scheduler"><span class="toc-number">12.3.</span> <span class="toc-text">启动 kube-scheduler</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%87%B3%E6%AD%A4Master%E4%B8%89%E5%A4%A7%E7%BB%84%E4%BB%B6%E5%AE%89%E8%A3%85%E5%AE%8C%E6%88%90"><span class="toc-number">13.</span> <span class="toc-text">至此Master三大组件安装完成</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Docker"><span class="toc-number">14.</span> <span class="toc-text">Docker</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85docker"><span class="toc-number">14.1.</span> <span class="toc-text">安装docker</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%96%E8%BE%91systemctl%E7%9A%84Docker%E5%90%AF%E5%8A%A8%E6%96%87%E4%BB%B6"><span class="toc-number">14.2.</span> <span class="toc-text">编辑systemctl的Docker启动文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9docker%E5%8F%82%E6%95%B0"><span class="toc-number">14.3.</span> <span class="toc-text">修改docker参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8docker"><span class="toc-number">14.4.</span> <span class="toc-text">启动docker</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B%E4%B8%8D%E5%8A%A0%E5%AF%86%E7%9A%84docker%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E6%96%B9%E5%BC%8F"><span class="toc-number">15.</span> <span class="toc-text">建立不加密的docker私有仓库方式</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E7%A7%81%E5%BA%93"><span class="toc-number">15.1.</span> <span class="toc-text">建立最简单的私库</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#kubelet"><span class="toc-number">16.</span> <span class="toc-text">kubelet</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-kubelet%E6%9C%8D%E5%8A%A1%E9%85%8D%E7%BD%AE%E2%BD%82%E4%BB%B6"><span class="toc-number">16.1.</span> <span class="toc-text">创建 kubelet服务配置⽂件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kubelet%E6%9C%8D%E5%8A%A1%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-etc-kubernetes-kubelet"><span class="toc-number">16.2.</span> <span class="toc-text">kubelet服务配置文件&#x2F;etc&#x2F;kubernetes&#x2F;kubelet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8kubelet"><span class="toc-number">16.3.</span> <span class="toc-text">启动kubelet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A3%80%E6%9F%A5node%E7%8A%B6%E6%80%81"><span class="toc-number">16.4.</span> <span class="toc-text">检查node状态</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#kube-proxy"><span class="toc-number">17.</span> <span class="toc-text">kube-proxy</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%87%B3%E6%AD%A4Node%E4%B8%89%E5%A4%A7%E7%BB%84%E4%BB%B6%E5%AE%89%E8%A3%85%E5%AE%8C%E6%88%90"><span class="toc-number">18.</span> <span class="toc-text">至此Node三大组件安装完成</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Calico"><span class="toc-number">19.</span> <span class="toc-text">Calico</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E8%A6%81%E6%B1%82%EF%BC%8C%E5%8E%9F%E6%89%93%E7%AE%97%E6%98%AF%E4%BD%BF%E7%94%A83-4-0%E7%89%88%E6%9C%AC%EF%BC%8C%E5%A5%88%E4%BD%95%E5%A4%B1%E8%B4%A5%E4%BA%86%E3%80%82"><span class="toc-number">19.1.</span> <span class="toc-text">环境要求，原打算是使用3.4.0版本，奈何失败了。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E4%B8%8D%E5%90%8C%E8%8A%82%E7%82%B9%E7%9A%84%E5%AE%B9%E5%99%A8%E4%B9%8B%E9%97%B4%E8%83%BD%E5%90%A6ping%E9%80%9A"><span class="toc-number">19.2.</span> <span class="toc-text">验证不同节点的容器之间能否ping通</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E8%83%BD%E5%90%A6%E8%AE%BF%E9%97%AE%E5%A4%96%E7%BD%91"><span class="toc-number">19.3.</span> <span class="toc-text">验证能否访问外网</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB"><span class="toc-number">19.4.</span> <span class="toc-text">问题汇总</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Coredns"><span class="toc-number">20.</span> <span class="toc-text">Coredns</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E6%95%88%E6%9E%9C"><span class="toc-number">20.1.</span> <span class="toc-text">验证效果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%86%8D%E6%AC%A1%E9%AA%8C%E8%AF%81%E6%95%B4%E4%B8%AA%E9%9B%86%E7%BE%A4%E7%9A%84proxy%E8%B4%9F%E8%BD%BD%E6%9C%BA%E5%88%B6%E6%95%88%E6%9E%9C"><span class="toc-number">21.</span> <span class="toc-text">再次验证整个集群的proxy负载机制效果</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Helm"><span class="toc-number">22.</span> <span class="toc-text">Helm</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Helm-client%E5%AE%89%E8%A3%85"><span class="toc-number">22.1.</span> <span class="toc-text">Helm client安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85socat"><span class="toc-number">22.2.</span> <span class="toc-text">安装socat</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Helm-server%E5%AE%89%E8%A3%85"><span class="toc-number">22.3.</span> <span class="toc-text">Helm server安装</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Traefik"><span class="toc-number">23.</span> <span class="toc-text">Traefik</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85traefik"><span class="toc-number">23.1.</span> <span class="toc-text">安装traefik</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E6%95%88%E6%9E%9C-1"><span class="toc-number">23.2.</span> <span class="toc-text">验证效果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Dashboard"><span class="toc-number">24.</span> <span class="toc-text">Dashboard</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BF%E9%97%AE%E9%AA%8C%E8%AF%81"><span class="toc-number">24.1.</span> <span class="toc-text">访问验证</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B4%E6%8E%A5https%E8%AE%BF%E9%97%AE%E6%96%B9%E5%BC%8F"><span class="toc-number">24.1.1.</span> <span class="toc-text">直接https访问方式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%BC%E2%BC%8A%E8%AF%81%E4%B9%A6"><span class="toc-number">24.1.1.1.</span> <span class="toc-text">导⼊证书</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BF%E9%97%AE"><span class="toc-number">24.1.1.2.</span> <span class="toc-text">访问</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ingress%E6%96%B9%E5%BC%8F"><span class="toc-number">24.1.2.</span> <span class="toc-text">ingress方式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E8%AE%BF%E9%97%AE%E6%95%88%E6%9E%9C"><span class="toc-number">24.1.2.1.</span> <span class="toc-text">验证访问效果</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Scope%E7%9B%91%E6%8E%A7"><span class="toc-number">25.</span> <span class="toc-text">Scope监控</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85scope"><span class="toc-number">25.1.</span> <span class="toc-text">安装scope</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9A%B4%E9%9C%B2%E8%AE%BF%E9%97%AE%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F"><span class="toc-number">25.2.</span> <span class="toc-text">暴露访问两种方式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Service%E7%9A%84nodeport%E6%96%B9%E5%BC%8F"><span class="toc-number">25.2.1.</span> <span class="toc-text">Service的nodeport方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Traefik%E4%BB%A3%E7%90%86"><span class="toc-number">25.2.2.</span> <span class="toc-text">Traefik代理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E6%95%88%E6%9E%9C-2"><span class="toc-number">25.3.</span> <span class="toc-text">验证效果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#EFK"><span class="toc-number">26.</span> <span class="toc-text">EFK</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Prometheus-operator"><span class="toc-number">27.</span> <span class="toc-text">Prometheus-operator </span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Nginx-ingress"><span class="toc-number">28.</span> <span class="toc-text">Nginx-ingress</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85nginx-ingress"><span class="toc-number">28.1.</span> <span class="toc-text">安装nginx-ingress</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E6%95%88%E6%9E%9C-3"><span class="toc-number">28.2.</span> <span class="toc-text">验证效果</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link"><span class="toc-number">28.2.0.0.0.1.</span> <span class="toc-text"></span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#-1"><span class="toc-number">28.2.0.0.0.2.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#nginx-ingress-controller-nginx%E5%8A%9F%E8%83%BD"><span class="toc-number">28.3.</span> <span class="toc-text">nginx-ingress-controller nginx功能</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#nginx-ingress-controller%E7%9A%84%E5%9B%9B%E5%B1%82%E4%BB%A3%E7%90%86"><span class="toc-number">28.4.</span> <span class="toc-text">nginx-ingress-controller的四层代理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%90%86%E9%9B%86%E7%BE%A4%E7%9A%84coredns"><span class="toc-number">28.4.1.</span> <span class="toc-text">代理集群的coredns</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Metrics-server"><span class="toc-number">29.</span> <span class="toc-text">Metrics-server</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2metrics-server"><span class="toc-number">29.1.</span> <span class="toc-text">部署metrics-server</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E6%95%88%E6%9E%9C-4"><span class="toc-number">29.2.</span> <span class="toc-text">验证效果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HPA"><span class="toc-number">30.</span> <span class="toc-text">HPA</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/05/21/vector+clickhouse/" title="vector+clickhouse:新型kubernetes集群日志收集方案"><img src="/img/vector+clickhouse.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="vector+clickhouse:新型kubernetes集群日志收集方案"/></a><div class="content"><a class="title" href="/2024/05/21/vector+clickhouse/" title="vector+clickhouse:新型kubernetes集群日志收集方案">vector+clickhouse:新型kubernetes集群日志收集方案</a><time datetime="2024-05-21T09:04:01.000Z" title="发表于 2024-05-21 17:04:01">2024-05-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/02/04/terraform-kubernetes/" title="terraform部署kubernetes1.26.12集群"><img src="/img/terraform-kubernetes.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="terraform部署kubernetes1.26.12集群"/></a><div class="content"><a class="title" href="/2024/02/04/terraform-kubernetes/" title="terraform部署kubernetes1.26.12集群">terraform部署kubernetes1.26.12集群</a><time datetime="2024-02-04T08:04:01.000Z" title="发表于 2024-02-04 16:04:01">2024-02-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/02/04/terraform-k8s-golangstruct/" title="golang terraform kubernetes结构体"><img src="/img/terraform-kubernetes.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="golang terraform kubernetes结构体"/></a><div class="content"><a class="title" href="/2024/02/04/terraform-k8s-golangstruct/" title="golang terraform kubernetes结构体">golang terraform kubernetes结构体</a><time datetime="2024-02-04T03:04:01.000Z" title="发表于 2024-02-04 11:04:01">2024-02-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/08/03/kubernetesalias/" title="kubernetesalias:kubernetes封装命令工具"><img src="/img/kubernetesalias.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="kubernetesalias:kubernetes封装命令工具"/></a><div class="content"><a class="title" href="/2023/08/03/kubernetesalias/" title="kubernetesalias:kubernetes封装命令工具">kubernetesalias:kubernetes封装命令工具</a><time datetime="2023-08-03T08:04:01.000Z" title="发表于 2023-08-03 16:04:01">2023-08-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/28/kube-prometheus-thanos/" title="kube-prometheus+thanos 多集群prometheus方案存储方案"><img src="/img/kube-prometheus-thanos.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="kube-prometheus+thanos 多集群prometheus方案存储方案"/></a><div class="content"><a class="title" href="/2023/06/28/kube-prometheus-thanos/" title="kube-prometheus+thanos 多集群prometheus方案存储方案">kube-prometheus+thanos 多集群prometheus方案存储方案</a><time datetime="2023-06-28T03:04:01.000Z" title="发表于 2023-06-28 11:04:01">2023-06-28</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/head.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By huisebug</div><div class="footer_custom_text">Good Luck</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>