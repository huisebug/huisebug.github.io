<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>huisebug</title>
  
  
  <link href="https://huisebug.github.io/atom.xml" rel="self"/>
  
  <link href="https://huisebug.github.io/"/>
  <updated>2023-02-07T07:04:01.000Z</updated>
  <id>https://huisebug.github.io/</id>
  
  <author>
    <name>huisebug</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>k8simage-operator改良版kube-deploymentimage</title>
    <link href="https://huisebug.github.io/2023/02/07/kube-deploymentimage/"/>
    <id>https://huisebug.github.io/2023/02/07/kube-deploymentimage/</id>
    <published>2023-02-07T03:04:01.000Z</published>
    <updated>2023-02-07T07:04:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>为kubernetes集群中，由deployment方式进行部署的服务提供版本回滚功能</p><span id="more"></span><h1 id="需求场景"><a href="#需求场景" class="headerlink" title="需求场景"></a>需求场景</h1><ol><li><p> 当master节点上存放的yaml无法在使用kubect set image时进行同步更新</p></li><li><p> 官方提供的—record参数仅可能在kubectl set image时使用，回滚还需要额外的回滚操作命令，没有统一的UI界面</p></li></ol><h1 id="功能描述"><a href="#功能描述" class="headerlink" title="功能描述"></a>功能描述</h1><ol><li><p> 提供展示部署后所有命名空间下的Deployment的容器镜像信息</p></li><li><p> 提供回滚到对应版本的按钮update</p></li><li><p> 提供本地存储的yaml文件预览（鼠标滑动到对应的yaml路径即可）</p></li><li><p> 清理不需要的镜像版本信息delete（假删除，数据库中还有记录）</p></li><li><p> 处理之前k8simage-operator中Create,Update event多次触发Reconcile方法</p></li></ol><h1 id="UI效果预览"><a href="#UI效果预览" class="headerlink" title="UI效果预览"></a>UI效果预览</h1><p><img src="/2023/02/07/kube-deploymentimage/media/kube-deployment-ui.png"></p><h1 id="部署要求"><a href="#部署要求" class="headerlink" title="部署要求"></a>部署要求</h1><p>kube-deploymentimage默认会统计集群中所有的Deployment，yamlfile文件在部署kube-deploymentimage容器时，此处默认挂载了/etc/kubernetes目录到容器中的/etc/kubernetes，使用nodeselector默认调度到master1，即存放yaml的机器。</p><p>想展示yaml的本地文件内容，需要在deployment中添加  annotations信息，例如：   </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">    version: v1</span><br><span class="line">  annotations:    </span><br><span class="line">    yamlfile.huisebug.io/yamlfile: /etc/kubernetes/nginx/test-k8s-nginx.yaml</span><br><span class="line">  name: nginxredis</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>yamlfile.huisebug.io/yamlfile是固定字段，后面是对应的yaml的文件路径（注意：挂载到kube-deploymentimage后路径是容器中的路径）</p><h1 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h1><p><a href="https://github.com/huisebug/kube-deploymentimage.git">https://github.com/huisebug/kube-deploymentimage.git</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;为kubernetes集群中，由deployment方式进行部署的服务提供版本回滚功能&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes-Dev" scheme="https://huisebug.github.io/categories/Kubernetes-Dev/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="kubebuilder" scheme="https://huisebug.github.io/tags/kubebuilder/"/>
    
    <category term="kubernetes" scheme="https://huisebug.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>logfile-operator:服务日志系统收集多方案</title>
    <link href="https://huisebug.github.io/2022/12/09/logfile-operator/"/>
    <id>https://huisebug.github.io/2022/12/09/logfile-operator/</id>
    <published>2022-12-09T03:04:01.000Z</published>
    <updated>2022-12-09T03:04:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>为kubernetes集群中部署服务提供多日志收集服务，快速接入多种部署方案的日志系统</p><span id="more"></span><h1 id="需求场景"><a href="#需求场景" class="headerlink" title="需求场景"></a>需求场景</h1><ul><li><p>   当pod中有多个容器时，需要对pod中的容器进行日志收集</p></li><li><p>   研发人员需要快速对服务进行日志收集后在kibana中集中查看</p></li></ul><h1 id="功能描述"><a href="#功能描述" class="headerlink" title="功能描述"></a>功能描述</h1><ul><li>  利用kubernetes pod注入对运行的服务进行注入容器，注入的容器和原容器在pod annotations配置的日志文件所在目录进行卷挂载，注解中配置的日志文件进行卷挂载，例如：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      annotations:</span><br><span class="line">        logfile.huisebug.org/log1: /var/log/nginx/*.log</span><br></pre></td></tr></table></figure></li><li>  提供六种日志收集部署方案</li></ul><ol><li><p> filebeat+elasticsearch+kibana</p></li><li><p> filebeat+elasticsearch-cluster+kibana</p></li><li><p> filebeat+logstash+elasticsearch+kibana</p></li><li><p> filebeat+logstash+elasticsearch-cluster+kibana</p></li><li><p> filebeat+kafka+logstash+elasticsearch+kibana</p></li><li><p> filebeat+kafka-cluster+logstash+elasticsearch-cluster+kibana</p></li></ol><p>版本信息：<br>filebeat：8.5.0<br>logstash：8.5.0<br>kafka：3.3<br>zookeeper：3.8<br>elasticsearch：8.5.0<br>kibana：8.5.0</p><p>注意：elasticsearch集群这里采用的https集群</p><ul><li>  git项目首页提供的num的6个yaml文件对应了六种方案</li></ul><h1 id="部署命令（按顺序执行）"><a href="#部署命令（按顺序执行）" class="headerlink" title="部署命令（按顺序执行）"></a>部署命令（按顺序执行）</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f cert-manager.yam</span><br><span class="line">kubectl apply -f deploy.yaml</span><br><span class="line">kubectl apply -f num数字.yaml</span><br></pre></td></tr></table></figure><h1 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h1><p><a href="https://github.com/huisebug/logfile-operator.git">https://github.com/huisebug/logfile-operator.git</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;为kubernetes集群中部署服务提供多日志收集服务，快速接入多种部署方案的日志系统&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes-Dev" scheme="https://huisebug.github.io/categories/Kubernetes-Dev/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="elasticsearch" scheme="https://huisebug.github.io/tags/elasticsearch/"/>
    
    <category term="kubebuilder" scheme="https://huisebug.github.io/tags/kubebuilder/"/>
    
    <category term="kubernetes" scheme="https://huisebug.github.io/tags/kubernetes/"/>
    
    <category term="logstash" scheme="https://huisebug.github.io/tags/logstash/"/>
    
    <category term="filebeat" scheme="https://huisebug.github.io/tags/filebeat/"/>
    
    <category term="kafka-cluster" scheme="https://huisebug.github.io/tags/kafka-cluster/"/>
    
    <category term="kafka" scheme="https://huisebug.github.io/tags/kafka/"/>
    
    <category term="zookeeper-cluster" scheme="https://huisebug.github.io/tags/zookeeper-cluster/"/>
    
    <category term="elasticsearch-cluster" scheme="https://huisebug.github.io/tags/elasticsearch-cluster/"/>
    
  </entry>
  
  <entry>
    <title>k8s版本回滚工具k8simage-operator</title>
    <link href="https://huisebug.github.io/2022/06/16/k8simage-operator/"/>
    <id>https://huisebug.github.io/2022/06/16/k8simage-operator/</id>
    <published>2022-06-16T03:04:01.000Z</published>
    <updated>2022-06-16T03:04:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>为kubernetes集群部署提供版本回滚功能</p><span id="more"></span><h1 id="需求场景"><a href="#需求场景" class="headerlink" title="需求场景"></a>需求场景</h1><ol><li><p> 当master节点上存放的yaml无法在使用kubect set image时进行同步更新</p></li><li><p> 官方提供的—record参数仅可能在kubectl set image时使用，回滚还需要额外的回滚操作命令，没有统一的UI界面</p></li></ol><h1 id="功能描述"><a href="#功能描述" class="headerlink" title="功能描述"></a>功能描述</h1><ol><li><p> 提供展示部署后所有命名空间下的Deployment的容器镜像信息</p></li><li><p> 提供回滚到对应版本的按钮update</p></li><li><p> 提供本地存储的yaml文件预览（鼠标滑动到对应的yaml路径即可）</p></li><li><p> 清理不需要的镜像版本信息delete（假删除，数据库中还有记录）</p></li></ol><h1 id="UI效果预览"><a href="#UI效果预览" class="headerlink" title="UI效果预览"></a>UI效果预览</h1><p><img src="/2022/06/16/k8simage-operator/media/3a30ddbcf61f5ef4340432b7ff10f67a.png"></p><h1 id="部署要求"><a href="#部署要求" class="headerlink" title="部署要求"></a>部署要求</h1><p>K8simage-operator默认会统计集群中所有的Deployment，yamlfile文件在部署k8simage-operator容器时，此处默认挂载了/etc/kubernetes目录到容器中的/etc/kubernetes，使用nodeselector默认调度到master1，即存放yaml的机器。</p><p>想展示yaml的本地文件内容，需要在deployment中添加  annotations信息，例如：   </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">    version: v1</span><br><span class="line">  annotations:    </span><br><span class="line">    yamlfile.huisebug.io/yamlfile: /etc/kubernetes/nginx/test-k8s-nginx.yaml</span><br><span class="line">  name: nginxredis</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>yamlfile.huisebug.io/yamlfile是固定字段，后面是对应的yaml的文件路径（注意：挂载到k8simage-operator后路径是容器中的路径）</p><h1 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h1><p><a href="https://github.com/huisebug/k8simage-operator.git">https://github.com/huisebug/k8simage-operator.git</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;为kubernetes集群部署提供版本回滚功能&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes-Dev" scheme="https://huisebug.github.io/categories/Kubernetes-Dev/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="kubebuilder" scheme="https://huisebug.github.io/tags/kubebuilder/"/>
    
    <category term="kubernetes" scheme="https://huisebug.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>greenplum集群操作指令及解决方法</title>
    <link href="https://huisebug.github.io/2021/09/10/greenplum%E9%9B%86%E7%BE%A4%E6%93%8D%E4%BD%9C%E6%8C%87%E4%BB%A4%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"/>
    <id>https://huisebug.github.io/2021/09/10/greenplum%E9%9B%86%E7%BE%A4%E6%93%8D%E4%BD%9C%E6%8C%87%E4%BB%A4%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</id>
    <published>2021-09-10T02:04:01.000Z</published>
    <updated>2021-09-10T02:04:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>gp集群如何部署就不过多介绍，主要是结合一下工作中要用到的指令和遇到的一些坑</p><span id="more"></span><h1 id="操作规范"><a href="#操作规范" class="headerlink" title="操作规范"></a>操作规范</h1><p>gp集群的所有操作都是由master节点进行的,由master机器ssh到其他节点上进行自动化操作</p><p>安装部署gp集群时已经给master角色机器和standby角色机器添加了gpadmin用户.bashrc脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;&quot;&quot;</span><br><span class="line">source /usr/local/greenplum-db/greenplum_path.sh</span><br><span class="line">export MASTER_DATA_DIRECTORY=/data/master/gpseg-1</span><br><span class="line">&quot;&quot;&quot; &gt;&gt; /home/gpadmin/.bashrc</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>必须在gpadmin用户下进行,登录master机器后需要切换到gpadmin用户下</p><p>su – gpadmin</p><p>下面的所有操作都是在gpadmin用户下的操作</p><h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><p>恢复mirror segment (primary复制到mirror)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gprecoverseg -qo ./recov</span><br><span class="line">gprecoverseg -i ./recov</span><br></pre></td></tr></table></figure><p>恢复到原来初始化时的角色设置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gprecoverseg -r</span><br></pre></td></tr></table></figure><p>恢复primary segment (mirror复制到primary)</p><p>快速恢复镜像，可以多次执行，让集群中的mirror端口7000的数据同步到primary端口6000，可以用于segment节点宕机后重新启动，期间可能修复失败，多次执行修复即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gprecoverseg -av #不会创建primary数据目录</span><br></pre></td></tr></table></figure><p>强制恢复</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gprecoverseg -Fav #会创建primary数据目录，（相当于把mirror的整个文件目录给复制到primary目录）</span><br></pre></td></tr></table></figure><p>启动master节点和segment</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpstart -a</span><br></pre></td></tr></table></figure><p>显示具有镜像状态问题的片段，如果集群正常则显示为running，可用于健康检查</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpstate -e</span><br></pre></td></tr></table></figure><p>查看端口分配情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpstate -p</span><br></pre></td></tr></table></figure><p>查看集群中的角色</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpstate -b</span><br></pre></td></tr></table></figure><p>显示主镜像mirror映射</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpstate -c</span><br></pre></td></tr></table></figure><p>显示镜像实例同步状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpstate -m</span><br></pre></td></tr></table></figure><p>停止所有实例，然后重启系统</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpstop -M fast -ar</span><br></pre></td></tr></table></figure><p>停止集群，包含master和segment</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpstop -a</span><br></pre></td></tr></table></figure><p>激活standby为master，在standby机器上执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpactivatestandby -a</span><br></pre></td></tr></table></figure><h1 id="Master角色故障转移"><a href="#Master角色故障转移" class="headerlink" title="Master角色故障转移"></a>Master角色故障转移</h1><p>问题场景：</p><p>Master机器故障</p><p>修复方法：</p><p>master机器故障时，无法自动故障转移，需要将standby机器升级为master角色，原有的master机器会被踢出集群master角色，此时集群中只有master角色(standby机器)，没有standby角色。</p><p>操作命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">登录standby机器</span></span><br><span class="line">su – gpadmin</span><br><span class="line">gpactivatestandby -a</span><br></pre></td></tr></table></figure><p>要想master机器重新成为master角色，需要移除master机器的MASTER_DATA_DIRECTORY目录数据(rm -rf $MASTER_DATA_DIRECTORY)或者备份，然后在standby机器上操作命令：gpinitstandby -s master机器； 让master机器重新以standby角色加入集群：</p><p>操作命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">登录master机器</span></span><br><span class="line">rm -rf $MASTER_DATA_DIRECTORY</span><br><span class="line"><span class="meta">#</span><span class="bash">登录standby机器</span></span><br><span class="line">su – gpadmin</span><br><span class="line">gpinitstandby -s master机器主机名</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>操作完成后，master机器就是集群中的standby角色; 经过gpstate -b确认master机器成为了集群的standby角色，然后停止standby机器(pg_ctl stop -D $MASTER_DATA_DIRECTORY)，在master机器上执行gpactivatestandby -a命令，抢夺standby机器的master角色并踢出集群，当master机器重新成为master角色后，再移除standby机器的$MASTER_DATA_DIRECTORY目录数据<br>然后再master机器上执行命令：gpinitstandby -s standby机器。让其成为standby角色。<br>操作命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">登录standby机器</span></span><br><span class="line">su – gpadmin</span><br><span class="line">pg_ctl stop -D $MASTER_DATA_DIRECTORY</span><br><span class="line">rm -rf $MASTER_DATA_DIRECTORY</span><br><span class="line"><span class="meta">#</span><span class="bash">登录master机器</span></span><br><span class="line">su – gpadmin</span><br><span class="line">gpactivatestandby -a</span><br><span class="line">gpinitstandby -s standby机器主机名</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="Segment角色故障修复"><a href="#Segment角色故障修复" class="headerlink" title="Segment角色故障修复"></a>Segment角色故障修复</h1><p>问题场景</p><p>segment节点机器重启</p><p>修复方法：</p><p>gp集群自带segment故障转移，故障segment节点的primary业务转移其对应的mirror业务上（一般是其他的segment节点上），此时不影响集群运行。</p><p>gp提供了segment节点修复工具gprecoverseg</p><p>当整个集群正常状态下，primary提供数据读写，mirror提供备份功能，当primary异常时，mirror接替primary的工作。使用命令gpstate -c命令进行查看</p><p>例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Status                             Data State     Primary        Datadir           Port   Mirror         Datadir           Port</span><br><span class="line">Primary Active, Mirror Available   Synchronized   gp-mdw1-sdw1   /data/p1/gpseg0   6000   gp-mdw2-sdw2   /data/m1/gpseg0   7000</span><br><span class="line">Primary Active, Mirror Available   Synchronized   gp-mdw1-sdw1   /data/p2/gpseg1   6001   gp-sdw3        /data/m2/gpseg1   7001</span><br><span class="line">Primary Active, Mirror Available   Synchronized   gp-mdw2-sdw2   /data/p1/gpseg2   6000   gp-sdw3        /data/m1/gpseg2   7000</span><br><span class="line">Primary Active, Mirror Available   Synchronized   gp-mdw2-sdw2   /data/p2/gpseg3   6001   gp-mdw1-sdw1   /data/m2/gpseg3   7001</span><br><span class="line">Primary Active, Mirror Available   Synchronized   gp-sdw3        /data/p1/gpseg4   6000   gp-mdw1-sdw1   /data/m1/gpseg4   7000</span><br><span class="line">Primary Active, Mirror Available   Synchronized   gp-sdw3        /data/p2/gpseg5   6001   gp-mdw2-sdw2   /data/m2/gpseg5   7001</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>方法1：</p><p>如果发现上面的DataState不是Synchronized，可以执行自动修复命令gprecoverseg，这条命令会检测集群的运行情况，如果segment节点的服务未启动也会进行启动</p><p>操作命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">登录master机器</span></span><br><span class="line">su – gpadmin</span><br><span class="line">gprecoverseg -a   #此命令可以多次执行，可以加上-F 强制进行修复，直到gpsta -c查看到的state正常</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>方法2：:</p><p>查询集群中的问题节点，进行修复</p><p>操作命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">登录master机器</span></span><br><span class="line">su – gpadmin</span><br><span class="line">gprecoverseg -qo ./recov     #查询集群中需要修复的节点，将内容写入到文件recov</span><br><span class="line">gprecoverseg -i ./recov     #读取recov文件中需要修复的节点</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="集群无法启动"><a href="#集群无法启动" class="headerlink" title="集群无法启动"></a>集群无法启动</h1><p>如果在执行gpstart -a时集群时，因为segment的数据目录问题导致无法启动整个集群</p><p>那么就只启动master的维护模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpstart -m</span><br></pre></td></tr></table></figure><p>可以支持gpstate的命令查询，可以查询对应的primary和mirror目录（Mirror segment采用物理文件复制的方案—primary segment中数据文件I / O被复制到mirror segment上，因此mirror segment的文件与primary segment上的文件相同）<br>使用scp命令进行文件一致<br>备份segment节点上的文件夹，例如<br>将mirror的数据目录完整发送到primary的目录，要复制整个文件夹，不要创建目标文件夹后复制文件，不然会提示文件类型无法识别（postgresql的数据目录检测机制）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv /data/primary/gpseg2 /data/primary/gpseg2-bak</span><br><span class="line">scp -r 另一台segment机器:/data/mirror/gpseg2 /data/primary/gpseg2</span><br></pre></td></tr></table></figure><h1 id="为primary-master和standby-master配置一个虚拟IP地址"><a href="#为primary-master和standby-master配置一个虚拟IP地址" class="headerlink" title="为primary master和standby master配置一个虚拟IP地址"></a>为primary master和standby master配置一个虚拟IP地址</h1><p>可以利用keepalived产生一个VIP，利用keepalived的检测进行权重增减，实现VIP漂移</p><h1 id="备份还原"><a href="#备份还原" class="headerlink" title="备份还原"></a><strong>备份还原</strong></h1><ol><li>Greenplum早期通过使用PG数据库的备份工具实现备份。但随着数据量不断增大，基于PG备份工具的串行备份模式无法满足用户对备份时效的需求。</li><li>Greenplum开始了第二代备份工具gp_dump的研发，并在2005年左右正式发布，gp_dump通过引入并行备份解决了串行备份时效低的问题，但使用上相对比较复杂。</li><li>为了进一步的完善了用户使用体验，官方基于gp_dump，开发了gpcrodump备份工具。gpcrodump非常的成功，推出以来，为用户服务了10多年，当前还有大量的用户在用（GPDB 4.22以前的版本建议的备份工具）。<br>但gpcrodump有一个非常大的限制，就是备份时长时间锁表，因此需要用户预留相应的时间窗口。随着用户数据量的持续提升，以及HTAP混合负载应用的需求，企业对在线联机备份（备份时不需要中断业务）的需求愈发强烈。</li><li>因此大概2015年，Pivotal开始研究新一代的备份工具，并在2017年正式发布，这就是新一代备份工具gpbackup。<br>gpbackup消除了锁表（专有锁）机制，同时，创新性的把GP的原有的外部表技术（一种高效的大规模并行数据加载和卸载技术）引入，用户无需为备份预留单独的时间窗口，同时扩展了备份存储的支持，<br>当前可用使用本地文件系统、Dell Data Domain、NBU、以及分布式存储，也可用使用公有云对象存储（S3），同时还支持用户自定义备份的存储接口。</li></ol><p>完整全量备份：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpbackup --dbname wyf --backup-dir /data/backup --leaf-partition-data</span><br></pre></td></tr></table></figure><p>创建增量备份,增量备份要确保之前的备份不呢丢失，不然无法还原：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpbackup --dbname wyf --backup-dir /data/backup --leaf-partition-data --incremental</span><br></pre></td></tr></table></figure><p>基于某个备份做增量备份，–from-timestamp 后面跟的是已经存在的备份时间戳（例如：/data/backup/gpseg-1/backups/备份日期/下查看）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpbackup --dbname wyf --backup-dir /mybackup --leaf-partition-data --incremental --from-timestamp 20210909145418</span><br></pre></td></tr></table></figure><p>恢复（不创建库）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gprestore --backup-dir /data/backup --timestamp 20210909152128</span><br></pre></td></tr></table></figure><p>恢复（创建库）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gprestore --backup-dir /data/backup --create-db --timestamp 20210909152128</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;gp集群如何部署就不过多介绍，主要是结合一下工作中要用到的指令和遇到的一些坑&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://huisebug.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="greenplum" scheme="https://huisebug.github.io/tags/greenplum/"/>
    
  </entry>
  
  <entry>
    <title>kata容器的一些分享</title>
    <link href="https://huisebug.github.io/2021/07/06/kata%E5%AE%B9%E5%99%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%88%86%E4%BA%AB/"/>
    <id>https://huisebug.github.io/2021/07/06/kata%E5%AE%B9%E5%99%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%88%86%E4%BA%AB/</id>
    <published>2021-07-06T06:04:01.000Z</published>
    <updated>2021-07-07T09:49:20.673Z</updated>
    
    <content type="html"><![CDATA[<p>容器运行时kata的一些知识分享</p><span id="more"></span><h1 id="kata运行时的地位"><a href="#kata运行时的地位" class="headerlink" title="kata运行时的地位"></a>kata运行时的地位</h1><p>kata-runtime的地位等同于runc运行时，所以可以替换containerd使用的运行时，在k8s中通过调用containerd，containerd使用kata达到使用。</p><p>docker使用kata是声明默认的运行时，以达到containerd替换默认运行时为kata</p><h1 id="kata的一些功能无法实现"><a href="#kata的一些功能无法实现" class="headerlink" title="kata的一些功能无法实现"></a>kata的一些功能无法实现</h1><p>使用kata会无法使用docker network的自动发现功能。</p><p>但是IP地址是可以通信的</p><h1 id="kata最新版"><a href="#kata最新版" class="headerlink" title="kata最新版"></a>kata最新版</h1><p>最新版</p><p>Release 2.1.0</p><h1 id="kata在VMware-个人桌面的一些问题解决"><a href="#kata在VMware-个人桌面的一些问题解决" class="headerlink" title="kata在VMware 个人桌面的一些问题解决"></a>kata在VMware 个人桌面的一些问题解决</h1><p>错误：ERROR: System is not capable of running Kata Containers</p><p>解决方法：</p><p>VMware虚拟机设置，处理器–》虚拟化引擎</p><p>勾选 虚拟化Intel VT-x/EPT</p><p>错误：ERRO[0000] kernel property not found arch=amd64 description=”Host Support<br>for Linux VM Sockets” name=vhost_vsock pid=16727 source=runtime type=module</p><p>System is capable of running Kata Containers</p><p>System can currently create Kata Containers</p><p>解决方法：</p><p>是linux 检测到在 vmware 环境中运行时，会加载一些 vmware 的模块并使用 vsock<br>从而产生了冲突</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo tee /etc/modules-load.d/blacklist-vmware.conf &lt;&lt; EOF</span><br><span class="line"></span><br><span class="line">blacklist vmw_vsock_virtio_transport_common</span><br><span class="line"></span><br><span class="line">blacklist vmw_vsock_vmci_transport</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>然后重启linux</p><h1 id="kata命令"><a href="#kata命令" class="headerlink" title="kata命令"></a>kata命令</h1><p>kata-runtime help</p><p>kata检查</p><p>kata-runtime kata-check</p><p>v2额外支持</p><p>kata-runtime check</p><p>版本</p><p>kata-runtime -v</p><p>环境变量</p><p>kata-runtime kata-env</p><p>v2额外支持</p><p>kata-runtime env</p><h1 id="kata配置文件"><a href="#kata配置文件" class="headerlink" title="kata配置文件"></a>kata配置文件</h1><p>查看配置文件存放位置，默认位置是/usr/share/defaults/kata-containers/configuration.toml标准系统的位置<br>。但是，如果/etc/kata-containers/configuration.toml存在，则优先</p><p>命令：kata-runtime –kata-show-default-config-paths</p><p>/etc/kata-containers/configuration.toml</p><p>/usr/share/defaults/kata-containers/configuration.toml ###centos7存放位置</p><p>/usr/share/kata-containers/defaults/configuration.toml ###centos8存放位置</p><p>kata存在基于alpine系统下运行java会提示library initialization failed - unable to<br>allocate file descriptor table - out of memory</p><p>在Linux的最新版本中，打开文件数量的默认限制已大大增加。Java 8在尝试为此数量的文件描述符预先分配内存方面做了错误的事情（请参阅<a href="https://bugs.openjdk.java.net/browse/JDK-8150460%EF%BC%89%E3%80%82%E4%BB%A5%E5%89%8D%EF%BC%8C%E5%BD%93%E9%BB%98%E8%AE%A4%E9%99%90%E5%88%B6%E8%A6%81%E4%BD%8E%E5%BE%97%E5%A4%9A%E6%97%B6%EF%BC%8C%E6%AD%A4%E6%96%B9%E6%B3%95%E6%9C%89%E6%95%88%EF%BC%8C%E4%BD%86%E7%8E%B0%E5%9C%A8%EF%BC%8C%E5%AE%83%E5%B0%9D%E8%AF%95%E5%88%86%E9%85%8D%E8%BF%87%E5%A4%9A%E8%80%8C%E5%A4%B1%E8%B4%A5%E3%80%82%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%E6%98%AF%E8%AE%BE%E7%BD%AE%E6%89%93%E5%BC%80%E6%96%87%E4%BB%B6%E6%95%B0%E7%9A%84%E4%B8%8B%E9%99%90%EF%BC%88%E6%88%96%E4%BD%BF%E7%94%A8%E8%BE%83%E6%96%B0%E7%9A%84java%EF%BC%89%EF%BC%9A">https://bugs.openjdk.java.net/browse/JDK-8150460）。以前，当默认限制要低得多时，此方法有效，但现在，它尝试分配过多而失败。解决方法是设置打开文件数的下限（或使用较新的java）：</a></p><h1 id="各操作系统安装kata"><a href="#各操作系统安装kata" class="headerlink" title="各操作系统安装kata"></a>各操作系统安装kata</h1><h2 id="cnetos7-kata-v1"><a href="#cnetos7-kata-v1" class="headerlink" title="cnetos7 kata v1"></a>cnetos7 kata v1</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum-config-manager --add-repo</span><br><span class="line">http://download.opensuse.org/repositories/home:/katacontainers:/releases:/x86_64:/stable-1.11/CentOS_7/home:katacontainers:releases:x86_64:stable-1.11.repo</span><br></pre></td></tr></table></figure><h2 id="centos8-kata-v1"><a href="#centos8-kata-v1" class="headerlink" title="centos8 kata v1"></a>centos8 kata v1</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">tee /etc/yum.repos.d/advanced-virt.repo &lt;&lt;-&#x27;EOF&#x27;</span><br><span class="line">[advanced-virt]</span><br><span class="line">name=Advanced Virtualization</span><br><span class="line">baseurl=https://mirrors.huaweicloud.com/centos/$releasever/virt/$basearch/advanced-virtualization</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">skip_if_unavailable=1</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">tee /etc/yum.repos.d/kata-containers.repo &lt;&lt;-&#x27;EOF&#x27;</span><br><span class="line">[kata-containers]</span><br><span class="line">name=Kata Containers</span><br><span class="line">baseurl=https://mirrors.huaweicloud.com/centos/$releasever/virt/$basearch/kata-containers</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">skip_if_unavailable=1</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">dnf module disable -y virt:rhel</span><br><span class="line">dnf install -y kata-runtime </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="centos8-kata-v2"><a href="#centos8-kata-v2" class="headerlink" title="centos8 kata v2"></a>centos8 kata v2</h2><p>v1和v2的差异在于，安装kata-containers即会安装v2，安装kata-runtime即会安装v1</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sudo -E dnf install -y centos-release-advanced-virtualization</span><br><span class="line">sudo -E dnf module disable -y virt:rhel</span><br><span class="line">source /etc/os-release</span><br><span class="line">cat &lt;&lt;EOF | sudo -E tee /etc/yum.repos.d/kata-containers.repo</span><br><span class="line">[kata-containers]</span><br><span class="line">name=Kata Containers</span><br><span class="line">baseurl=http://mirror.centos.org/\$contentdir/\$releasever/virt/\$basearch/kata-containers</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">skip_if_unavailable=1</span><br><span class="line">EOF</span><br><span class="line">sudo -E dnf install -y kata-containers</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>由于Docker尚不直接支持容器化的shimv2架构，因此Kata 2.0.0不适用于Docker。</p><p>Kata 1.x运行时可与docker一起使用</p><h1 id="kata中使用alpine系统存在的java问题"><a href="#kata中使用alpine系统存在的java问题" class="headerlink" title="kata中使用alpine系统存在的java问题"></a>kata中使用alpine系统存在的java问题</h1><p>当前都是kata运行时，并且版本都是1.11+</p><p>经过我的测试是文件打开数太大导致java无法运行</p><p>当image是FROM alpine时无法运行，</p><p>当image是FROM slim时可以运行，</p><p>Centos8系统安装的docker，所有容器的文件打开数默认是1048576</p><p>Centos7系统安装的docker，所有容器的文件打开数默认是1073741816</p><p>并且docker版本都是最新的20.10.6</p><p>解决方法：</p><p>方法1，不使用基于alpine为基础image的docker镜像，很显然是当文件打开数太大，kata下的alpine会运行错误。很多服务都是以alpine运行的，这个方法不推荐</p><p>方法2，使用kata运行时，使用centos8系统运行docker，不使用centos7。这个是推荐方法</p><p>方法3，可以在docker系统服务文件加上dockerd启动参数</p><p>--default-ulimit nofile=65535:65535 设置所有容器的文件打开数</p><p>设置LimitNOFILE=65535是无效的，并不会更改docker所有容器的文件打开数。</p><h1 id="containerd下可以指定是否使用kata-runtime运行时"><a href="#containerd下可以指定是否使用kata-runtime运行时" class="headerlink" title="containerd下可以指定是否使用kata-runtime运行时"></a>containerd下可以指定是否使用kata-runtime运行时</h1><p>使用ctr命令行启动容器</p><p>要通过容器命令行使用Kata Containers运行容器，可以运行以下命令：</p><p>sudo ctr image pull docker.io/library/busybox:latest</p><p>sudo ctr run –runtime io.containerd.run.kata.v2 -t –rm<br>docker.io/library/busybox:latest hello sh</p><p>这将启动一个名为的BusyBox容器hello，–rm退出后将被删除。</p><h1 id="在k8s1-12-下利用RuntimeClass选择性使用kata"><a href="#在k8s1-12-下利用RuntimeClass选择性使用kata" class="headerlink" title="在k8s1.12+下利用RuntimeClass选择性使用kata"></a>在k8s1.12+下利用RuntimeClass选择性使用kata</h1><p>查看当前机器下containerd容器使用了什么运行时来运行容器</p><p>ctr -n k8s.io c ls</p><p>kata-runtime是隔断网络访问的，所以为了k8s组件可以正常运行，containerd默认的运行时不可以设置为kata，可以使用下面的2种方式指定运行时运行容器</p><p>containerd支持kata-runtimev2的接口，在配置untrusted_workload_runtime时，直接使用katav2的类型io.containerd.kata.v2，没有io.containerd.kata.v1这种类型</p><p>因为不存在 runtime_type = “io.containerd.kata.v1”，我们可以使用runtime_type =<br>“io.containerd.kata.v2”来调用kata-runtime，containerd并不关心运行的kata是v1还是v2</p><p>1、untrusted_workload_runtime 的方式</p><p>kata-v1</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.untrusted_workload_runtime]</span><br><span class="line">  runtime_type = &quot;io.containerd.runtime.v1.linux&quot;</span><br><span class="line">  runtime_engine = &quot;/usr/bin/kata-runtime&quot;</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.untrusted_workload_runtime]</span><br><span class="line">  runtime_type = &quot;io.containerd.kata.v2&quot;</span><br></pre></td></tr></table></figure><p>kata-v2</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.untrusted_workload_runtime]</span><br><span class="line">  runtime_type = &quot;io.containerd.kata.v2&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>k8s调用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-untrusted</span><br><span class="line">  annotations:</span><br><span class="line">    io.kubernetes.cri.untrusted-workload: &quot;true&quot;</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>2、RuntimeClass 方式</p><p>首先需要在containerd配置文件中增加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">kata-v2</span><br><span class="line">        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.kata]</span><br><span class="line">          runtime_type = &quot;io.containerd.kata.v2&quot;  </span><br><span class="line">          [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.kata.options]</span><br><span class="line">            SystemdCgroup = true</span><br><span class="line"></span><br><span class="line">kata-v1</span><br><span class="line">        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.kata]</span><br><span class="line">          runtime_type = &quot;io.containerd.kata.v2&quot;  </span><br><span class="line">          [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.kata.options]</span><br><span class="line">            SystemdCgroup = true</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>因为不存在 runtime_type = “io.containerd.kata.v1”，我们可以使用runtime_type =<br>“io.containerd.kata.v2”来调用kata-runtime，containerd并不关心运行的kata是v1还是v2</p><p>在 K8s 中创建 RuntimeClass</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: node.k8s.io/v1beta1  # RuntimeClass is defined in the node.k8s.io API group</span><br><span class="line">kind: RuntimeClass</span><br><span class="line">metadata:</span><br><span class="line">  name: kata  </span><br><span class="line">handler: kata  # 这里与containerd配置文件中的 [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.&#123;handler&#125;] 匹配</span><br><span class="line">创建pod</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: kata-nginx</span><br><span class="line">spec:</span><br><span class="line">  runtimeClassName: kata</span><br><span class="line">  containers:</span><br><span class="line">    - name: nginx</span><br><span class="line">      image: nginx</span><br><span class="line">      ports:</span><br><span class="line">      - containerPort: 80</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="查看kata建立的容器kata-v1命令"><a href="#查看kata建立的容器kata-v1命令" class="headerlink" title="查看kata建立的容器kata-v1命令"></a>查看kata建立的容器kata-v1命令</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kata-runtime list</span><br></pre></td></tr></table></figure><h1 id="docker指定kata"><a href="#docker指定kata" class="headerlink" title="docker指定kata"></a>docker指定kata</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --runtime=kata-runtime nginx:1.9</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;容器运行时kata的一些知识分享&lt;/p&gt;</summary>
    
    
    
    <category term="容器运行时" scheme="https://huisebug.github.io/categories/%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6/"/>
    
    
    <category term="containerd" scheme="https://huisebug.github.io/tags/containerd/"/>
    
    <category term="docker" scheme="https://huisebug.github.io/tags/docker/"/>
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="kata" scheme="https://huisebug.github.io/tags/kata/"/>
    
    <category term="runtime" scheme="https://huisebug.github.io/tags/runtime/"/>
    
  </entry>
  
  <entry>
    <title>containerd基础概念</title>
    <link href="https://huisebug.github.io/2021/04/19/containerd%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"/>
    <id>https://huisebug.github.io/2021/04/19/containerd%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/</id>
    <published>2021-04-19T06:04:01.000Z</published>
    <updated>2021-08-03T08:04:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>containerd的前世今生，与docker的关系、如何部署containerd、如何接入k8s中</p><span id="more"></span><h1 id="OCI"><a href="#OCI" class="headerlink" title="OCI"></a>OCI</h1><p>OCI（Open Container Initiative）开放容器倡议，OCI由docker以及其他容器行业领导者创建于2015年，目前主要有两个标准：容器运行时标准(runtime-spec)和容器镜像标准(image-spec)</p><p>这两个标准通过OCI runtime filesytem bundle的标准格式连接在一起,OCI镜像可以通过工具转换成bundle,然后 OCI 容器引擎能够识别这个bundle来运行容器</p><h1 id="CRI"><a href="#CRI" class="headerlink" title="CRI"></a>CRI</h1><p>CRI（Container Runtime Interface）容器运行时接口，CRI是kubernetes推出的一个标准,推出标准可见其在容器编排领域的地位</p><p>runc、containerd 等运行时都去支持此接口。</p><h1 id="runc"><a href="#runc" class="headerlink" title="runc"></a>runc</h1><p>runC的前身是docker的libcontainer项目,在libcontainer的基础上做了封装,捐赠给OCI的一个符合标准的runtime实现,docker引擎内部也是基于runC构建的</p><p>runC只做一件事情就是运行容器,提供创建和运行容器的CLI(command-line interface)工具, runC直接与容器所依赖的cgroup/namespace linux kernel等进行交互，</p><p>负责为容器配置cgroup/namespace等启动容器所需的环境，创建启动容器的相关进程</p><h1 id="shim"><a href="#shim" class="headerlink" title="shim"></a>shim</h1><p>中文意思 ： 楔子</p><p>例如： dockershim和containershim</p><h2 id="containerd-shim"><a href="#containerd-shim" class="headerlink" title="containerd-shim"></a>containerd-shim</h2><p>containerd-shim进程由containerd进程拉起,即containerd进程是containerd-shim的父进程,容器进程由containerd-shim进程拉起,</p><p>这样的优点比如升级,重启docker或者containerd 不会影响已经running的容器进程,而假如这个父进程就是containerd,那每次containerd挂掉或升级,整个宿主机上所有的容器都得退出了. 而引入了 containerd-shim 就规避了这个问题(当containerd 退出或重启时, shim 会 re-parent 到 systemd 这样的 1 号进程上)</p><h1 id="部署containerd服务"><a href="#部署containerd服务" class="headerlink" title="部署containerd服务"></a>部署containerd服务</h1><h2 id="安装runc"><a href="#安装runc" class="headerlink" title="安装runc"></a>安装runc</h2><p>yum -y install runc</p><p>如果未安装则会ctr run 容器时会提示如下：</p><p>ctr: OCI runtime create failed: unable to retrieve OCI runtime error (open /run/containerd/io.containerd.runtime.v2.task/default/nginx/log.json: no such file or directory): exec: “runc”: executable file not found in $PATH: unknown</p><h2 id="安装containerd"><a href="#安装containerd" class="headerlink" title="安装containerd"></a>安装containerd</h2><h3 id="二进制分发"><a href="#二进制分发" class="headerlink" title="二进制分发"></a>二进制分发</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf containerd-1.4.4-linux-amd64.tar.gz -C /usr/local/</span><br><span class="line">mkdir -p /etc/containerd</span><br><span class="line">containerd config default &gt; /etc/containerd/config.toml</span><br></pre></td></tr></table></figure><h3 id="创建系统服务"><a href="#创建系统服务" class="headerlink" title="创建系统服务"></a>创建系统服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF | sudo tee /usr/lib/systemd/system/containerd.service</span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description=containerd container runtime</span><br><span class="line">Documentation=https://containerd.io</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">ExecStartPre=/sbin/modprobe overlay</span><br><span class="line">ExecStart=/usr/local/bin/containerd</span><br><span class="line">Delegate=yes</span><br><span class="line">KillMode=process</span><br><span class="line">LimitNOFILE=1048576</span><br><span class="line"><span class="meta">#</span><span class="bash"> Having non-zero Limit*s causes performance problems due to accounting overhead</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">in</span> the kernel. We recommend using cgroups to <span class="keyword">do</span> container-local accounting.</span></span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br><span class="line">EOF</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="配置介绍："><a href="#配置介绍：" class="headerlink" title="配置介绍："></a>配置介绍：</h4><p>Delegate : 这个选项允许 Containerd 以及运行时自己管理自己创建的容器的 cgroups。</p><p>如果不设置这个选项，systemd 就会将进程移到自己的 cgroups 中，从而导致 Containerd 无法正确获取容器的资源使用情况。</p><p>KillMode : 这个选项用来处理 Containerd 进程被杀死的方式。</p><p>默认情况下，systemd 会在进程的 cgroup 中查找并杀死 Containerd 的所有子进程，这肯定不是我们想要的。KillMode字段可以设置的值如下。</p><p>我们需要将 KillMode 的值设置为 process，这样可以确保升级或重启 Containerd时不杀死现有的容器。</p><p>control-group（默认值）：当前控制组里面的所有子进程，都会被杀掉</p><p>process：只杀主进程</p><p>mixed：主进程将收到 SIGTERM 信号，子进程收到 SIGKILL 信号</p><p>none：没有进程会被杀掉，只是执行服务的 stop 命令。</p><h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable containerd</span><br><span class="line">systemctl start containerd</span><br><span class="line">systemctl status containerd</span><br></pre></td></tr></table></figure><h1 id="Container命令ctr-crictl的用法"><a href="#Container命令ctr-crictl的用法" class="headerlink" title="Container命令ctr,crictl的用法"></a>Container命令ctr,crictl的用法</h1><p>containerd 相比于docker , 多了namespace概念, 每个image和container<br>都会在各自的namespace下可见, 目前k8s会使用k8s.io 作为命名空间</p><h2 id="image"><a href="#image" class="headerlink" title="image"></a>image</h2><h3 id="查看ctr-image"><a href="#查看ctr-image" class="headerlink" title="查看ctr image"></a>查看ctr image</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ctr image list</span><br><span class="line">ctr i list</span><br><span class="line">ctr i ls</span><br></pre></td></tr></table></figure><h3 id="镜像标记tag"><a href="#镜像标记tag" class="headerlink" title="镜像标记tag"></a>镜像标记tag</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io i tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2 k8s.gcr.io/pause:3.2</span><br></pre></td></tr></table></figure><p>注意: 若新镜像reference 已存在, 需要先删除新reference, 或者如下方式强制替换</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io i tag --force registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2 k8s.gcr.io/pause:3.2</span><br></pre></td></tr></table></figure><h3 id="删除镜像"><a href="#删除镜像" class="headerlink" title="删除镜像"></a>删除镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io i rm k8s.gcr.io/pause:3.2</span><br></pre></td></tr></table></figure><h3 id="拉取镜像"><a href="#拉取镜像" class="headerlink" title="拉取镜像"></a>拉取镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io i pull -k k8s.gcr.io/pause:3.2</span><br></pre></td></tr></table></figure><h3 id="推送镜像"><a href="#推送镜像" class="headerlink" title="推送镜像"></a>推送镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io i push -k k8s.gcr.io/pause:3.2</span><br></pre></td></tr></table></figure><h3 id="导出镜像"><a href="#导出镜像" class="headerlink" title="导出镜像"></a>导出镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io i export pause.tar k8s.gcr.io/pause:3.2</span><br></pre></td></tr></table></figure><h3 id="导入镜像"><a href="#导入镜像" class="headerlink" title="导入镜像"></a>导入镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io i import pause.tar</span><br></pre></td></tr></table></figure><h3 id="不支持-build-commit-镜像"><a href="#不支持-build-commit-镜像" class="headerlink" title="不支持 build,commit 镜像"></a>不支持 build,commit 镜像</h3><h2 id="运行容器"><a href="#运行容器" class="headerlink" title="运行容器"></a>运行容器</h2><p>例子:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io run --null-io --net-host -d \</span><br><span class="line">–env PASSWORD=$drone_password \</span><br><span class="line">–mount type=bind,src=/etc,dst=/host-etc,options=rbind:rw \</span><br><span class="line">–mount type=bind,src=/root/.kube,dst=/root/.kube,options=rbind:rw \</span><br><span class="line"><span class="meta">$</span><span class="bash">image sysreport bash /sysreport/run.sh</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--null-io: 将容器内标准输出重定向到/dev/null</span><br><span class="line">--net-host: 主机网络</span><br></pre></td></tr></table></figure><p>-d:<br>当task执行后就进行下一步shell命令,如没有选项,则会等待用户输入,并定向到容器内建立容器,ctr run 必须提供容器ID</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr c delete down</span><br></pre></td></tr></table></figure><p>ctr run 镜像全名称 容器ID 命令 （相对于docker的建立：docker run –name=容器ID 镜像名 命令）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例如：ctr run -t --rm docker.io/huisebug/down:latest down ls</span><br></pre></td></tr></table></figure><p>建立容器，同时运行了task，并后台运行-d</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr run -d docker.io/library/nginx:latest nginx</span><br></pre></td></tr></table></figure><h3 id="过程总结"><a href="#过程总结" class="headerlink" title="过程总结"></a>过程总结</h3><p>容器container是分配和附加资源的元数据对象。其实就是相当于docker create，此时容器并未运行<br>任务task是系统上正在运行的实时过程。每次运行ctr t start会启动容器<br>（相当于docker start）后都应删除任务ctr t kill （相当于docker stop），而容器可以多次使用，更新和查询。<br>一般我们都是直接docker run 就是相当于执行了ctr run，就是执行了ctr c create 创建容器后马上执行ctr t start<br>如果ctr run （docker run）时没有加上-d参数后台运行，执行后会在终端进行输出。中断终端后task结束<br>命令为ctr t ls 相当于使用docker ps -a（会列出所有的容器）<br>docker rm就是相当于执行了ctr t rm和ctr c rm删除task和容器，一般删除容器，对应的task也会删除</p><h2 id="进入容器"><a href="#进入容器" class="headerlink" title="进入容器"></a>进入容器</h2><p>进入容器–exec-id 0必须填写，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr t exec --exec-id 0 -t nginx sh</span><br></pre></td></tr></table></figure><h2 id="停止容器"><a href="#停止容器" class="headerlink" title="停止容器,"></a>停止容器,</h2><p>需要先停止容器内的task, 再删除容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io tasks kill -a -s 9 &#123;id&#125;</span><br><span class="line">ctr -n k8s.io c rm &#123;id&#125;</span><br></pre></td></tr></table></figure><h2 id="容器日志"><a href="#容器日志" class="headerlink" title="容器日志"></a>容器日志</h2><p>注意: 容器默认使用fifo创建日志文件,如果不读取日志文件,会因为fifo容量导致业务运行阻塞</p><p>如要创建日志文件,建议如下方式创建:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io run --log-uri file:///var/log/xx.log …</span><br></pre></td></tr></table></figure><h1 id="ctr命令与docker命令"><a href="#ctr命令与docker命令" class="headerlink" title="ctr命令与docker命令"></a>ctr命令与docker命令</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctr image ls</span><br><span class="line">docker images</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctr image pull</span><br><span class="line">docker pull</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctr image tag</span><br><span class="line">docker tag</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctr image push</span><br><span class="line">docker push</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctr image import 不支持压缩</span><br><span class="line">docker load &lt;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctr run 镜像名 容器名称</span><br><span class="line">docker run --name=容器名称 镜像名</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctr task ls 会显示stop的task和running的task</span><br><span class="line">docker ps -a</span><br></pre></td></tr></table></figure><h1 id="crictl用法"><a href="#crictl用法" class="headerlink" title="crictl用法"></a>crictl用法</h1><p>crictl 工具 是为k8s使用containerd而制作的, 其他非k8s的创建的crictl是无法看到和调试的, 也就是说用ctr run 运行的容器无法使用crictl 看到</p><p>crictl 使用命名空间 k8s.io.</p><p>cri plugin区别对待pod和container</p><p>ps: 列出在k8s.io 命名空间下的业务容器</p><p>pods: 列出在k8s.io 命名空间下的sandbox容器,在k8s里,通常是pause容器</p><p>logs: 打印业务容器日志</p><p>create: 创建容器,这里需要先创建sandbox,获取sandbox容器的id后,再用此id创建业务容器</p><p>inspect: 列出业务容器状态</p><p>inspectp: 列出sandbox容器状态</p><p>crictl 是cri的控制命令<br>因为暂时还未完全舍弃docker，所以docker.sock是排在第一位的，指定使用containerd.sock</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crictl -r unix:///run/containerd/containerd.sock  images</span><br></pre></td></tr></table></figure><p>ctr指定命名空间必须跟在ctr命令后面</p><p>crictl所做的操作是在ctr 的 k8s.io命名空间下<br>例如镜像， crictl -r unix:///run/containerd/containerd.sock images 和 ctr -n k8s.io i ls<br>docker所做的操作是在ctr的moby命名空间下，例如docker正在运行的容器，docker ps 和 ctr -n moby t ls，镜像不是一致的<br>crictl使用镜像docker.io仓库的镜像不用写全名</p><p>containerd配置文件/etc/containerd/config.toml所做的修改是给k8s（或者说是crictl命令使用）使用，可以利用下面的命令看到当前containerd基于k8s的配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crictl -r unix:///run/containerd/containerd.sock info | jq</span><br></pre></td></tr></table></figure><p>清理k8s集群中未使用的镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crictl -r unix:///run/containerd/containerd.sock rmi  --prune </span><br></pre></td></tr></table></figure><h1 id="docker在containerd的命名空间"><a href="#docker在containerd的命名空间" class="headerlink" title="docker在containerd的命名空间"></a>docker在containerd的命名空间</h1><p>除了k8s有命名空间以外，containerd也支持命名空间。docker创建的默认的都在moby空间，而k8s默认是k8s.io这个空间下面，不同空间的容器互相隔离。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr -n moby c ls</span><br></pre></td></tr></table></figure><p>上面只会显示正在运行的docker容器，没有运行的不会显示</p><h1 id="拉取dockerhub镜像"><a href="#拉取dockerhub镜像" class="headerlink" title="拉取dockerhub镜像"></a>拉取dockerhub镜像</h1><p>containerd要求必须先pull镜像，并且镜像需要写全镜像信息，包含仓库地址、用户、镜像tag</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例如： ctr i pull docker.io/huisebug/down:latest</span><br></pre></td></tr></table></figure><p>docker官方维护的镜像，用户名是library，比如nginx、redis</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr i pull docker.io/library/nginx:latest</span><br></pre></td></tr></table></figure><h1 id="login自建仓库"><a href="#login自建仓库" class="headerlink" title="login自建仓库"></a>login自建仓库</h1><p>ctr 不支持http,必须使用https, 所有需要添加 –plain-http 参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctr images pull --plain-http --user admin:123456 192.168.137.101:5000/debug/registry:latest</span><br><span class="line">ctr images pull --plain-http 192.168.137.101:5000/debug/registry:latest</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;containerd的前世今生，与docker的关系、如何部署containerd、如何接入k8s中&lt;/p&gt;</summary>
    
    
    
    <category term="容器运行时" scheme="https://huisebug.github.io/categories/%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6/"/>
    
    
    <category term="containerd" scheme="https://huisebug.github.io/tags/containerd/"/>
    
  </entry>
  
  <entry>
    <title>k8s集群高版本1.18以上常见服务部署yaml</title>
    <link href="https://huisebug.github.io/2020/12/15/k8s%E9%AB%98%E7%89%88%E6%9C%AC%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2yaml/"/>
    <id>https://huisebug.github.io/2020/12/15/k8s%E9%AB%98%E7%89%88%E6%9C%AC%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2yaml/</id>
    <published>2020-12-15T09:09:01.000Z</published>
    <updated>2021-07-09T02:59:27.639Z</updated>
    
    <content type="html"><![CDATA[<p>此模块专门提供k8s集群中的常见服务的部署yaml<br>下面的服务是建立在k8s集群1.18.6以上的</p><span id="more"></span><p>下面涉及到的namespace请修改为你的namespace<br><a href="/download/">下载页面</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;此模块专门提供k8s集群中的常见服务的部署yaml&lt;br&gt;下面的服务是建立在k8s集群1.18.6以上的&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes" scheme="https://huisebug.github.io/categories/Kubernetes/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Prometheus-Operator告警场景</title>
    <link href="https://huisebug.github.io/2020/12/15/prometheus-operator%E5%91%8A%E8%AD%A6%E5%9C%BA%E6%99%AF/"/>
    <id>https://huisebug.github.io/2020/12/15/prometheus-operator%E5%91%8A%E8%AD%A6%E5%9C%BA%E6%99%AF/</id>
    <published>2020-12-15T09:00:01.000Z</published>
    <updated>2021-07-07T08:49:41.212Z</updated>
    
    <content type="html"><![CDATA[<p>利用Prometheus-Operator来监控k8s集群并进行告警</p><span id="more"></span><h1 id="prometheus-operator告警场景"><a href="#prometheus-operator告警场景" class="headerlink" title="prometheus-operator告警场景"></a>prometheus-operator告警场景</h1><h1 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h1><p>准备环境k8s1.12+以上，k8s的部署就不过多介绍，有kube-admin，也有二进制部署，当然也可以参考我的<a href="https://huisebug.github.io/2019/08/24/k8s/">k8s集群部署</a></p><p>基本上环境准备好了就如我的集群如下</p><p><img src="/2020/12/15/prometheus-operator%E5%91%8A%E8%AD%A6%E5%9C%BA%E6%99%AF/media/7738d57508ef6a61185e1e3b1f5adf0f.png"></p><p>各组件已经成功安装。</p><h1 id="安装Prometheus-Operator"><a href="#安装Prometheus-Operator" class="headerlink" title="安装Prometheus-Operator"></a>安装Prometheus-Operator</h1><p>Prometheus-Operator原理介绍：参考我的<a href="https://huisebug.github.io/2019/08/27/Prometheus-Operator/">Prometheus-Operator监控k8s</a></p><p>此处演示的Prometheus-Operator场景的安装yaml，下载地址：<a href="https://github.com/huisebug/Prometheus-Operator-Rules.git">https://github.com/huisebug/Prometheus-Operator-Rules.git</a></p><h2 id="manifests目录介绍"><a href="#manifests目录介绍" class="headerlink" title="manifests目录介绍"></a>manifests目录介绍</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f manifests/</span><br></pre></td></tr></table></figure><p>多次执行上面的初始化安装确保prometheus-operator可用</p><p>注：<font color="red" size="3">上面的yaml是官方4月份发布的版本，经过多次测试可以适用于多个集群，其中修改了一些镜像地址为国内可快速拉取的地址和移除了prometheus-adapter的部署。</font></p><h2 id="new目录介绍"><a href="#new目录介绍" class="headerlink" title="new目录介绍"></a>new目录介绍</h2><p>alertmanager-alertmanager.yaml:增加告警模板自定义位置的配置</p><p>alertmanager-secret.yaml:alertmanager服务的配置文件，包含企业微告警，印制告警，告警模板，告警间隔时间。</p><p>alertmanager-temp-configmap.yaml:告警模板</p><p>auth:访问prometheus、alertmanager、grafana的http认证文件</p><p>ingress.yaml:访问prometheus、alertmanager、grafana的域名配置，并增加http密码校验</p><p>prometheus-prometheus.yaml:增加prometheus数据保留时间，抓取metrics的间隔时间</p><p>state-rules.yaml:告警规则，也是本文章的主要介绍内容。</p><h2 id="执行安装"><a href="#执行安装" class="headerlink" title="执行安装"></a>执行安装</h2><p>将auth认证文件传递到secret</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl -n monitoring create secret generic basic-auth --from-file=. /new/auth</span><br></pre></td></tr></table></figure><p>应用更改后的配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f new/</span><br></pre></td></tr></table></figure><h1 id="告警模板"><a href="#告警模板" class="headerlink" title="告警模板"></a>告警模板</h1><p>在之前的文章中写过<a href="https://huisebug.github.io/2019/08/27/Prometheus-Operator/#%E5%BB%BA%E7%AB%8B%E8%87%AA%E5%AE%9A%E4%B9%89%E5%91%8A%E8%AD%A6%E6%A8%A1%E6%9D%BF">自定义告警模板</a>，后续进行了模板的不断完善，告警信息我认为不必要的信息没必要展示，精确告警，新的告警模板如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#123;- define &quot;__text_alert_list&quot; -&#125;&#125;</span><br><span class="line">&#123;&#123;- range .Alerts.Firing -&#125;&#125;</span><br><span class="line">错误告警:</span><br><span class="line">告警消息: &#123;&#123; .Annotations.message &#125;&#125;</span><br><span class="line">告警级别: &#123;&#123; .Labels.severity &#125;&#125;</span><br><span class="line">告警alert: &#123;&#123; .Labels.alertname &#125;&#125;</span><br><span class="line">触发时间: &#123;&#123; (.StartsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125;</span><br><span class="line">&#123;&#123; end &#125;&#125;   </span><br><span class="line">&#123;&#123;- range .Alerts.Resolved -&#125;&#125;</span><br><span class="line">服务已恢复:</span><br><span class="line">原告警消息: &#123;&#123; .Annotations.message &#125;&#125;</span><br><span class="line">恢复时间: &#123;&#123; (.EndsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125;</span><br><span class="line">&#123;&#123; end &#125;&#125;</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;&#123;- define &quot;wechat.default.message&quot; -&#125;&#125;</span><br><span class="line">&#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;</span><br><span class="line">&#123;&#123; template &quot;__text_alert_list&quot; . &#125;&#125;</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br><span class="line">&#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;</span><br><span class="line">&#123;&#123; template &quot;__text_alert_list&quot; . &#125;&#125;</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br></pre></td></tr></table></figure><p>将告警信息精简为从声明annotations中取值自定义的键值对，键名为messages一条即可，根据不同的rules定义每一个rule的messages告警内容</p><h1 id="Rules"><a href="#Rules" class="headerlink" title="Rules"></a>Rules</h1><h2 id="已监听job状态"><a href="#已监听job状态" class="headerlink" title="已监听job状态"></a>已监听job状态</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">- name: up</span><br><span class="line">  rules:</span><br><span class="line">  - alert: 已监听job状态</span><br><span class="line">    expr: up == 0</span><br><span class="line">    for: 30s</span><br><span class="line">    labels:</span><br><span class="line">      severity: Erroring</span><br><span class="line">    annotations:</span><br><span class="line">      message: &quot;&#123;&#123;$labels.job&#125;&#125;服务所在实例&#123;&#123;$labels.instance&#125;&#125;关闭&quot;</span><br></pre></td></tr></table></figure><p>监控prometheus的target下的所有服务</p><h2 id="pod等待状态错误原因"><a href="#pod等待状态错误原因" class="headerlink" title="pod等待状态错误原因"></a>pod等待状态错误原因</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">- name: kube_pod_container_status_waiting_reason</span><br><span class="line">  rules:</span><br><span class="line">  - alert: pod等待状态错误原因</span><br><span class="line">    expr: kube_pod_container_status_waiting_reason&#123;reason=~&quot;ErrImagePull|CrashLoopBackOff|ImagePullBackOff&quot;&#125; &gt; 0</span><br><span class="line">    for: 30s</span><br><span class="line">    labels:</span><br><span class="line">      severity: critical</span><br><span class="line">      reason: waiting</span><br><span class="line">    annotations:</span><br><span class="line">      message: &quot;&#123;&#123;$labels.namespace&#125;&#125;.&#123;&#123;$labels.pod&#125;&#125;等待错误原因:&#123;&#123;$labels.reason&#125;&#125;&quot;</span><br></pre></td></tr></table></figure><p>监控pod非running状态的原因进行告警</p><h2 id="pod容器内存不足"><a href="#pod容器内存不足" class="headerlink" title="pod容器内存不足"></a>pod容器内存不足</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">- name: kube_pod_container_status_last_terminated_reason</span><br><span class="line">  rules:</span><br><span class="line">  - alert: pod容器内存不足</span><br><span class="line">    expr: kube_pod_container_status_last_terminated_reason&#123;reason=~&quot;OOMKilled&quot;&#125; &gt; 0</span><br><span class="line">    for: 3s</span><br><span class="line">    labels:</span><br><span class="line">      severity: critical</span><br><span class="line">      reason: terminated</span><br><span class="line">    annotations:</span><br><span class="line">      message: &quot;&#123;&#123;$labels.namespace&#125;&#125;.&#123;&#123;$labels.pod&#125;&#125;容器内存不足&quot;</span><br></pre></td></tr></table></figure><p>如果pod设置了resource内存限制，当pod超过内存被kill的时候进行告警，提醒使用者是否该调整该pod的resource</p><h2 id="pod状态有变动"><a href="#pod状态有变动" class="headerlink" title="pod状态有变动"></a>pod状态有变动</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">- name: kube_pod_container_status_ready</span><br><span class="line">  rules:</span><br><span class="line">  - alert: pod状态有变动</span><br><span class="line">    expr: kube_pod_container_status_ready != 1</span><br><span class="line">    for: 0s</span><br><span class="line">    labels:</span><br><span class="line">      severity: critical</span><br><span class="line">      reason: waiting</span><br><span class="line">    annotations:</span><br><span class="line">      message: &quot;&#123;&#123;$labels.namespace&#125;&#125;.&#123;&#123;$labels.pod&#125;&#125;容器&#123;&#123;$labels.container&#125;&#125;状态有变动&quot;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;利用Prometheus-Operator来监控k8s集群并进行告警&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes" scheme="https://huisebug.github.io/categories/Kubernetes/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="prometheus" scheme="https://huisebug.github.io/tags/prometheus/"/>
    
    <category term="prometheus-operator" scheme="https://huisebug.github.io/tags/prometheus-operator/"/>
    
    <category term="alertmanager" scheme="https://huisebug.github.io/tags/alertmanager/"/>
    
    <category term="grafana" scheme="https://huisebug.github.io/tags/grafana/"/>
    
    <category term="企业微信" scheme="https://huisebug.github.io/tags/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes集群搭建</title>
    <link href="https://huisebug.github.io/2020/08/26/k8s/"/>
    <id>https://huisebug.github.io/2020/08/26/k8s/</id>
    <published>2020-08-26T05:04:01.000Z</published>
    <updated>2021-07-07T09:15:36.648Z</updated>
    
    <content type="html"><![CDATA[<p>这是一个k8s集群，从零开始搭建服务器环境，keepalived+haproxy实现集群高可用VIP；glusterfs搭建实现可持续存储；k8s1.12版本集群；efk日志系统；prometheus-operator告警系统；HPA v2横向pod扩容；k8s1.11+以上的版本部署基本没什么区别，无非就是优化了一些参数的配置和增加一些功能，舍弃一些api，此文档适用后续发布的其他版本的部署，提供一些部署思路。</p><span id="more"></span><h1 id="硬件环境准备"><a href="#硬件环境准备" class="headerlink" title="硬件环境准备"></a>硬件环境准备</h1><ul><li>  服务器三台centos7.6</li><li>  每台服务器两块硬盘，一块作为服务器硬盘，一块作为glusterFS硬盘</li><li>  关闭firewalld防火墙和selinux</li><li>  IP地址：192.168.137.10—12/24；主机名api、node1、node2.huisebug.com</li><li>  使用阿里云yum源</li><li>  下面的操作是之前使用k8s1.12.4搭建的，其中的参数同样适用于k8s.12.*,因为hpa的原因替换为了k8s1.11.8。</li><li>  k8s版本1.11.8（2019年3月1号发布）；</li><li>  hpa在如下版本会会存在hpa无法获取内存的使用情况的bug，所以不推荐使用这些版本，其他版本未测试。如果你在使用了1.11.*版本后切换到了如下版本，是因为数据在etcd还没更新，一段时间后就会提示获取不到<blockquote><p>  k8s1.13.3<br>  k8s1.12.4<br>  k8s1.13.4<br>  k8s1.12.5<br>  k8s1.12.6</p></blockquote></li><li>  总结出k8s1.12+版本的HPA是无法获取内存使用情况，进一步跟进官方，k8s1.11.*版本是不支持autoscaling/v2beta2.</li></ul><h1 id="进行系统配置并开启IPVS"><a href="#进行系统配置并开启IPVS" class="headerlink" title="进行系统配置并开启IPVS"></a>进行系统配置并开启IPVS</h1><ul><li>  关闭防火墙、selinux</li><li>  关闭系统的Swap，Kubernetes 1.8开始要求。</li><li>  关闭linux swap空间的swappiness</li><li>  配置L2网桥在转发包时会被iptables的FORWARD规则所过滤，该配置被CNI插件需要，更多信息请参考<a href="https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/#network-plugin-requirements">NetworkPluginRequirements</a></li><li>  开启IPVS，将会后续应用到kube-proxy中去</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 所有主机：基本系统配置</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 关闭Selinux/firewalld</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config</span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 关闭交换分区</span></span><br><span class="line">swapoff -a</span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 增加个谷歌dns，方便访问外网</span></span><br><span class="line">echo &quot;nameserver 8.8.8.8&quot; &gt;&gt; /etc/resolv.conf</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置网桥包经IPTables，core文件生成路径</span></span><br><span class="line">echo &quot;&quot;&quot;</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">vm.overcommit_memory=1</span><br><span class="line">vm.panic_on_oom=0</span><br><span class="line">&quot;&quot;&quot; &gt; /etc/sysctl.conf</span><br><span class="line">modprobe br_netfilter</span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure><p>从Linux内核3.18-rc1开始，你必须使用modprobe br_netfilter来启用bridge-netfilter。，不然会提示以下错误<br><img src="/2020/08/26/k8s/media/10033b726854f2231957b8048571e073.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 所有主机：基本系统配置</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 同步时间</span></span><br><span class="line">yum install -y ntpdate</span><br><span class="line">ntpdate -u ntp.api.bz</span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装内核组件</span></span><br><span class="line">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm ;yum --enablerepo=elrepo-kernel install kernel-lt-devel kernel-lt -y</span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查默认内核版本高于4.1，否则请调整默认启动参数</span></span><br><span class="line">grub2-editenv list</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 确认内核版本</span></span><br><span class="line">uname -a</span><br><span class="line">注解：一般centos7的内核是3.10.当你升级后重启服务器，显示的内核还是之前的，因为你没切换到新内核，需要在主机启动页面选择，如果是云服务器，是看不到选择界面的，这时候就需要将新安装的内核设定为操作系统的默认内核，或者说如何将新版本的内核设置为重启后的默认内核？ </span><br><span class="line">仅需两步，之后重启即可。</span><br><span class="line">grub2-set-default 0</span><br><span class="line">grub2-mkconfig -o /etc/grub2.cfg</span><br><span class="line">reboot</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 重启后再次执行以下命令，确认内核版本</span></span><br><span class="line">uname –a</span><br><span class="line"><span class="meta">#</span><span class="bash"> 确认内核高于4.1后，开启IPVS</span></span><br><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">ipvs_modules=&quot;ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4&quot;</span><br><span class="line">for kernel_module in \$&#123;ipvs_modules&#125;; do</span><br><span class="line"> /sbin/modinfo -F filename \$&#123;kernel_module&#125; &gt; /dev/null 2&gt;&amp;1</span><br><span class="line"> if [ \$? -eq 0 ]; then</span><br><span class="line"> /sbin/modprobe \$&#123;kernel_module&#125;</span><br><span class="line"> fi</span><br><span class="line">done</span><br><span class="line">EOF</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 授权并执行</span></span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs</span><br></pre></td></tr></table></figure><p>Kubernetes要求集群中所有机器具有不同的Mac地址、产品uuid、Hostname。可以使用如下命令查看Mac和uuid</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 所有主机：检查UUID和Mac</span></span><br><span class="line">cat /sys/class/dmi/id/product_uuid</span><br><span class="line">ip link</span><br></pre></td></tr></table></figure><h1 id="GlusterFS"><a href="#GlusterFS" class="headerlink" title="GlusterFS"></a>GlusterFS</h1><p>搭建glusterfs来作为可持续存储k8s的CSI（可以跳过不部署）</p><h2 id="安装glusterfs"><a href="#安装glusterfs" class="headerlink" title="安装glusterfs"></a>安装glusterfs</h2><p><font color="red" size="3">三台服务器都要执行</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 先安装 gluster 源</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> yum install centos-release-gluster -y</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装 glusterfs 组件（这里包含了server和client）</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建 glusterfs服务运行目录</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir /opt/glusterd</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改 glusterd 目录，将/var/lib改成/opt</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sed -i &amp;<span class="comment">#39;s/var/lib/opt/g&amp;#39; /etc/glusterfs/glusterd.vol</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动 glusterfs（为挂载提供服务）</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl start glusterd.service</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置开机启动</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl <span class="built_in">enable</span> glusterd.service</span></span><br><span class="line"><span class="meta">#</span><span class="bash">查看状态</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl status glusterd.service</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="配置-glusterfs"><a href="#配置-glusterfs" class="headerlink" title="配置 glusterfs"></a>配置 glusterfs</h2><p><font color="red" size="3">三台服务器都要执行</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置本地解析文件hosts</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vi /etc/hosts</span></span><br><span class="line">192.168.137.10 api.huisebug.com</span><br><span class="line">192.168.137.11 node1.huisebug.com</span><br><span class="line">192.168.137.12 node2.huisebug.com</span><br><span class="line"><span class="meta">#</span><span class="bash"> 开放端口（24007是gluster服务运行所需的端口号）如果关闭了防火墙就省略此步操作。</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> iptables -I INPUT -p tcp --dport 24007 -j ACCEPT</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建存储目录</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir /opt/gfs_data</span></span><br><span class="line"></span><br><span class="line">为了方便管理可使用一块新硬盘新建一个分区将其挂载到 /opt/gfs_data</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看硬盘</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> fdisk -l /dev/sdb</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 建立分区并格式化为xfs类型</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> fdisk /dev/sdb</span></span><br><span class="line">1.  n 建立新分区</span><br><span class="line">2.  p 建立主分区</span><br><span class="line">3.  1 分区号为1</span><br><span class="line">4.  一直回车默认，将整块磁盘建立为一个分区，使用全部空间</span><br><span class="line">5.  w 保存退出</span><br><span class="line"><span class="meta">#</span><span class="bash"> 使分区生效</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> partprobe /dev/sdb</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 强制格式化为xfs</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkfs.xfs -f /dev/sdb1</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 手动将/dev/sdb1分区挂载到/opt/gfs_data</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mount /dev/sdb1 /opt/gfs_data</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 开机自动挂载</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&quot;/dev/sdb1 /opt/gfs_data xfs defaults 0 0&quot;</span> &gt;&gt; /etc/fstab</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看挂载状态</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> df –hT</span></span><br></pre></td></tr></table></figure><h2 id="添加节点到集群"><a href="#添加节点到集群" class="headerlink" title="添加节点到集群"></a>添加节点到集群</h2><p>三台服务器都安装好服务并成功启动后</p><p>执行probe操作即将三台服务器的gluster服务建立集群，选择其中任意一台执行probe操作，将会建立一个集群，执行完成后在任意一台执行以下命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gluster peer status</span><br></pre></td></tr></table></figure><p>将会看到其他两台的信息<br>执⾏操作的本机不需要 probe本机，并且只需执行一次,此处我在api.huisebug.com上操作。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gluster peer probe node1.huisebug.com</span><br><span class="line">gluster peer probe node2.huisebug.com</span><br></pre></td></tr></table></figure><p>查看状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gluster peer status </span><br></pre></td></tr></table></figure><h2 id="volume-类型"><a href="#volume-类型" class="headerlink" title="volume 类型"></a>volume 类型</h2><p>GlusterFS中的volume的模式有很多中，包括以下⼏种：</p><ul><li>  分布卷（默认模式）：即DHT, 也叫 分布卷: 将⽂件已hash算法随机分布到⼀台服务器节点中存储。</li><li>  复制模式：即AFR, 创建volume 时带 replica x 数量: 将⽂件复制到replica x个节点中。</li><li>  条带模式：即Striped, 创建volume 时带 stripe x 数量：将⽂件切割成数据块，分别存储到 stripe x 个节点中 ( 类似raid 0 )。</li><li>  分布式条带模式：最少需要4台服务器才能创建。 创建volume 时stripe 2 server = 4个节点： 是DHT 与 Striped 的组合型。</li><li>  分布式复制模式：最少需要4台服务器才能创建。 创建volume 时replica 2 server =4 个节点：是DHT 与 AFR 的组合型。</li><li>  条带复制卷模式：最少需要4台服务器才能创建。 创建volume 时stripe 2 replica 2 server = 4 个节点： 是 Striped 与 AFR 的组合型。</li><li>  三种模式混合： ⾄少需要8台 服务器才能创建。 stripe 2 replica 2 ,每4个节点组成⼀个组。<br>可参考以下链接<br><a href="http://www.cnblogs.com/jicki/p/5801712.html">http://www.cnblogs.com/jicki/p/5801712.html</a></li></ul><p><font color="red" size="5">为了项目要求，我这里建立复制模式的volume</font></p><h2 id="建立复制卷"><a href="#建立复制卷" class="headerlink" title="建立复制卷"></a>建立复制卷</h2><p>现在整个gluster集群是一个整体，任意节点执行都可以</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 建立复制卷，卷名为test-volume</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume create test-volume replica 3 \</span></span><br><span class="line"><span class="bash">api.huisebug.com:/opt/gfs_data \</span></span><br><span class="line"><span class="bash">node1.huisebug.com:/opt/gfs_data \</span></span><br><span class="line"><span class="bash">node2.huisebug.com:/opt/gfs_data \</span></span><br><span class="line"><span class="bash">force</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 开启复制卷</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume start test-volume</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看volume状态</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume info</span></span><br><span class="line">Volume Name: test-volume &lt;br&gt;Type: Replicate &lt;br&gt;Volume ID: b52e2b36-0a88-43c5-b596-5ec0e01ca529 &lt;br&gt;Status: Started &lt;br&gt;Snapshot Count: 0 &lt;br&gt;Number of Bricks: 1 x 3 = 3&lt;br&gt; Transport-type: tcp &lt;br&gt;Bricks: &lt;br&gt;Brick1: api.huisebug.com:/opt/gfs_data &lt;br&gt;Brick2: node1.huisebug.com:/opt/gfs_data &lt;br&gt;Brick3: node2.huisebug.com:/opt/gfs_data &lt;br&gt;Options Reconfigured: &lt;br&gt;transport.address-family: inet &lt;br&gt;nfs.disable: on &lt;br&gt;performance.client-io-threads: off </span><br></pre></td></tr></table></figure><h2 id="配额限制"><a href="#配额限制" class="headerlink" title="配额限制"></a>配额限制</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开启 指定 volume 的配额</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume quota test-volume <span class="built_in">enable</span></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 限制 指定 volume 的配额，我们这里的硬盘是50G，所以分配给他10G</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume quota test-volume limit-usage / 10GB</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 cache ⼤⼩, 默认32MB,这个千万别设置太大了，不然会导致挂载不上</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume <span class="built_in">set</span> test-volume performance.cache-size 160MB</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 io 线程, 太⼤会导致进程崩溃</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume <span class="built_in">set</span> test-volume performance.io-thread-count 16</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 ⽹络检测时间, 默认42s</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume <span class="built_in">set</span> test-volume network.ping-timeout 10</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 写缓冲区的⼤⼩, 默认1M</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume <span class="built_in">set</span> test-volume performance.write-behind-window-size 1024MB</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 回写 (写数据时间，先写入缓存内，再写入硬盘)</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume <span class="built_in">set</span> test-volume performance.write-behind on</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置好了voleme的配额后要重新启动卷</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume stop test-volume</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume start test-volume</span></span><br></pre></td></tr></table></figure><h2 id="挂载使用"><a href="#挂载使用" class="headerlink" title="挂载使用"></a>挂载使用</h2><p><font color="red" size="3">三台服务器都要执行</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 建立使用服务器的目录</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p /opt/gfs_datause</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 挂载关联</span></span><br><span class="line">mount –t 指定挂载的卷类型 主机地址:卷名 挂载到本地的哪个目录（主机地址输入哪台服务器的地址都可以）</span><br><span class="line"><span class="meta">$</span><span class="bash"> mount -t glusterfs api.huisebug.com:test-volume /opt/gfs_datause</span></span><br><span class="line">如果挂载失败，查看日志文件/var/log/glusterfs/opt-gfs_datause.log解决问题所在，我遇到是我把gluster volume set test-volume performance.cache-size 160MB设置为了4GB，导致无法挂载，然后我修改为了160MB，就可以了</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">查看挂载情况，如果要持续使用记得设置自动挂载</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> df -hT</span></span><br><span class="line">文件系统 类型 容量 已用 可用 已用% 挂载点 </span><br><span class="line">api.huisebug.com:test-volume fuse.glusterfs 10G 0 10G 0% /opt/gfs_datause </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">自动挂载</span></span><br><span class="line">echo &quot;api.huisebug.com:test-volume /opt/gfs_datause glusterfs defaults 0 0&quot; &gt;&gt; /etc/fstab</span><br></pre></td></tr></table></figure><p><font color="red" size="3">记得执行node1和node2上述操作</font></p><h2 id="测试同步效果"><a href="#测试同步效果" class="headerlink" title="测试同步效果"></a>测试同步效果</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@api gfs_datause]# touch &#123;1..10&#125; </span><br><span class="line">[root@api gfs_datause]# ls </span><br><span class="line">1 10 2 3 4 5 6 7 8 9 </span><br><span class="line"></span><br><span class="line">[root@node1 opt]# ls gfs_datause/</span><br><span class="line">1 10 2 3 4 5 6 7 8 9 </span><br><span class="line"></span><br><span class="line">[root@node2 gfs_datause]# ls</span><br><span class="line">1 10 2 3 4 5 6 7 8 9 </span><br></pre></td></tr></table></figure><p>GlusterFS搭建完毕！！！</p><h1 id="Keepalived"><a href="#Keepalived" class="headerlink" title="Keepalived"></a>Keepalived</h1><p>整个集群的高可用VIP</p><p><font color="red" size="3">三台服务器都要执行</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 安装keepalived和ipvs、socat</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> yum install -y socat keepalived ipvsadm</span></span><br></pre></td></tr></table></figure><h2 id="启动Keepalived服务"><a href="#启动Keepalived服务" class="headerlink" title="启动Keepalived服务"></a>启动Keepalived服务</h2><p>修改配置文件keepalived.conf为如下三台服务器的，这里我们暂时先让VIP运行起来，后续监听端口和进程后续再增加。</p><p>api</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim /etc/keepalived/keepalived.conf</span> </span><br><span class="line"></span><br><span class="line">! Configuration File for keepalived </span><br><span class="line">global_defs &#123;</span><br><span class="line"> router_id vip1 </span><br><span class="line">&#125; </span><br><span class="line">vrrp_instance VI_1 &#123; </span><br><span class="line"><span class="meta">#</span><span class="bash">三台配置此处均是BACKUP</span> </span><br><span class="line">state BACKUP </span><br><span class="line">interface ens33 </span><br><span class="line">virtual_router_id 55 </span><br><span class="line"><span class="meta">#</span><span class="bash">优先级</span> </span><br><span class="line">priority 95 </span><br><span class="line">advert_int 1 </span><br><span class="line">authentication &#123; </span><br><span class="line"> auth_type PASS </span><br><span class="line"> auth_pass 123456 </span><br><span class="line">&#125; </span><br><span class="line">virtual_ipaddress &#123;</span><br><span class="line"> 192.168.137.13 </span><br><span class="line">&#125; </span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure><p>node1</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim /etc/keepalived/keepalived.conf</span> </span><br><span class="line"></span><br><span class="line">! Configuration File for keepalived </span><br><span class="line">global_defs &#123;</span><br><span class="line"> router_id vip2</span><br><span class="line"> &#125; </span><br><span class="line">vrrp_instance VI_1 &#123; </span><br><span class="line"><span class="meta">#</span><span class="bash">三台配置此处均是BACKUP</span> </span><br><span class="line">state BACKUP</span><br><span class="line">interface ens33 </span><br><span class="line">virtual_router_id 55 </span><br><span class="line"><span class="meta">#</span><span class="bash">优先级</span> </span><br><span class="line">priority 90 </span><br><span class="line">advert_int 1 </span><br><span class="line">authentication &#123;</span><br><span class="line"> auth_type PASS </span><br><span class="line"> auth_pass 123456 </span><br><span class="line">&#125; </span><br><span class="line">virtual_ipaddress &#123; </span><br><span class="line">192.168.137.13</span><br><span class="line">&#125;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure><p>node2</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim /etc/keepalived/keepalived.conf</span> </span><br><span class="line"></span><br><span class="line">! Configuration File for keepalived </span><br><span class="line">global_defs &#123;</span><br><span class="line"> router_id vip3</span><br><span class="line"> &#125;</span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line"><span class="meta"> #</span><span class="bash">三台配置此处均是BACKUP</span> </span><br><span class="line">state BACKUP </span><br><span class="line">interface ens33 </span><br><span class="line">virtual_router_id 55 </span><br><span class="line"><span class="meta">#</span><span class="bash">优先级</span> </span><br><span class="line">priority 85 </span><br><span class="line">advert_int 1 </span><br><span class="line">authentication &#123;</span><br><span class="line"> auth_type PASS</span><br><span class="line"> auth_pass 123456 </span><br><span class="line">&#125; </span><br><span class="line">virtual_ipaddress &#123;</span><br><span class="line"> 192.168.137.13 </span><br><span class="line">&#125;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure><p>重启服务，自启</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> systemctl restart keepalived</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl <span class="built_in">enable</span> keepalived</span></span><br></pre></td></tr></table></figure><p>在优先级最高的那台查看VIP地址是否建立</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip addr</span></span><br></pre></td></tr></table></figure><p>验证可以使用新建一个终端来验证是否会连接到优先级高的那台服务器</p><h1 id="证书"><a href="#证书" class="headerlink" title="证书"></a>证书</h1><p>Kubernetes系统的各组件需要使⽤TLS证书对通信进⾏加密，本⽂档使⽤CloudFlare的PKI⼯具集cfssl来⽣成Certificate Authority (CA)和其它证书；⽣成的CA证书和秘钥⽂件如下：</p><ul><li>  ca-key.pem</li><li>  ca.pem</li><li>  kubernetes-key.pem</li><li>  kubernetes.pem</li><li>  kube-proxy.pem</li><li>  kube-proxy-key.pem</li><li>  admin.pem</li><li>  admin-key.pem</li></ul><p>使⽤证书的组件如下：</p><ul><li>  etcd：使⽤ ca.pem、kubernetes-key.pem、kubernetes.pem；</li><li>  kube-apiserver：使⽤ ca.pem、kubernetes-key.pem、kubernetes.pem；</li><li>  kubelet：使⽤ ca.pem；</li><li>  kube-proxy：使⽤ ca.pem、kube-proxy-key.pem、kubeproxy.pem；</li><li>  kubectl：使⽤ ca.pem、admin-key.pem、admin.pem；</li><li>  kube-controller-manager：使⽤ ca-key.pem、ca.pem</li></ul><p>注意：以下操作都在 <strong>api</strong> 节点即 <strong>192.168.137.10</strong>这台主机上执⾏，证书只需要创建⼀次即可，以后在向集群中添加新节点时只要将**/etc/kubernetes/**⽬录下的证书拷⻉到新节点上即可。</p><h2 id="安装cfssl"><a href="#安装cfssl" class="headerlink" title="安装cfssl"></a>安装cfssl</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64</span><br><span class="line">chmod +x cfssl_linux-amd64</span><br><span class="line">mv cfssl_linux-amd64 /usr/local/bin/cfssl</span><br><span class="line">wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64</span><br><span class="line">chmod +x cfssljson_linux-amd64</span><br><span class="line">mv cfssljson_linux-amd64 /usr/local/bin/cfssljson</span><br><span class="line">wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64</span><br><span class="line">chmod +x cfssl-certinfo_linux-amd64</span><br><span class="line">mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo</span><br><span class="line">export PATH=/usr/local/bin:$PATH</span><br></pre></td></tr></table></figure><h2 id="创建ca配置文件"><a href="#创建ca配置文件" class="headerlink" title="创建ca配置文件"></a>创建ca配置文件</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir /root/ssl</span><br><span class="line">cd /root/ssl</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下面的操作是将模板文件复制过来，可以不操作，直接建立。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cfssl print-defaults config &gt; config.json</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cfssl print-defaults csr &gt; csr.json</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cat config.json &gt; ca-config.json</span></span><br></pre></td></tr></table></figure><p>根据config.json⽂件的格式创建如下的ca-config.json⽂件<br>过期时间设置成了 87600h(10年)<br>修改 ca-config.json内容为如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">vim ca-config.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;signing&quot;: &#123;</span><br><span class="line">        &quot;default&quot;: &#123;</span><br><span class="line">            &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;profiles&quot;: &#123;</span><br><span class="line">            &quot;kubernetes&quot;: &#123;</span><br><span class="line">                &quot;usages&quot;: [</span><br><span class="line">                    &quot;signing&quot;,</span><br><span class="line">                    &quot;key encipherment&quot;,</span><br><span class="line">                    &quot;server auth&quot;,</span><br><span class="line">                    &quot;client auth&quot;</span><br><span class="line">                ],</span><br><span class="line">                &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>字段说明:</p><ul><li>  ca-config.json ：可以定义多个profiles，分别指定不同的过期时间、使⽤场景等参数；后续在签名证书时使⽤某个profile；</li><li>  signing ：表示该证书可⽤于签名其它证书；⽣成的 ca.pem 证书中CA=TRUE ；</li><li>  server auth ：表示client可以⽤该 CA 对server提供的证书进⾏验证；</li><li>  client auth ：表示server可以⽤该CA对client提供的证书进⾏验证；</li></ul><h2 id="创建CA证书"><a href="#创建CA证书" class="headerlink" title="创建CA证书"></a>创建CA证书</h2><h3 id="创建-CA-证书签名请求"><a href="#创建-CA-证书签名请求" class="headerlink" title="创建 CA 证书签名请求"></a>创建 CA 证书签名请求</h3><p>创建 ca-csr.json ⽂件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">vim ca-csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>  “CN”： Common Name ，kube-apiserver 从证书中提取该字段作为请求的⽤户名 (User Name)；浏览器使⽤该字段验证⽹站是否合法；</li><li>  “O”： Organization ，kube-apiserver 从证书中提取该字段作为请求⽤户所属的组(Group)；</li></ul><h3 id="⽣成-CA-证书和私钥ca-key-pem-ca-pem"><a href="#⽣成-CA-证书和私钥ca-key-pem-ca-pem" class="headerlink" title="⽣成 CA 证书和私钥ca-key.pem ca.pem"></a>⽣成 CA 证书和私钥ca-key.pem ca.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert -initca ca-csr.json | cfssljson -bare ca</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls</span></span><br><span class="line">ca-config.json ca.csr ca-csr.json ca-key.pem ca.pem config.json csr.json</span><br></pre></td></tr></table></figure><h2 id="创建-kubernetes-证书"><a href="#创建-kubernetes-证书" class="headerlink" title="创建 kubernetes 证书"></a>创建 kubernetes 证书</h2><h3 id="创建-kubernetes-证书签名请求⽂件-kubernetes-csr-json"><a href="#创建-kubernetes-证书签名请求⽂件-kubernetes-csr-json" class="headerlink" title="创建 kubernetes 证书签名请求⽂件 kubernetes-csr.json"></a>创建 kubernetes 证书签名请求⽂件 kubernetes-csr.json</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">cat ca-csr.json &gt; kubernetes-csr.json</span><br><span class="line">vim kubernetes-csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">        &quot;127.0.0.1&quot;,</span><br><span class="line">        &quot;192.168.137.10&quot;,</span><br><span class="line">        &quot;192.168.137.11&quot;,</span><br><span class="line">        &quot;192.168.137.12&quot;,</span><br><span class="line">        &quot;192.168.137.13&quot;,</span><br><span class="line">        &quot;10.254.0.1&quot;,</span><br><span class="line">        &quot;kubernetes&quot;,</span><br><span class="line">        &quot;kubernetes.default&quot;,</span><br><span class="line">        &quot;kubernetes.default.svc&quot;,</span><br><span class="line">        &quot;kubernetes.default.svc.cluster&quot;,</span><br><span class="line">        &quot;kubernetes.default.svc.cluster.local&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>  如果 hosts 字段不为空则需要指定授权使⽤该证书的 <strong>IP</strong>或域名列表，由于该证书后续被 etcd 集群和 kubernetes master集群使⽤，所以上⾯分别指定了 etcd 集群、 kubernetes master集群的主机 IP 和<strong>kubernetes</strong> 服务的集群 <strong>IP</strong>（⼀般是 kube-apiserver 指定的service-cluster-ip-range ⽹段的第⼀个IP，如 10.254.0.1。</li><li>  hosts中的内容可以为空，即使按照上⾯的置，向集群中增加新节点后也不需要重新⽣成证书。</li></ul><h3 id="⽣成-kubernetes-证书和私钥kubernetes-key-pem-kubernetes-pem"><a href="#⽣成-kubernetes-证书和私钥kubernetes-key-pem-kubernetes-pem" class="headerlink" title="⽣成 kubernetes 证书和私钥kubernetes-key.pem kubernetes.pem"></a>⽣成 kubernetes 证书和私钥kubernetes-key.pem kubernetes.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls kubernetes*</span></span><br><span class="line">kubernetes.csr kubernetes-csr.json kubernetes-key.pem kubernetes.pem</span><br></pre></td></tr></table></figure><h2 id="创建admin证书"><a href="#创建admin证书" class="headerlink" title="创建admin证书"></a>创建admin证书</h2><h3 id="创建-admin-证书签名请求⽂件-admin-csr-json-："><a href="#创建-admin-证书签名请求⽂件-admin-csr-json-：" class="headerlink" title="创建 admin 证书签名请求⽂件 admin-csr.json ："></a>创建 admin 证书签名请求⽂件 admin-csr.json ：</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cat ca-csr.json &gt; admin-csr.json</span><br><span class="line">vim admin-csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;admin&quot;,</span><br><span class="line">    &quot;hosts&quot;:[],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;system:masters&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>  后续 kube-apiserver 使⽤ RBAC 对客户端(如 kubelet 、 kubeproxy、 Pod)请求进⾏授权；</li><li>  kube-apiserver 预定义了⼀些 RBAC 使⽤的 RoleBindings ，如cluster-admin 将Group system:masters 与 Role cluster-admin绑定，该 Role 授予了调⽤kube-apiserver 的所有 <strong>API</strong>的权限；</li><li>  OU 指定该证书的 Group 为 system:masters ， kubelet 使⽤该证书访问kube-apiserver 时 ，由于证书被 CA签名，所以认证通过，同时由于证书⽤户组为经过预授权的 system:masters，所以被授予访问所有 API 的权限；</li></ul><h3 id="⽣成-admin-证书和私钥admin-key-pem-admin-pem"><a href="#⽣成-admin-证书和私钥admin-key-pem-admin-pem" class="headerlink" title="⽣成 admin 证书和私钥admin-key.pem admin.pem"></a>⽣成 admin 证书和私钥admin-key.pem admin.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls admin*</span></span><br><span class="line">admin.csr admin-csr.json admin-key.pem admin.pem</span><br></pre></td></tr></table></figure><h2 id="创建-kube-controller-manager-证书"><a href="#创建-kube-controller-manager-证书" class="headerlink" title="创建 kube-controller-manager 证书"></a>创建 kube-controller-manager 证书</h2><h3 id="创建-kube-controller-manager-证书签名请求⽂件-kube-controller-manager-csr-json"><a href="#创建-kube-controller-manager-证书签名请求⽂件-kube-controller-manager-csr-json" class="headerlink" title="创建 kube-controller-manager 证书签名请求⽂件 kube-controller-manager-csr.json"></a>创建 kube-controller-manager 证书签名请求⽂件 kube-controller-manager-csr.json</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim kube-controller-manager-csr.json</span></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;system:kube-controller-manager&quot;,</span><br><span class="line">    &quot;hosts&quot;: [],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;system:kube-controller-manager&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="⽣成kube-controller-manager证书和私钥kube-controller-manager-key-pem-kube-controller-manager-pem"><a href="#⽣成kube-controller-manager证书和私钥kube-controller-manager-key-pem-kube-controller-manager-pem" class="headerlink" title="⽣成kube-controller-manager证书和私钥kube-controller-manager-key.pem kube-controller-manager.pem"></a>⽣成kube-controller-manager证书和私钥kube-controller-manager-key.pem kube-controller-manager.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls kube-controller-manager*</span></span><br><span class="line">kube-controller-manager.csr kube-controller-manager-csr.json kube-controller-manager-key.pem kube-controller-manager.pem</span><br></pre></td></tr></table></figure><h2 id="创建-kube-scheduler-证书"><a href="#创建-kube-scheduler-证书" class="headerlink" title="创建 kube-scheduler 证书"></a>创建 kube-scheduler 证书</h2><h3 id="创建kube-scheduler-证书签名请求⽂件-kube-scheduler-csr-json"><a href="#创建kube-scheduler-证书签名请求⽂件-kube-scheduler-csr-json" class="headerlink" title="创建kube-scheduler 证书签名请求⽂件 kube-scheduler-csr.json"></a>创建kube-scheduler 证书签名请求⽂件 kube-scheduler-csr.json</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim kube-scheduler-csr.json</span></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;:&quot;system:kube-scheduler&quot;,</span><br><span class="line">    &quot;hosts&quot;: [],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;system:kube-scheduler&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="⽣成kube-scheduler证书和私钥kube-scheduler-key-pem-kube-scheduler-pem"><a href="#⽣成kube-scheduler证书和私钥kube-scheduler-key-pem-kube-scheduler-pem" class="headerlink" title="⽣成kube-scheduler证书和私钥kube-scheduler-key.pem kube-scheduler.pem"></a>⽣成kube-scheduler证书和私钥kube-scheduler-key.pem kube-scheduler.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls kube-scheduler*</span></span><br><span class="line">kube-scheduler.csr kube-scheduler-csr.json kube-scheduler-key.pem kube-scheduler.pem</span><br></pre></td></tr></table></figure><h2 id="创建-front-proxy-证书"><a href="#创建-front-proxy-证书" class="headerlink" title="创建 front-proxy 证书"></a>创建 front-proxy 证书</h2><h3 id="创建-front-proxy-证书签名请求⽂件front-proxy-ca-csr-json"><a href="#创建-front-proxy-证书签名请求⽂件front-proxy-ca-csr-json" class="headerlink" title="创建 front-proxy 证书签名请求⽂件front-proxy-ca-csr.json"></a>创建 front-proxy 证书签名请求⽂件<strong>front-proxy-ca-csr.json</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim front-proxy-ca-csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="生成-front-proxy-ca证书和私钥front-proxy-ca-key-pem-front-proxy-ca-pem"><a href="#生成-front-proxy-ca证书和私钥front-proxy-ca-key-pem-front-proxy-ca-pem" class="headerlink" title="生成 front-proxy-ca证书和私钥front-proxy-ca-key.pem  front-proxy-ca.pem"></a>生成 front-proxy-ca证书和私钥front-proxy-ca-key.pem  front-proxy-ca.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-ca</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls front-proxy-ca*</span></span><br><span class="line">front-proxy-ca.csr front-proxy-ca-csr.json front-proxy-ca-key.pem front-proxy-ca.pem</span><br></pre></td></tr></table></figure><h3 id="创建-front-proxy-client-证书签名请求⽂件-front-proxy-client-csr-json"><a href="#创建-front-proxy-client-证书签名请求⽂件-front-proxy-client-csr-json" class="headerlink" title="创建 front-proxy-client 证书签名请求⽂件 front-proxy-client-csr.json"></a>创建 front-proxy-client 证书签名请求⽂件 front-proxy-client-csr.json</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim front-proxy-client-csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;front-proxy-client&quot;,</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="⽣成front-proxy-client证书和私钥front-proxy-client-key-pem-front-proxy-client-pem"><a href="#⽣成front-proxy-client证书和私钥front-proxy-client-key-pem-front-proxy-client-pem" class="headerlink" title="⽣成front-proxy-client证书和私钥front-proxy-client-key.pem  front-proxy-client.pem"></a>⽣成front-proxy-client证书和私钥front-proxy-client-key.pem  front-proxy-client.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert \</span></span><br><span class="line"><span class="bash">-ca=front-proxy-ca.pem \</span></span><br><span class="line"><span class="bash">-ca-key=front-proxy-ca-key.pem \</span></span><br><span class="line"><span class="bash">-config=ca-config.json \</span></span><br><span class="line"><span class="bash">-profile=kubernetes \</span></span><br><span class="line"><span class="bash">front-proxy-client-csr.json | cfssljson -bare front-proxy-client</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls front-proxy-client*</span></span><br><span class="line">front-proxy-client.csr front-proxy-client-csr.json front-proxy-client-key.pem front-proxy-client.pem</span><br></pre></td></tr></table></figure><h2 id="校验证书"><a href="#校验证书" class="headerlink" title="校验证书"></a>校验证书</h2><p>以 kubernetes 证书为例</p><h3 id="使⽤-opsnssl-命令"><a href="#使⽤-opsnssl-命令" class="headerlink" title="使⽤ opsnssl 命令"></a>使⽤ opsnssl 命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl x509 -noout -text -in kubernetes.pem</span><br></pre></td></tr></table></figure><ul><li>  确认 Issuer 字段的内容和 ca-csr.json ⼀致；</li><li>  确认 Subject 字段的内容和 kubernetes-csr.json ⼀致；</li><li>  确认 X509v3 Subject Alternative Name 字段的内容和 kubernetescsr.json ⼀致；</li><li>  确认 X509v3 Key Usage、Extended Key Usage 字段的内容和 caconfig.json 中kubernetes profile ⼀致；</li></ul><h3 id="使⽤-cfssl-certinfo-命令"><a href="#使⽤-cfssl-certinfo-命令" class="headerlink" title="使⽤ cfssl-certinfo 命令"></a>使⽤ cfssl-certinfo 命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cfssl-certinfo -cert kubernetes.pem</span><br></pre></td></tr></table></figure><h2 id="分发证书"><a href="#分发证书" class="headerlink" title="分发证书"></a>分发证书</h2><p><font color="red" size="3">三台服务器都要执行</font></p><p>将⽣成的证书和秘钥⽂件（后缀名为 .pem ）拷⻉到所有机器的/etc/kubernetes/ssl<br>⽬录下备⽤；具体复制方法自己操作</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/kubernetes/ssl</span><br><span class="line">cp -rf *.pem /etc/kubernetes/ssl</span><br></pre></td></tr></table></figure><h1 id="Haproxy"><a href="#Haproxy" class="headerlink" title="Haproxy"></a>Haproxy</h1><p>1.结合keepalived保证整个集群的HA（高可用）</p><p>2.使用 keepalived 和 haproxy 实现 kube-apiserver 高可用的步骤：</p><ul><li>  keepalived 提供 kube-apiserver 对外服务的 VIP；</li><li>  haproxy 监听 VIP，后端连接所有 kube-apiserver实例，提供健康检查和负载均衡功能；</li><li>  运行 keepalived 和 haproxy 的节点称为 LB 节点。由于 keepalived是一主多备运行模式，故至少两个 LB 节点。复用 master 节点的三台机器，haproxy监听的端口(9443) 需要与 kube-apiserver 的端口6443不同，避免冲突。</li><li>  keepalived 在运行过程中周期检查本机的 haproxy 进程状态，如果检测到 haproxy进程异常，则触发重新选主的过程，VIP 将飘移到新选出来的主节点，从而实现 VIP的高可用。</li><li>  所有组件（如kubectl、kube-apiserver、kube-controller-manager、kube-scheduler、kubelet等）都通过VIP和haproxy监听的9443 端口访问kube-apiserver服务。</li></ul><p><font color="red" size="3">三台服务器都要执行</font></p><h2 id="安装haproxy"><a href="#安装haproxy" class="headerlink" title="安装haproxy"></a>安装haproxy</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> yum -y install haproxy</span></span><br></pre></td></tr></table></figure><p>修改haproxy配置文件如下，这里将9443端口（这个端口将代理到各k8s集群api端口6443）和haproxy进程绑定在一起，并定义后端服务器。并且开启100端口监听haproxy进程状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">haproxy 配置文件：</span></span><br><span class="line"><span class="meta">cat&gt;</span><span class="bash">haproxy.cfg&lt;&lt;<span class="string">EOF</span></span></span><br><span class="line">global</span><br><span class="line">    log /dev/log    local0</span><br><span class="line">    log /dev/log    local1 notice</span><br><span class="line">    chroot /var/lib/haproxy</span><br><span class="line">    stats socket /run/haproxy/admin.sock mode 660 level admin</span><br><span class="line">    stats timeout 30s</span><br><span class="line">    user haproxy</span><br><span class="line">    group haproxy</span><br><span class="line">    daemon</span><br><span class="line">    nbproc 1</span><br><span class="line">    </span><br><span class="line">defaults</span><br><span class="line">    log     global</span><br><span class="line">    timeout connect 5000</span><br><span class="line">    timeout client  10m</span><br><span class="line">    timeout server  10m</span><br><span class="line"></span><br><span class="line">listen  admin_stats</span><br><span class="line">    bind 0.0.0.0:100</span><br><span class="line">    mode http</span><br><span class="line">    log 127.0.0.1 local0 err</span><br><span class="line">    stats refresh 30s </span><br><span class="line">    stats uri /status</span><br><span class="line">    stats realm welcome login\ Haproxy</span><br><span class="line">    stats auth admin:123456</span><br><span class="line">    stats hide-version</span><br><span class="line">    stats admin if TRUE</span><br><span class="line"></span><br><span class="line"> listen kube-master</span><br><span class="line">     bind 0.0.0.0:9443</span><br><span class="line">     mode tcp</span><br><span class="line">     option tcplog</span><br><span class="line">     balance source</span><br><span class="line">     server api 192.168.137.10:6443 check inter 2000 fall 2 rise 2 weight 1</span><br><span class="line">     server node1 192.168.137.11:6443 check inter 2000 fall 2 rise 2 weight 1</span><br><span class="line">     server node2 192.168.137.12:6443 check inter 2000 fall 2 rise 2 weight 1</span><br><span class="line">EOF</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>建立haproxy的工作目录并启动服务，开机自启，工作目录/run/haproxy重启服务器会丢失，所以将其加入到随系统启动而建立</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 赋予执行权限</span></span><br><span class="line">chmod +x /etc/rc.d/rc.local</span><br><span class="line">echo &quot;mkdir -p /run/haproxy&quot; &gt;&gt; /etc/rc.local </span><br><span class="line">mkdir -p /run/haproxy </span><br><span class="line">systemctl start haproxy </span><br><span class="line">systemctl enable haproxy</span><br></pre></td></tr></table></figure><p>查看是否监听9443端口。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">netstat -lntp | grep 9443</span><br><span class="line">tcp 0 0 0.0.0.0:9443 0.0.0.0:* LISTEN 10019/haproxy</span><br></pre></td></tr></table></figure><h2 id="增加keepalived配置，使其嗅探haproxy的状态"><a href="#增加keepalived配置，使其嗅探haproxy的状态" class="headerlink" title="增加keepalived配置，使其嗅探haproxy的状态"></a>增加keepalived配置，使其嗅探haproxy的状态</h2><p><font color="red" size="3">三台服务器都要执行</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vrrp_script check-haproxy &#123;</span><br><span class="line">    script &quot;killall -0 haproxy&quot;</span><br><span class="line">    interval 1</span><br><span class="line">    weight -20</span><br><span class="line">&#125;</span><br><span class="line">并且在VIP下面配置track_script以对应上面的脚本</span><br><span class="line">  virtual_ipaddress &#123;</span><br><span class="line">         192.168.137.13</span><br><span class="line">     &#125;</span><br><span class="line">   track_script &#123;</span><br><span class="line">     check-haproxy</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>使用 killall -0 haproxy 命令检查所在节点的 haproxy<br>进程是否正常。如果异常则将权重减少（-20）,从而触发重新选主过程；</p><p>例如api节点配置<br><img src="/2020/08/26/k8s/media/07aa73fd92ffd1f9e62dbe8c48c64de2.png"></p><p>重启服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart keepalived</span><br></pre></td></tr></table></figure><p>验证HA效果，用户名和密码是之前开启的100端口中定义的admin:123456<br><img src="/2020/08/26/k8s/media/5f9ad336cd3c31746b5f7a08ac951d0a.png"><br><img src="/2020/08/26/k8s/media/bfbddf798678dc16cf6abd1a4f1514b0.png"></p><h1 id="kubectl"><a href="#kubectl" class="headerlink" title="kubectl"></a>kubectl</h1><p>kubectl作为k8s集群的客户端工具，首先需要安装来生成集群配置文件kubeconfig<br>kubernetes二进制下载地址，为了快速下载，我们可以在机器上下载后再上传到其他主机<br>下载地址：<br><a href="https://dl.k8s.io/v1.12.4/kubernetes-server-linux-amd64.tar.gz">https://dl.k8s.io/v1.12.4/kubernetes-server-linux-amd64.tar.gz</a></p><p>因为我们要做HA集群，所以这里三台服务器都会作为api服务器，所以我们直接将所有二进制程序放到指定地方</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget https://dl.k8s.io/v1.12.4/kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">tar zxf kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">cd kubernetes</span><br><span class="line">cp -rf server/bin/&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet&#125; /usr/local/bin/</span><br></pre></td></tr></table></figure><h2 id="创建-kubectl所需kubeconfig文件"><a href="#创建-kubectl所需kubeconfig文件" class="headerlink" title="创建 kubectl所需kubeconfig文件"></a>创建 kubectl所需kubeconfig文件</h2><p>主要是用于kubectl命令和kubelet服务进行获取apiserver信息并且集群角色cluster-admin与自建用户admin进行绑定</p><p>kubeconfig.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export KUBE_APISERVER=&quot;https://192.168.137.13:9443&quot;</span><br><span class="line">kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/ssl/ca.pem --embed-certs=true --server=$&#123;KUBE_APISERVER&#125;</span><br><span class="line">kubectl config set-credentials admin --client-certificate=/etc/kubernetes/ssl/admin.pem --embed-certs=true --client-key=/etc/kubernetes/ssl/admin-key.pem</span><br><span class="line">kubectl config set-context kubernetes --cluster=kubernetes --user=admin</span><br><span class="line">kubectl config use-context kubernetes</span><br></pre></td></tr></table></figure><ul><li>  KUBEAPISERVER变量中定义的是VIP地址。</li><li>  admin.pem证书OU字段值为 system:masters,kube-apiserver预定义的RoleBinding cluster-admin将Group system:masters与 Role cluster-admin绑定，该 Role 授予了调⽤ kube-apiserver相关 API 的权限；</li><li>  ⽣成的 kubeconfig 被保存到 ~/.kube/config ⽂件;</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source kubeconfig.sh</span><br></pre></td></tr></table></figure><h1 id="k8s集群建立前预备配置文件"><a href="#k8s集群建立前预备配置文件" class="headerlink" title="k8s集群建立前预备配置文件"></a>k8s集群建立前预备配置文件</h1><h2 id="创建-TLS-Bootstrapping-token文件"><a href="#创建-TLS-Bootstrapping-token文件" class="headerlink" title="创建 TLS Bootstrapping token文件"></a>创建 TLS Bootstrapping token文件</h2><h3 id="Token-auth-file"><a href="#Token-auth-file" class="headerlink" title="Token auth file"></a>Token auth file</h3><p>Token可以是任意的包涵128 bit的字符串，可以使⽤安全的随机数发⽣器⽣成</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x |tr -d &#x27; &#x27;)</span><br></pre></td></tr></table></figure><p>建立token.csv</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; token.csv &lt;&lt; EOF</span><br><span class="line"><span class="meta">$</span><span class="bash">&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,<span class="string">&quot;system:kubelet-bootstrap&quot;</span></span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat token.csv</span><br><span class="line">b156d4d29fe7a73e554a145fc996e3c6,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;</span><br></pre></td></tr></table></figure><p>注意：在进⾏后续操作前请检查 <strong>token.csv</strong>⽂件，确认其中的**${BOOTSTRAP_TOKEN}** 环境变量已经被真实的值替换。</p><p><strong>BOOTSTRAP_TOKEN</strong> 将被写⼊到 kube-apiserver 使⽤的 token.csv ⽂件和 kubelet使⽤的 bootstrap.kubeconfig ⽂件，如果后续重新⽣成了BOOTSTRAP_TOKEN，则需要：</p><ol><li>更新 token.csv ⽂件，分发到所有机器 (master 和 node）的/etc/kubernetes/⽬录下，分发到node节点上⾮必需；</li><li>重新⽣成 bootstrap.kubeconfig ⽂件，分发到所有 node 机器的/etc/kubernetes/⽬录下；</li><li>重启 kube-apiserver 和 kubelet 进程；</li><li>重新 approve kubelet 的 csr 请求；</li></ol><p>!!!注意，需要将token.csv文件复制到另外两台服务器，不要重新生成token.csv文件，文件目录是之前的ssl文件的上一级目录/etc/kubernetes</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp token.csv /etc/kubernetes/</span><br></pre></td></tr></table></figure><h2 id="创建服务需要的kubeconfig文件"><a href="#创建服务需要的kubeconfig文件" class="headerlink" title="创建服务需要的kubeconfig文件"></a>创建服务需要的kubeconfig文件</h2><p>在开启了 TLS 的集群中，每当与集群交互的时候少不了的是身份认证，使用kubeconfig（即证书） 和 token 两种认证方式是最简单也最通用的认证方式。</p><p><font color="red" size="6">总之kubeconfig就是为访问集群所作的配置。</font></p><h3 id="创建-kubelet服务所需-bootstrapping-kubeconfig文件"><a href="#创建-kubelet服务所需-bootstrapping-kubeconfig文件" class="headerlink" title="创建 kubelet服务所需 bootstrapping.kubeconfig文件"></a>创建 kubelet服务所需 bootstrapping.kubeconfig文件</h3><p>主要是用于生成的bootstrap.kubeconfig提供给kubelet进行node注册<br>bootstrap.kubeconfig.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/kubernetes</span><br><span class="line">export KUBE_APISERVER=&quot;https://192.168.137.13:9443&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置集群参数</span></span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">--certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">--embed-certs=true \</span><br><span class="line">--server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">--kubeconfig=bootstrap.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置客户端认证参数</span></span><br><span class="line">kubectl config set-credentials kubelet-bootstrap \</span><br><span class="line">--token=$&#123;BOOTSTRAP_TOKEN&#125; \</span><br><span class="line">--kubeconfig=bootstrap.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置上下文参数</span></span><br><span class="line">kubectl config set-context default \</span><br><span class="line">--cluster=kubernetes \</span><br><span class="line">--user=kubelet-bootstrap \</span><br><span class="line">--kubeconfig=bootstrap.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置默认上下文</span></span><br><span class="line">kubectl config use-context default --kubeconfig=bootstrap.kubeconfig</span><br></pre></td></tr></table></figure><ul><li>  上述也用到了变量BOOTSTRAP_TOKEN，所以kubeconfig生成后也是复制到其他服务</li><li>  –embed-certs 为 true 时表示将 certificate-authority 证书写⼊到⽣成的bootstrap.kubeconfig⽂件中；</li><li>  设置客户端认证参数时没有指定秘钥和证书，后续由 kubeapiserver⾃动⽣成；</li></ul><h3 id="创建-kubelet服务所需-kubelet-kubeconfig文件"><a href="#创建-kubelet服务所需-kubelet-kubeconfig文件" class="headerlink" title="创建 kubelet服务所需 kubelet.kubeconfig文件"></a>创建 kubelet服务所需 kubelet.kubeconfig文件</h3><p>这里我们直接使用/root/.kube/config文件，也可以自己建立kubelet证书后来生成</p><h3 id="创建-kube-controller-manager服务所需kube-controller-manager-kubeconfig-⽂件"><a href="#创建-kube-controller-manager服务所需kube-controller-manager-kubeconfig-⽂件" class="headerlink" title="创建 kube-controller-manager服务所需kube-controller-manager.kubeconfig ⽂件"></a>创建 kube-controller-manager服务所需kube-controller-manager.kubeconfig ⽂件</h3><p>kube-controller-manager.kubeconfig.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/kubernetes</span><br><span class="line">export KUBE_APISERVER=&quot;https://192.168.137.13:9443&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置集群参数</span></span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">--certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">--embed-certs=true \</span><br><span class="line">--server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">--kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置客户端认证参数</span></span><br><span class="line">kubectl config set-credentials system:kube-controller-manager \</span><br><span class="line">--client-certificate=/etc/kubernetes/ssl/kube-controller-manager.pem \</span><br><span class="line">--client-key=/etc/kubernetes/ssl/kube-controller-manager-key.pem \</span><br><span class="line">--embed-certs=true \</span><br><span class="line">--kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置上下文参数</span></span><br><span class="line">kubectl config set-context system:kube-controller-manager@kubernetes \</span><br><span class="line">--cluster=kubernetes \</span><br><span class="line">--user=system:kube-controller-manager \</span><br><span class="line">--kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置默认上下文</span></span><br><span class="line">kubectl config use-context system:kube-controller-manager@kubernetes --kubeconfig=kube-controller-manager.kubeconfig</span><br></pre></td></tr></table></figure><h3 id="创建-kube-scheduler服务所需kube-scheduler-kubeconfig-⽂件"><a href="#创建-kube-scheduler服务所需kube-scheduler-kubeconfig-⽂件" class="headerlink" title="创建 kube-scheduler服务所需kube-scheduler.kubeconfig ⽂件"></a>创建 kube-scheduler服务所需kube-scheduler.kubeconfig ⽂件</h3><p>kube-scheduler.kubeconfig.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/kubernetes</span><br><span class="line">export KUBE_APISERVER=&quot;https://192.168.137.13:9443&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置集群参数</span></span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">--certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">--embed-certs=true \</span><br><span class="line">--server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">--kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置客户端认证参数</span></span><br><span class="line">kubectl config set-credentials system:kube-scheduler \</span><br><span class="line">--client-certificate=/etc/kubernetes/ssl/kube-scheduler.pem \</span><br><span class="line">--client-key=/etc/kubernetes/ssl/kube-scheduler-key.pem \</span><br><span class="line">--embed-certs=true \</span><br><span class="line">--kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置上下文参数</span></span><br><span class="line">kubectl config set-context system:kube-scheduler@kubernetes \</span><br><span class="line">--cluster=kubernetes \</span><br><span class="line">--user=system:kube-scheduler \</span><br><span class="line">--kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置默认上下文</span></span><br><span class="line">kubectl config use-context system:kube-scheduler@kubernetes --kubeconfig=kube-scheduler.kubeconfig</span><br></pre></td></tr></table></figure><h3 id="执行脚本文件生成kubeconfig文件"><a href="#执行脚本文件生成kubeconfig文件" class="headerlink" title="执行脚本文件生成kubeconfig文件"></a>执行脚本文件生成kubeconfig文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">source /root/bootstrap.kubeconfig.sh</span><br><span class="line">source /root/kube-controller-manager.kubeconfig.sh</span><br><span class="line">source /root/kube-scheduler.kubeconfig.sh</span><br></pre></td></tr></table></figure><h3 id="分发-kubeconfig-⽂件"><a href="#分发-kubeconfig-⽂件" class="headerlink" title="分发 kubeconfig ⽂件"></a>分发 kubeconfig ⽂件</h3><p>将 kubeconfig ⽂件分发到其他服务器的 /etc/kubernetes/ ⽬录，具体怎么分发自己操作</p><h1 id="Etcd3-3-4"><a href="#Etcd3-3-4" class="headerlink" title="Etcd3.3.4"></a>Etcd3.3.4</h1><p><font color="red" size="3">三台服务器都要执行</font></p><h2 id="TLS-认证⽂件"><a href="#TLS-认证⽂件" class="headerlink" title="TLS 认证⽂件"></a>TLS 认证⽂件</h2><p>需要为 etcd 集群创建加密通信的 TLS 证书，这⾥复⽤以前创建的kubernetes 证书</p><p>ca.pem kubernetes-key.pem kubernetes.pem</p><p>kubernetes 证书的 hosts 字段列表中包含上⾯三台服务器的IP，否则后续证书校验会失败；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> wget https://github.com/coreos/etcd/releases/download/v3.3.4/etcd-v3.3.4-linux-amd64.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar zxf etcd-v3.3.4-linux-amd64.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mv etcd-v3.3.4-linux-amd64/etcd* /usr/<span class="built_in">local</span>/bin/</span></span><br></pre></td></tr></table></figure><h2 id="ectd服务services配置文件"><a href="#ectd服务services配置文件" class="headerlink" title="ectd服务services配置文件"></a>ectd服务services配置文件</h2><p>/usr/lib/systemd/system/etcd.service</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line"></span><br><span class="line">Description=Etcd Server</span><br><span class="line"></span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">After=network-online.target</span><br><span class="line"></span><br><span class="line">Wants=network-online.target</span><br><span class="line"></span><br><span class="line">Documentation=https://github.com/coreos</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line"></span><br><span class="line">Type=notify</span><br><span class="line"></span><br><span class="line">WorkingDirectory=/var/lib/etcd/</span><br><span class="line"></span><br><span class="line">EnvironmentFile=-/etc/etcd/etcd.conf</span><br><span class="line"></span><br><span class="line">ExecStart=/usr/local/bin/etcd </span><br><span class="line"></span><br><span class="line">--name $&#123;ETCD_NAME&#125; </span><br><span class="line"></span><br><span class="line">--cert-file=/etc/kubernetes/ssl/kubernetes.pem </span><br><span class="line"></span><br><span class="line">--key-file=/etc/kubernetes/ssl/kubernetes-key.pem </span><br><span class="line"></span><br><span class="line">--peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem </span><br><span class="line"></span><br><span class="line">--peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem </span><br><span class="line"></span><br><span class="line">--trusted-ca-file=/etc/kubernetes/ssl/ca.pem </span><br><span class="line"></span><br><span class="line">--peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem </span><br><span class="line"></span><br><span class="line">--initial-advertise-peer-urls $&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; </span><br><span class="line"></span><br><span class="line">--listen-peer-urls $&#123;ETCD_LISTEN_PEER_URLS&#125; </span><br><span class="line"></span><br><span class="line">--listen-client-urls $&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 </span><br><span class="line"></span><br><span class="line">--advertise-client-urls $&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; </span><br><span class="line"></span><br><span class="line">--initial-cluster-token $&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; </span><br><span class="line"></span><br><span class="line">--initial-cluster-infra1=https://192.168.137.10:2380,infra2=https://192.168.137.11:2380,infra3=https://192.168.137.12:2380 </span><br><span class="line"></span><br><span class="line">--initial-cluster-state new </span><br><span class="line"></span><br><span class="line">--data-dir=$&#123;ETCD_DATA_DIR&#125;</span><br><span class="line"></span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">RestartSec=5</span><br><span class="line"></span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line"></span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>  指定 etcd 的⼯作⽬录为 /var/lib/etcd ，数据⽬录为/var/lib/etcd，需在启动服务前创建这两个⽬录；</li><li>  为了保证通信安全，需要指定 etcd 的公私钥(cert-file和key-file)、Peers通信的公私钥和 CA证书(peer-cert-file、peer-key-file、peertrusted-ca-file)、客户端的CA证书（trusted-ca-file)；</li><li>  创建 kubernetes.pem 证书时使⽤的 kubernetes-csr.json ⽂件的hosts字段包含所有 <strong>etcd</strong> 节点的<strong>IP</strong>，否则证书校验会出错；</li><li>  –initial-cluster-state 值为 new 时， –name 的参数值必须位于–initial-cluster 列表中；<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /var/lib/etcd</span><br><span class="line">mkdir /etc/etcd</span><br></pre></td></tr></table></figure></li></ul><h2 id="环境变量配置⽂件"><a href="#环境变量配置⽂件" class="headerlink" title="环境变量配置⽂件"></a>环境变量配置⽂件</h2><p>/etc/etcd/etcd.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> [member]</span></span><br><span class="line">ETCD_NAME=infra1</span><br><span class="line">ETCD_DATA_DIR=&quot;/var/lib/etcd&quot;</span><br><span class="line">ETCD_LISTEN_PEER_URLS=&quot;https://192.168.137.10:2380&quot;</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.137.10:2379&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash">[cluster]</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.137.10:2380&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=https://192.168.137.10:2379</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这是192.168.137.10节点的配置，其他两个etcd节点只要将上⾯的IP地址改成相应节点的IP地址即可。ETCD_NAME换成对应节点的infra1/2/3。<br>/etc/etcd/etcd.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> [member]</span></span><br><span class="line">ETCD_NAME=infra2</span><br><span class="line">ETCD_DATA_DIR=&quot;/var/lib/etcd&quot;</span><br><span class="line">ETCD_LISTEN_PEER_URLS=&quot;https://192.168.137.11:2380&quot;</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.137.11:2379&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash">[cluster]</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.137.11:2380&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=https://192.168.137.11:2379</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> [member]</span></span><br><span class="line">ETCD_NAME=infra3</span><br><span class="line">ETCD_DATA_DIR=&quot;/var/lib/etcd&quot;</span><br><span class="line">ETCD_LISTEN_PEER_URLS=&quot;https://192.168.137.12:2380&quot;</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.137.12:2379&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash">[cluster]</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.137.12:2380&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.137.12:2379&quot;</span><br></pre></td></tr></table></figure><h2 id="启动etcd服务"><a href="#启动etcd服务" class="headerlink" title="启动etcd服务"></a>启动etcd服务</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable etcd</span><br><span class="line">systemctl start etcd</span><br></pre></td></tr></table></figure><p>验证</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">etcdctl \</span><br><span class="line">--ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">--cert-file=/etc/kubernetes/ssl/kubernetes.pem \</span><br><span class="line">--key-file=/etc/kubernetes/ssl/kubernetes-key.pem \</span><br><span class="line">cluster-health</span><br></pre></td></tr></table></figure><p><font color="red" size="6">结果最后⼀⾏为 cluster is healthy 时表示集群服务正常。</font></p><h1 id="kube-apiserver"><a href="#kube-apiserver" class="headerlink" title="kube-apiserver"></a>kube-apiserver</h1><h2 id="创建-kube-apiserver的服务配置⽂件"><a href="#创建-kube-apiserver的服务配置⽂件" class="headerlink" title="创建 kube-apiserver的服务配置⽂件"></a>创建 kube-apiserver的服务配置⽂件</h2><p>/usr/lib/systemd/system/kube-apiserver.service</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes API Service</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=network.target</span><br><span class="line">After=etcd.service</span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/apiserver</span><br><span class="line">ExecStart=/usr/local/bin/kube-apiserver \</span><br><span class="line"><span class="meta">$</span><span class="bash">KUBE_API_ARGS</span></span><br><span class="line">Restart=on-failure</span><br><span class="line">Type=notify</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h2 id="apiserver服务配置⽂件-etc-kubernetes-apiserver-内容"><a href="#apiserver服务配置⽂件-etc-kubernetes-apiserver-内容" class="headerlink" title="apiserver服务配置⽂件 /etc/kubernetes/apiserver 内容"></a>apiserver服务配置⽂件 /etc/kubernetes/apiserver 内容</h2><p>此处是主机名api的服务器配置，另外两台服务器修改其中的IP地址即可。<br>/etc/kubernetes/apiserver</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">KUBE_API_ARGS=&quot;--advertise-address=192.168.137.10 \</span><br><span class="line">    --allow-privileged=true \</span><br><span class="line">    --client-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">    --disable-admission-plugins=PersistentVolumeLabel \</span><br><span class="line">--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \</span><br><span class="line">--authorization-mode=RBAC,Node \</span><br><span class="line">--token-auth-file=/etc/kubernetes/token.csv \</span><br><span class="line">    --enable-bootstrap-token-auth=true \</span><br><span class="line">    --etcd-cafile=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">    --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \</span><br><span class="line">    --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \</span><br><span class="line">    --etcd-servers=https://192.168.137.10:2379,https://192.168.137.11:2379,https://192.168.137.12:2379 \</span><br><span class="line">    --insecure-port=0 \</span><br><span class="line">    --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \</span><br><span class="line">    --secure-port=6443 \</span><br><span class="line">    --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \</span><br><span class="line">    --service-cluster-ip-range=10.254.0.0/16 \</span><br><span class="line">    --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem  \</span><br><span class="line">    --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \</span><br><span class="line">    --kubelet-client-certificate=/etc/kubernetes/ssl/admin.pem \</span><br><span class="line">    --kubelet-client-key=/etc/kubernetes/ssl/admin-key.pem\</span><br><span class="line">    --proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.pem \</span><br><span class="line">    --proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client-key.pem \</span><br><span class="line">    --requestheader-allowed-names=front-proxy-client \</span><br><span class="line">    --requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem \</span><br><span class="line">    --requestheader-extra-headers-prefix=X-Remote-Extra- \</span><br><span class="line">    --requestheader-group-headers=X-Remote-Group \</span><br><span class="line">    --requestheader-username-headers=X-Remote-User \</span><br><span class="line">    --v=2 \</span><br><span class="line">    --logtostderr=true \</span><br><span class="line">    --audit-log-maxage=30  \</span><br><span class="line">     --audit-log-maxbackup=3  \</span><br><span class="line">     --audit-log-maxsize=100  \</span><br><span class="line">     --audit-log-path=/var/log/kubernetes/audit.log  \</span><br><span class="line">     --audit-policy-file=/etc/kubernetes/audit-policy.yml  \</span><br><span class="line">     --experimental-encryption-provider-config=/etc/kubernetes/encryption.yaml  \</span><br><span class="line">     --event-ttl=1h&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>  –authorization-mode=Node,RBAC： 开启 Node 和 RBAC授权模式，拒绝未授权的请求；</li><li>  –enable-admission-plugins：启用 ServiceAccount 和 NodeRestriction；</li><li>  –service-account-key-file：签名 ServiceAccount Token的公钥文件，kube-controller-manager 的 –service-account-private-key-file指定私钥文件，两者配对使用；</li><li>  –tls-*-file：指定 apiserver 使用的证书、私钥和 CA 文件。–client-ca-file用于验证 client (kue-controller-manager、kube-scheduler、kubelet、kube-proxy等)请求所带的证书；</li><li>  –kubelet-client-certificate、–kubelet-client-key：如果指定，则使用 https访问 kubelet APIs；需要为 kubernete 用户定义 RBAC 规则，否则无权访问 kubeletAPI；</li><li>  –service-cluster-ip-range： 指定 Service Cluster IP 地址段；</li><li>  –service-node-port-range： 指定 NodePort 的端口范围；</li><li>  –runtime-config=api/all=true： 启用所有版本的 APIs，如autoscaling/v2alpha1；</li><li>  –enable-bootstrap-token-auth：启用 kubelet bootstrap 的 token 认证；</li><li>  –kubelet-client-certificate=/etc/kubernetes/ssl/admin.pem这里我为什么不重新生成kubelet的证书呢，因为后面安装kubelet的时候使用的kubeconfig就是将admin用户生成给kubectl使用的kubeconfig（即.kube/config文件）复制给他使用了，所以这里直接使用admin证书，当然你也可以去生成kubelet证书。</li><li>  –kubelet-client-key=/etc/kubernetes/ssl/admin-key.pem，同上述</li><li>–proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.pem<br>  –proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client-key.pem<br>  –requestheader-allowed-names=front-proxy-client<br>  –requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem<br>  kube-proxy客户端使用证书。 如果不指定这些文件，将会无法exec进去pod和无法logs –f 查看pod 日志</li><li>–requestheader-allowed-names=front-proxy-client<br>  –requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem<br>  –requestheader-extra-headers-prefix=X-Remote-Extra-<br>  –requestheader-group-headers=X-Remote-Group<br>  –requestheader-username-headers=X-Remote-User<br>  用于支持metrics-server，后续将会讲解。</li><li>  –audit-policy-file=/etc/kubernetes/audit-policy.yml ,文件内容如下<br>参考地址：<a href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/etc/kubernetes/audit-policy.yml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/etc/kubernetes/audit-policy.yml</a></li><li>–experimental-encryption-provider-config：启用加密特性；文件生成如下<br>参考地址：<a href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/etc/kubernetes/encryption.yaml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/etc/kubernetes/encryption.yaml</a><br>  生成 EncryptionConfig 所需的加密 key<br>文件下载地址：<a href="https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/etc/kubernetes/encryption-source.yaml">https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/etc/kubernetes/encryption-source.yaml</a><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">下载后将其更名为encryption.yaml</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mv encryption-source.yaml encryption.yaml</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ENCRYPT_SECRET=$( head -c 32 /dev/urandom | base64 )</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sed -ri <span class="string">&quot;/secret:/s#(: ).+#\1<span class="variable">$&#123;ENCRYPT_SECRET&#125;</span>#&quot;</span> encryption.yaml</span></span><br><span class="line"> </span><br></pre></td></tr></table></figure></li><li>  node1的apiserver服务配置文件只需要修改此处–advertise-address=192.168.137.11即可；</li><li>  node2的apiserver服务配置文件只需要修改此处–advertise-address=192.168.137.12即可；</li></ul><h2 id="启动kube-apiserver"><a href="#启动kube-apiserver" class="headerlink" title="启动kube-apiserver"></a>启动kube-apiserver</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-apiserver</span><br><span class="line">systemctl start kube-apiserver</span><br><span class="line">systemctl status kube-apiserver</span><br></pre></td></tr></table></figure><h1 id="kube-controller-manager"><a href="#kube-controller-manager" class="headerlink" title="kube-controller-manager"></a>kube-controller-manager</h1><p><font color="red" size="3">三台服务器都要执行</font></p><h2 id="创建-kube-controller-manager服务配置⽂件"><a href="#创建-kube-controller-manager服务配置⽂件" class="headerlink" title="创建 kube-controller-manager服务配置⽂件"></a>创建 kube-controller-manager服务配置⽂件</h2><p>/usr/lib/systemd/system/kube-controller-manager.service</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line"></span><br><span class="line">Description=Kubernetes Controller Manager</span><br><span class="line"></span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line"></span><br><span class="line">EnvironmentFile=-/etc/kubernetes/controller-manager</span><br><span class="line"></span><br><span class="line">ExecStart=/usr/local/bin/kube-controller-manager </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash">KUBE_CONTROLLER_MANAGER_ARGS</span></span><br><span class="line"></span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line"></span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h2 id="controller-manager服务配置⽂件-etc-kubernetes-controller-manager"><a href="#controller-manager服务配置⽂件-etc-kubernetes-controller-manager" class="headerlink" title="controller-manager服务配置⽂件 /etc/kubernetes/controller-manager "></a>controller-manager服务配置⽂件 /etc/kubernetes/controller-manager </h2><p>/etc/kubernetes/controller-manager</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">KUBE_CONTROLLER_MANAGER_ARGS=&quot;--address=0.0.0.0 \</span><br><span class="line">    --allocate-node-cidrs=true \</span><br><span class="line">    --cluster-cidr=10.10.0.0/16 \</span><br><span class="line">    --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">    --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  \</span><br><span class="line">    --controllers=*,bootstrapsigner,tokencleaner \</span><br><span class="line">    --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span><br><span class="line">    --leader-elect=true \</span><br><span class="line">    --node-cidr-mask-size=24 \</span><br><span class="line">    --root-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">    --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \</span><br><span class="line">--use-service-account-credentials=true &quot;</span><br></pre></td></tr></table></figure><ul><li>  –address=0.0.0.0此处为了方便后续的prometheus-operator访问服务端口设置为所有网卡可以访问，之前设置的是127.0.0.1</li><li>  –allocate-node-cidrs=true，您的Kubernetes控制器管理器配置为分配pod CIDR（即通过传递–allocate-node-cidrs=true给控制器管理器）</li><li>  –cluster-cidr=10.10.0.0/16，您的Kubernetes控制器管理器已经提供了一个cluster-cidr（即通过传递–cluster-cidr=10.10.0.0/16，默认情况下清单需要）。</li></ul><h2 id="启动-kube-controller-manager"><a href="#启动-kube-controller-manager" class="headerlink" title="启动 kube-controller-manager"></a>启动 kube-controller-manager</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-controller-manager</span><br><span class="line">systemctl start kube-controller-manager</span><br><span class="line">systemctl status kube-controller-manager</span><br></pre></td></tr></table></figure><h1 id="kube-scheduler"><a href="#kube-scheduler" class="headerlink" title="kube-scheduler"></a>kube-scheduler</h1><p><font color="red" size="3">三台服务器都要执行</font></p><h2 id="创建-kube-scheduler服务配置⽂件"><a href="#创建-kube-scheduler服务配置⽂件" class="headerlink" title="创建 kube-scheduler服务配置⽂件"></a>创建 kube-scheduler服务配置⽂件</h2><p>/usr/lib/systemd/system/kube-scheduler.service</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line"></span><br><span class="line">Description=Kubernetes Scheduler Plugin</span><br><span class="line"></span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line"></span><br><span class="line">EnvironmentFile=-/etc/kubernetes/scheduler</span><br><span class="line"></span><br><span class="line">ExecStart=/usr/local/bin/kube-scheduler </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash">KUBE_SCHEDULER_ARGS</span></span><br><span class="line"></span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line"></span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line"></span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h2 id="scheduler服务配置⽂件-etc-kubernetes-scheduler"><a href="#scheduler服务配置⽂件-etc-kubernetes-scheduler" class="headerlink" title="scheduler服务配置⽂件 /etc/kubernetes/scheduler "></a>scheduler服务配置⽂件 /etc/kubernetes/scheduler </h2><p>/etc/kubernetes/scheduler</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">KUBE_SCHEDULER_ARGS=&quot;--address=0.0.0.0 \</span><br><span class="line">    --leader-elect=true \</span><br><span class="line">--kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>  –address=0.0.0.0此处为了方便后续的prometheus-operator访问服务端口设置为所有网卡可以访问，之前设置的是127.0.0.1</li></ul><h2 id="启动-kube-scheduler"><a href="#启动-kube-scheduler" class="headerlink" title="启动 kube-scheduler"></a>启动 kube-scheduler</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-scheduler</span><br><span class="line">systemctl start kube-scheduler</span><br><span class="line">systemctl status kube-scheduler</span><br></pre></td></tr></table></figure><h1 id="至此Master三大组件安装完成"><a href="#至此Master三大组件安装完成" class="headerlink" title="至此Master三大组件安装完成"></a>至此Master三大组件安装完成</h1><p>查看运行状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for i in kube-apiserver kube-controller-manager kube-scheduler; do systemctl restart $i ; done</span><br><span class="line">for i in kube-apiserver kube-controller-manager kube-scheduler; do systemctl status $i -l ; done</span><br></pre></td></tr></table></figure><p>验证集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl cluster-info</span></span><br><span class="line">Kubernetes master is running at https://192.168.137.13:9443</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get cs</span></span><br><span class="line">NAME STATUS MESSAGE ERROR</span><br><span class="line">scheduler Healthy ok</span><br><span class="line">controller-manager Healthy ok</span><br><span class="line">etcd-0 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br><span class="line">etcd-1 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br><span class="line">etcd-2 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br></pre></td></tr></table></figure><h1 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h1><p>Docker从1.13版本开始调整了默认的防火墙规则，禁用了iptables filter表中FOWARD链，这样会引起Kubernetes集群中跨Node的Pod无法通信，因此docker安装完成后，还需要手动修改iptables规则。</p><p><font color="red" size="3">三台服务器都要执行</font></p><h2 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line">yum makecache fast</span><br><span class="line">yum install -y docker-ce</span><br></pre></td></tr></table></figure><h2 id="编辑systemctl的Docker启动文件"><a href="#编辑systemctl的Docker启动文件" class="headerlink" title="编辑systemctl的Docker启动文件"></a>编辑systemctl的Docker启动文件</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &quot;13i ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT&quot; /usr/lib/systemd/system/docker.service</span><br></pre></td></tr></table></figure><h2 id="修改docker参数"><a href="#修改docker参数" class="headerlink" title="修改docker参数"></a>修改docker参数</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/docker/</span><br><span class="line"><span class="meta">cat&gt;</span><span class="bash">/etc/docker/daemon.json&lt;&lt;<span class="string">EOF</span></span></span><br><span class="line">&#123;</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https://fz5yth0r.mirror.aliyuncs.com&quot;],</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;storage-opts&quot;: [</span><br><span class="line">    &quot;overlay2.override_kernel_check=true&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;100m&quot;,</span><br><span class="line">    &quot;max-file&quot;: &quot;3&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="启动docker"><a href="#启动docker" class="headerlink" title="启动docker"></a>启动docker</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable docker</span><br><span class="line">systemctl start docker</span><br></pre></td></tr></table></figure><h1 id="建立不加密的docker私有仓库方式"><a href="#建立不加密的docker私有仓库方式" class="headerlink" title="建立不加密的docker私有仓库方式"></a>建立不加密的docker私有仓库方式</h1><h2 id="建立最简单的私库"><a href="#建立最简单的私库" class="headerlink" title="建立最简单的私库"></a>建立最简单的私库</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --restart=always --name dockerregistry \</span><br><span class="line"> -p 5000:5000 \</span><br><span class="line"> -v /huisebug/dockerimagestorehouse/registry:/var/lib/registry \</span><br><span class="line"> -v /huisebug/dockerimagestorehouse/config.yml:/etc/docker/registry/config.yml  registry</span><br></pre></td></tr></table></figure><p>config.yml内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">version: 0.1</span><br><span class="line">log:</span><br><span class="line">  fields:</span><br><span class="line">    service: registry</span><br><span class="line">storage:</span><br><span class="line">  delete:</span><br><span class="line">    enabled: true</span><br><span class="line">  cache:</span><br><span class="line">    blobdescriptor: inmemory</span><br><span class="line">  filesystem:</span><br><span class="line">    rootdirectory: /var/lib/registry</span><br><span class="line">http:</span><br><span class="line">  addr: :5000</span><br><span class="line">  headers:</span><br><span class="line">    X-Content-Type-Options: [nosniff]</span><br><span class="line">health:</span><br><span class="line">  storagedriver:</span><br><span class="line">    enabled: true</span><br><span class="line">    interval: 10s</span><br><span class="line">    threshold: 3</span><br></pre></td></tr></table></figure><p>更改docker的service文件增加–insecure-registry 192.168.137.10:5000<br>ExecStart=/usr/bin/dockerd –insecure-registry 192.168.137.10:5000</p><p>重启docker</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure><h1 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h1><p><font color="red" size="3">三台服务器都要执行</font></p><h2 id="创建-kubelet服务配置⽂件"><a href="#创建-kubelet服务配置⽂件" class="headerlink" title="创建 kubelet服务配置⽂件"></a>创建 kubelet服务配置⽂件</h2><p> /usr/lib/systemd/system/kubelet.service</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line"></span><br><span class="line">Description=Kubernetes Kubelet Server</span><br><span class="line"></span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">After=docker.service</span><br><span class="line"></span><br><span class="line">Requires=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line"></span><br><span class="line">WorkingDirectory=/var/lib/kubelet</span><br><span class="line"></span><br><span class="line">EnvironmentFile=-/etc/kubernetes/kubelet</span><br><span class="line"></span><br><span class="line">ExecStart=/usr/local/bin/kubelet \</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash">KUBELET_ARGS</span></span><br><span class="line"></span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line"></span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>创建工作目录，需要手动创建</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /var/lib/kubelet</span><br></pre></td></tr></table></figure><h2 id="kubelet服务配置文件-etc-kubernetes-kubelet"><a href="#kubelet服务配置文件-etc-kubernetes-kubelet" class="headerlink" title="kubelet服务配置文件/etc/kubernetes/kubelet"></a>kubelet服务配置文件/etc/kubernetes/kubelet</h2><p>/etc/kubernetes/kubelet</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">KUBELET_ARGS=&quot;--bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><br><span class="line">--kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><br><span class="line">--config=/var/lib/kubelet/config.yaml \</span><br><span class="line">--cni-bin-dir=/opt/cni/bin \</span><br><span class="line">--cni-conf-dir=/etc/cni/net.d \</span><br><span class="line">--network-plugin=cni&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>  bootstrap.kubeconfig之前我们已经建立了。kubelet 使⽤该⽂件中的⽤户名和 token向 kube-apiserver 发送 TLS Bootstrapping 请求；</li><li>  –kubeconfig=/etc/kubernetes/kubelet.kubeconfig 中指定的 kubelet.kubeconfig⽂件在第⼀次启动kubelet之前并不存在，请看下⽂，当通过CSR请求后会⾃动⽣成kubelet.kubeconfig ⽂件，如果你的节点上已经⽣成了 ~/.kube/config⽂件，你可以将该⽂件拷⻉到该路径下，并重命名为 kubelet.kubeconfig，所有node节点可以共⽤同⼀个kubelet.kubeconfig⽂件，这样新添加的节点就不需要再创建CSR请求就能⾃动添加到kubernetes集群中。同样，在任意能够访问到kubernetes集群的主机上使⽤kubectl –kubeconfig 命令操作集群时，只要使⽤ ~/.kube/config⽂件就可以通过权限认证，因为这⾥⾯已经有认证信息并认为你是admin⽤户，对集群拥有所有权限。<br>执行以下命令<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp -rf /root/.kube/config /etc/kubernetes/kubelet.kubeconfig</span><br></pre></td></tr></table></figure></li></ul><p><font color="red" size="3">并且分发到所有安装kubelet的服务器</font></p><ul><li>  –config=/var/lib/kubelet/config.yaml中声明了kubelet服务的端口信息等，参考地址：<a href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/var/lib/kubelet/config.yaml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/var/lib/kubelet/config.yaml</a></li><li>–cni-bin-dir=/opt/cni/bin<br> –cni-conf-dir=/etc/cni/net.d<br> –network-plugin=cni，你有一个Kubernetes集群配置为使用CNI网络插件（即通过传递–network-plugin=cni给kubelet）</li></ul><p>建立文件目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /etc/kubernetes/manifests</span><br><span class="line">echo &quot; mkdir /etc/kubernetes/manifests&quot; &gt;&gt; /etc/rc.local</span><br></pre></td></tr></table></figure><p><font color="red" size="6">!!!注意</font></p><ul><li>  hostPort不适用于CNI</li><li>  使用<em>hostPort</em>和CNI插件的组合将导致Kubernetes静默忽略<em>hostPort</em>属性。</li></ul><h2 id="启动kubelet"><a href="#启动kubelet" class="headerlink" title="启动kubelet"></a>启动kubelet</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kubelet</span><br><span class="line">systemctl start kubelet</span><br><span class="line">systemctl status kubelet</span><br></pre></td></tr></table></figure><h2 id="检查node状态"><a href="#检查node状态" class="headerlink" title="检查node状态"></a>检查node状态</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node</span></span><br><span class="line">NAME STATUS ROLES AGE VERSION</span><br><span class="line">api.huisebug.com NotReady &lt;none&gt; 6m8s v1.12.4</span><br><span class="line">node1.huisebug.com NotReady &lt;none&gt; 4s v1.12.4</span><br><span class="line">node2.huisebug.com NotReady &lt;none&gt; 1s v1.12.4</span><br></pre></td></tr></table></figure><p>可以看到状态为NotReady 因为缺少cni配置文件而导致kubelet服务（systemctl status kubelet -l）在日志中提示找不到配置而无法变为Ready</p><p>我们可以将/etc/cni/net.d/下的配置文件暂时建立10-calico.conflist，使状态变为Ready。后续再做修改或者删除。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p /etc/cni/net.d/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vim /etc/cni/net.d/10-calico.conflist</span></span><br></pre></td></tr></table></figure><p>参考下载地址：<br><a href="https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/etc/cni/net.d/10-calico.conflist">https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/etc/cni/net.d/10-calico.conflist</a></p><p>再次查看状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node</span></span><br><span class="line">NAME STATUS ROLES AGE VERSION</span><br><span class="line">api.huisebug.com Ready &lt;none&gt; 34m v1.12.4</span><br><span class="line">node1.huisebug.com Ready &lt;none&gt; 28m v1.12.4</span><br><span class="line">node2.huisebug.com Ready &lt;none&gt; 28m v1.12.4</span><br></pre></td></tr></table></figure><h1 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h1><p>这里我们采用daemonsets方式建立</p><p>这里的代理模式使用的是ipvs，不再使用iptables，并且ipvs依赖于nf_conntrack_ipv4，所以需要将服务器的内核目录挂载到pod中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 建立serviceaccount</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl -n kube-system create serviceaccount kube-proxy</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 建立集群角色绑定</span></span><br><span class="line">kubectl create clusterrolebinding system:kube-proxy \</span><br><span class="line">--clusterrole system:node-proxier \</span><br><span class="line">--serviceaccount kube-system:kube-proxy</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 建立daemonsets方式建立的pod和需要的参数configmap，参考地址:</span></span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/kube-proxy/kube-proxy.yaml</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 建立集群的第一个pod，会需要基础pod镜像，每台服务器执行一遍。拉取基础镜像。</span></span><br><span class="line">docker pull huisebug/sec_re:pause3.1 &amp;&amp; docker tag huisebug/sec_re:pause3.1 k8s.gcr.io/pause:3.1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看状态</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod --all-namespaces -o wide</span></span><br><span class="line">![](media/3bf05a168dbd6d583e0c0d5dbb65b226.png)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">通过ipvsadm查看 proxy 规则</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ipvsadm -ln</span></span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> RemoteAddress:Port Forward Weight ActiveConn InActConn</span></span><br><span class="line">TCP 10.254.0.1:443 rr</span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> 192.168.137.10:6443 Masq 1 0 0</span></span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> 192.168.137.11:6443 Masq 1 0 0</span></span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> 192.168.137.12:6443 Masq 1 0 0</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 确认使用ipvs模式</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> curl localhost:10249/proxyMode</span></span><br><span class="line">ipvs</span><br></pre></td></tr></table></figure><h1 id="至此Node三大组件安装完成"><a href="#至此Node三大组件安装完成" class="headerlink" title="至此Node三大组件安装完成"></a>至此Node三大组件安装完成</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">查看状态</span><br><span class="line">for i in etcd kube-apiserver kube-controller-manager kube-scheduler kubelet docker; do systemctl status $i -l; done</span><br><span class="line"></span><br><span class="line">重启所有二进制安装的</span><br><span class="line">for i in etcd kube-apiserver kube-controller-manager kube-scheduler kubelet docker; do systemctl restart $i; done</span><br></pre></td></tr></table></figure><h1 id="Calico"><a href="#Calico" class="headerlink" title="Calico"></a>Calico</h1><h2 id="环境要求，原打算是使用3-4-0版本，奈何失败了。"><a href="#环境要求，原打算是使用3-4-0版本，奈何失败了。" class="headerlink" title="环境要求，原打算是使用3.4.0版本，奈何失败了。"></a>环境要求，原打算是使用3.4.0版本，奈何失败了。</h2><ul><li>  您的Kubernetes控制器管理器配置为分配pod CIDR（即通过传递–allocate-node-cidrs=true给控制器管理器）</li><li>  您的Kubernetes控制器管理器已经提供了一个cluster-cidr（即通过传递–cluster-cidr=10.10.0.0/16，默认情况下清单需要）。</li><li>  你有一个Kubernetes集群配置为使用CNI网络插件（即通过传递–network-plugin=cni给kubelet）<blockquote><p>  我犯了一个错误就是将cluster-cidr和cluster-range-ip配置成相同了，修改后如果出现pod无法建立，<br>  那么就将etcd的数据目录清空，然后重新启动所有服务，让apiserver重新请求etcd并写入数据。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /var/lib/etcd/*</span><br><span class="line">for i in etcd kube-apiserver kube-controller-manager kube-scheduler kubelet docker; do systemctl restart $i ; done</span><br></pre></td></tr></table></figure>参考我的yaml文件,calico3.1.3<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export interface=ens33</span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Calico/calico3.1.3node-ctl.yaml</span><br></pre></td></tr></table></figure></li><li>  记得上面要将cid地址修改为你的cidr地址</li><li>  记得查看interface名称，系统不同网卡名称也不同</li></ul><p>执行后检测pod建立情况<br><img src="/2020/08/26/k8s/media/6ffc2f19054e181ed137799a29bd742e.png"><br>检测网卡情况<br><img src="/2020/08/26/k8s/media/e94fcb31b73adab699fc0580db9a037b.png"><br>会发现缺少cali*开头的网卡名称，那是我们现在还没建立过不使用hostNetwork: true的pod，我们这里使用建立一个普通的nginx pod</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f https://kubernetes.io/docs/user-guide/nginx-deployment.yaml --record</span><br></pre></td></tr></table></figure><p>再次查看网卡信息<br><img src="/2020/08/26/k8s/media/27573e66d33decbf6a61f9f972f9ce11.png"></p><h2 id="验证不同节点的容器之间能否ping通"><a href="#验证不同节点的容器之间能否ping通" class="headerlink" title="验证不同节点的容器之间能否ping通"></a>验证不同节点的容器之间能否ping通</h2><p><img src="/2020/08/26/k8s/media/c45fba5757f980bff79e58f73ff0456c.png"></p><h2 id="验证能否访问外网"><a href="#验证能否访问外网" class="headerlink" title="验证能否访问外网"></a>验证能否访问外网</h2><p>建立一个测试工具pod，busybox需要一个持续输出，这里我将网关地址赋予它</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl run busybox --image=busybox --command -- ping 192.168.137.1</span><br><span class="line">kubectl exec -it busybox-759d8dbd98-tf9hd ping 61.139.2.69</span><br><span class="line">kubectl exec -it busybox-759d8dbd98-tf9hd nslookup baidu.com 61.139.2.69</span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/dc12719300e4ac7400b7b62617a2ce72.png"></p><p>至此calico安装完成。</p><h2 id="问题汇总"><a href="#问题汇总" class="headerlink" title="问题汇总"></a>问题汇总</h2><p>如果kubectl get node一直是notready，</p><p>执行下面步骤</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Calico/calico3.1.3node-ctl.yaml</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">三台服务器都执行</span></span><br><span class="line">rm -rf /etc/cni/net.d/*</span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Calico/calico3.1.3node-ctl.yaml</span><br></pre></td></tr></table></figure><h1 id="Coredns"><a href="#Coredns" class="headerlink" title="Coredns"></a>Coredns</h1><p>参考地址,记得将其中的集群ip地址修改为你在kubelet中定义的地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Coredns/coredns.yaml</span><br></pre></td></tr></table></figure><h2 id="验证效果"><a href="#验证效果" class="headerlink" title="验证效果"></a>验证效果</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">查看是否成功建立coredns pod</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod --all-namespaces -o wide 查看是否成功建立coredns pod</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc</span></span><br><span class="line">NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</span><br><span class="line">kubernetes ClusterIP 10.254.0.1 &lt;none&gt; 443/TCP 16m</span><br><span class="line"></span><br><span class="line">可以看到default命名空间下现在就一个service</span><br><span class="line">将之前建立的nginx-deployment暴露为service</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl expose deploy nginx-deployment</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc</span></span><br><span class="line">NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</span><br><span class="line">kubernetes ClusterIP 10.254.0.1 &lt;none&gt; 443/TCP 16m</span><br><span class="line">nginx-deployment ClusterIP 10.254.136.63 &lt;none&gt; 80/TCP 3s</span><br></pre></td></tr></table></figure><p>测试能否解析成对应的IP地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -it nginx-deployment-5c689d88bb-2mzx4 ping nginx-deployment</span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/82e7d16fd70b1d760463183eeafd14c6.png"></p><p>测试不同命名空间</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc --all-namespaces</span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/0080be720975ed9265171ad7919a5b22.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -it nginx-deployment-5c689d88bb-2mzx4 ping calico-typha.kube-system</span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/172614abc7d8086efdaa1aa3e170edbb.png"></p><p>至此，coredns安装完成。</p><h1 id="再次验证整个集群的proxy负载机制效果"><a href="#再次验证整个集群的proxy负载机制效果" class="headerlink" title="再次验证整个集群的proxy负载机制效果"></a>再次验证整个集群的proxy负载机制效果</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -ln</span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/23eea17b92a059fc23388c2672365cb2.png"></p><h1 id="Helm"><a href="#Helm" class="headerlink" title="Helm"></a>Helm</h1><p>相当于centos的yum，Ubuntu的apt-get命令</p><p>此处安装的helm是2.12.2版本，之前的2.6版本在安装后续的chart会出现：</p><p>Error: parse error in *** function “genCA” not defined 错误</p><h2 id="Helm-client安装"><a href="#Helm-client安装" class="headerlink" title="Helm client安装"></a>Helm client安装</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 下载</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> wget https://storage.googleapis.com/kubernetes-helm/helm-v2.12.2-linux-amd64.tar.gz</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 解包并将二进制文件helm拷贝到/usr/<span class="built_in">local</span>/bin目录下</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar -zxvf helm-v2.12.2-linux-amd64.tar.gz &amp;&amp; mv linux-amd64/helm /usr/<span class="built_in">local</span>/bin/helm</span></span><br></pre></td></tr></table></figure><h2 id="安装socat"><a href="#安装socat" class="headerlink" title="安装socat"></a>安装socat</h2><p><font color="red" size="3">用于端口转发，在准备初始环境安装keepalived已经安装，必须在所有Node服务器安装</font></p><h2 id="Helm-server安装"><a href="#Helm-server安装" class="headerlink" title="Helm server安装"></a>Helm server安装</h2><p>创建tiller的 serviceaccount 和 clusterrolebinding</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl create serviceaccount --namespace kube-system tiller</span><br><span class="line">kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller</span><br></pre></td></tr></table></figure><p>然后安装helm服务端tiller<br>这里因为一些原因，使用的是阿里云的镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.12.2 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</span><br></pre></td></tr></table></figure><p>修改serviceAccount ：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl patch deploy -n kube-system tiller-deploy -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;serviceAccount&quot;:&quot;tiller&quot;&#125;&#125;&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure><p>检查是否安装成功，等待一段时间后。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> helm version</span></span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/f858112ba11661d4a205598f9c0a426f.png"></p><h1 id="Traefik"><a href="#Traefik" class="headerlink" title="Traefik"></a>Traefik</h1><p>k8s集群中的http反向代理服务</p><p>直接下载官方的charts，然后找到traefik就可以执行安装了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> &lt;https://github.com/helm/charts.git&gt;</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> charts/stable/traefik</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 执行默认配置，显然是不符合我这里的需求。</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> helm install .</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 列出helm安装的资源类型</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> helm ls</span></span><br><span class="line">NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE</span><br><span class="line">gilded-skunk 1 Fri Jan 18 10:36:42 2019 DEPLOYED traefik-1.59.0 1.7.6 default</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">删除helm安装的资源类型</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> helm delete gilded-skunk --purge</span></span><br><span class="line">release &quot;gilded-skunk&quot; deleted</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装我想要的一些功能</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认的template目录下的deployment.yaml文件没有开启hostNetwork: <span class="literal">true</span>，执行下面语句开启（主要是我这儿环境配置了hostPort不生效，（因为CNI插件的原因）配置hostNetwork就生效）</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /root/charts/stable/traefik/templates</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> Lnum=$(sed -n <span class="string">&#x27;/spec/=&#x27;</span> deployment.yaml |sed -n <span class="string">&quot;2&quot;</span>p)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下面的格式为sed -ie 单引单引双引变量名双引单引a六个空格hostNetwork: <span class="literal">true</span>单引 deployment.yaml</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sed -ie &amp;<span class="comment">#39;&amp;#39;&amp;#34;$Lnum&amp;#34;&amp;#39;a&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;hostNetwork: true&amp;#39;  deployment.yaml</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">中间是6个空格，不能少也不能多</span></span><br></pre></td></tr></table></figure><p>也可以参考我已经修改好的charts，参考地址：<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/Traefik">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/Traefik</a><br>参考地址仅做参考，因为官方charts是不断更新的，或许后续会不断添加新功能，例如后续可能会添加支持hostNetwork: true功能，推荐还是根据官方charts来设置。</p><h2 id="安装traefik"><a href="#安装traefik" class="headerlink" title="安装traefik"></a>安装traefik</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> helm delete traefik --purge</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> helm install --name traefik --namespace kube-system \</span></span><br><span class="line"><span class="bash">--<span class="built_in">set</span> replicas=3,cpuLimit=1000m,memoryLimit=1Gi,rbac.enabled=<span class="literal">true</span>,\</span></span><br><span class="line"><span class="bash">dashboard.enabled=<span class="literal">true</span>,dashboard.domain=traefik.huisebug.com,\</span></span><br><span class="line"><span class="bash">metrics.prometheus.enabled=<span class="literal">true</span> \</span></span><br><span class="line"><span class="bash">/root/charts/stable/traefik</span></span><br></pre></td></tr></table></figure><p>确保服务已经运行<br><img src="/2020/08/26/k8s/media/9e90faf47ed6c8b79f1a213d8f1865ad.png"><br><img src="/2020/08/26/k8s/media/12dffb5c85ff5fea46833c01e4a7a070.png"></p><p>重启服务器后如果traefik没有正确启动，那就删除后重新建立。</p><h2 id="验证效果-1"><a href="#验证效果-1" class="headerlink" title="验证效果"></a>验证效果</h2><p>之前我们验证coredns时候将一个nginx暴露为service，现在建立一个ingress来对应这个service</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc --all-namespaces</span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/76bd44213cfe4012bf900395b9497aa7.png"></p><p>参考yaml地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Traefik/nginx-ingress.yaml</span><br></pre></td></tr></table></figure><p>在本地解析文件中追加域名解析<br>192.168.137.10 traefik.huisebug.com nginx-deployment.huisebug.com</p><p>访问验证效果<br><img src="/2020/08/26/k8s/media/1089f58ff64fe2e1987d9d72af840686.png"><br><img src="/2020/08/26/k8s/media/bacc338b37613e8ee8c525253c1c6a42.png"></p><p>至此，traefik安装完成</p><h1 id="Dashboard"><a href="#Dashboard" class="headerlink" title="Dashboard"></a>Dashboard</h1><p>会额外建立一个名称为anonymous-dashboard-proxy的 Cluster Role(Binding)来让system:anonymous这个匿名使用者能够通过 API Server 来 proxy 到 KubernetesDashboard,而这个 RBAC规则仅能够存取services/proxy资源,以及https:kubernetes-dashboard:资源名称同时在1.7 版本以后的 Dashboard 将不再提供所有权限,因此需要建立一个 service account来绑定 cluster-admin role</p><p>具体yaml参考</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Dashboard/dashboard.yaml</span><br></pre></td></tr></table></figure><h2 id="访问验证"><a href="#访问验证" class="headerlink" title="访问验证"></a>访问验证</h2><ul><li>  方式一：可以通过浏览器读取Dashboard</li><li>  方式二：ingress既然上述我们已经搭建了traefik，所以我们建立一个ingress来访问。</li></ul><h3 id="直接https访问方式"><a href="#直接https访问方式" class="headerlink" title="直接https访问方式"></a>直接https访问方式</h3><h4 id="导⼊证书"><a href="#导⼊证书" class="headerlink" title="导⼊证书"></a>导⼊证书</h4><p>将⽣成的admin.pem证书转换格式（/etc/kubernetes/ssl目录下）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/kubernetes/ssl</span><br><span class="line">openssl pkcs12 -export -in admin.pem -out admin.p12 -inkey admin-key.pem</span><br></pre></td></tr></table></figure><p>将⽣成的 admin.p12<br>证书导⼊的你的电脑的浏览器，导出的时候记住你设置的密码，导⼊的时候还要⽤到。</p><h4 id="访问"><a href="#访问" class="headerlink" title="访问"></a>访问</h4><p>这里不能写成VIP+代理端口来跳转，我们三台服务器都安装了api-server，可以任意如下一个访问</p><p><a href="https://192.168.137.10:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/">https://192.168.137.10:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</a></p><p><a href="https://192.168.137.11:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/">https://192.168.137.11:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</a></p><p><a href="https://192.168.137.12:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/">https://192.168.137.12:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</a></p><p>成功向浏览器导入证书后访问成功效果</p><p><img src="/2020/08/26/k8s/media/778d54e157730e8122dfc3f5ae727361.png"></p><p>使用令牌（token）进行访问</p><p>令牌获取。</p><p>$ kubectl -n kube-system describe secrets | sed -rn &#34;/sdashboard-token-/,/^token/{/^token/s#S+s+##p}&#34;</p><p><img src="/2020/08/26/k8s/media/49791fb732caf260cf373180c5f08891.png"></p><p>将token值复制到令牌一栏即可成功访问dashboard</p><p><img src="/2020/08/26/k8s/media/de528414d705dc0816e623fb1c5725d4.png"></p><p><img src="/2020/08/26/k8s/media/1eeb76fe6060a353e0fd0548d47b94e4.png"></p><h3 id="ingress方式"><a href="#ingress方式" class="headerlink" title="ingress方式"></a>ingress方式</h3><p>参考yaml文件地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Dashboard/kubernetes-dashboard-ingress.yaml</span><br></pre></td></tr></table></figure><p>这里我们需要开启traefik的https功能，并且开启ssl.insecureSkipVerify=true跳过验证SSL连接上的证书，如果不开启此处，就无法使用ingress进行访问。此处是相当于使用了https连接，没有进行证书验证。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">helm delete traefik --purge</span><br><span class="line">helm install --name traefik --namespace kube-system \</span><br><span class="line">--set imageTag=1.6.5,replicas=3,\</span><br><span class="line">cpuLimit=1000m,memoryLimit=1Gi,rbac.enabled=true,\</span><br><span class="line">dashboard.enabled=true,dashboard.domain=traefik.huisebug.com,\</span><br><span class="line">metrics.prometheus.enabled=true,\</span><br><span class="line">ssl.enabled=true,ssl.insecureSkipVerify=true \</span><br><span class="line">/root/charts/stable/traefik</span><br></pre></td></tr></table></figure><p>如果出现traefik出现问题就降低配置中的traefik镜像版本。</p><h4 id="验证访问效果"><a href="#验证访问效果" class="headerlink" title="验证访问效果"></a>验证访问效果</h4><p><img src="/2020/08/26/k8s/media/1a75a341166a1a3c3fc59aff8d3b53c1.png"></p><h1 id="Scope监控"><a href="#Scope监控" class="headerlink" title="Scope监控"></a>Scope监控</h1><p>用于监控整个k8s集群的网络TOP</p><h2 id="安装scope"><a href="#安装scope" class="headerlink" title="安装scope"></a>安装scope</h2><p>直接使用官方yaml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f &quot;https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d &#x27;\n&#x27;)&quot;</span><br></pre></td></tr></table></figure><p>注意上面的namespace是weave</p><h2 id="暴露访问两种方式"><a href="#暴露访问两种方式" class="headerlink" title="暴露访问两种方式"></a>暴露访问两种方式</h2><h3 id="Service的nodeport方式"><a href="#Service的nodeport方式" class="headerlink" title="Service的nodeport方式"></a>Service的nodeport方式</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">wget https://cloud.weave.works/k8s/scope.yaml</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改service的值</span></span><br><span class="line">    spec:</span><br><span class="line">      type: NodePort</span><br><span class="line">      ports:</span><br><span class="line">        - name: app</span><br><span class="line">          port: 80</span><br><span class="line">          protocol: TCP</span><br><span class="line">          targetPort: 4040</span><br><span class="line">          nodePort: 30040</span><br></pre></td></tr></table></figure><p>注意：Nodeport只能暴露30000到32767，不然会报错如下：</p><p>The Service “weave-scope-app” is invalid: spec.ports[0].nodePort: Invalid value:<br>4040: provided port is not in the valid range. The range of valid ports is<br>30000-32767</p><h3 id="Traefik代理"><a href="#Traefik代理" class="headerlink" title="Traefik代理"></a>Traefik代理</h3><p>编写一个ingress</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">参考yaml地址</span><br><span class="line">kubectl create -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Scope/scope-ingress.yaml</span><br></pre></td></tr></table></figure><h2 id="验证效果-2"><a href="#验证效果-2" class="headerlink" title="验证效果"></a>验证效果</h2><p><img src="/2020/08/26/k8s/media/3be2bb4c074a23be29d02276b0ce7390.png"></p><h1 id="EFK"><a href="#EFK" class="headerlink" title="EFK"></a>EFK</h1><p>此栏目已经迁移到<a href="https://huisebug.github.io/2019/05/08/gluster-heketi-efk/">gluster-heketi-efk</a></p><h1 id="Prometheus-operator"><a href="#Prometheus-operator" class="headerlink" title="Prometheus-operator "></a>Prometheus-operator </h1><p>此栏目已经迁移到<a href="https://huisebug.github.io/2019/08/27/Prometheus-Operator/">Prometheus-operator</a> </p><h1 id="Nginx-ingress"><a href="#Nginx-ingress" class="headerlink" title="Nginx-ingress"></a>Nginx-ingress</h1><p>同样的是整个集群的反向代理，可支持四层代理</p><p>此处安装是建立在prometheus-operator的基础上的。</p><p>直接下载官方的charts，然后找到traefik就可以执行安装了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone &lt;https://github.com/helm/charts.git&gt;</span><br></pre></td></tr></table></figure><p>详细参数介绍请参考官方地址<br><a href="https://github.com/helm/charts/tree/master/stable/nginx-ingress#configuration">https://github.com/helm/charts/tree/master/stable/nginx-ingress#configuration</a></p><p>具体的镜像下载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker pull mirrorgooglecontainers/defaultbackend:1.4</span><br><span class="line">docker tag mirrorgooglecontainers/defaultbackend:1.4 k8s.gcr.io/defaultbackend:1.4</span><br><span class="line"></span><br><span class="line">docker pull huisebug/sec_re:nginx-ingress-controller-0.21.0</span><br><span class="line">docker tag huisebug/sec_re:nginx-ingress-controller-0.21.0 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0</span><br></pre></td></tr></table></figure><h2 id="安装nginx-ingress"><a href="#安装nginx-ingress" class="headerlink" title="安装nginx-ingress"></a>安装nginx-ingress</h2><p>首先需要删除traefik</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">helm delete traefik --purge</span><br><span class="line">helm delete nginx-ingress --purge</span><br></pre></td></tr></table></figure><p>官方的可配置参数各种坑不断，不推荐用命令行参数，直接修改values.yaml比较方便。下面是我修改的参数值：</p><ul><li>  部署类型为daemonset</li><li>  开启hostNetwork功能</li><li>  资源配额</li><li>  更改service type： LoadBalancer为ClusterIP</li><li>  rbac开启</li><li>  serviceAccount名称为nginx-ingress-controller</li><li>  开启metrics</li><li>  开启servicemonitor，设置namespace为monitoring便于prometheus-operator监控</li></ul><p>部署<br>参考我的valuesdaemonset.yaml文件地址：<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/nginx-ingress/valuesdaemonset.yaml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/nginx-ingress/valuesdaemonset.yaml</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helm delete nginx-ingress --purge</span><br><span class="line">helm install --name nginx-ingress --namespace kube-system /root/charts/stable/nginx-ingress </span><br><span class="line">-f /root/charts/stable/nginx-ingress/valuesdaemonset.yaml</span><br></pre></td></tr></table></figure><h2 id="验证效果-3"><a href="#验证效果-3" class="headerlink" title="验证效果"></a>验证效果</h2><p><img src="/2020/08/26/k8s/media/341628f03517a9fa7797699c65852ce4.png"></p><p>访问prometheus查看是否添加了nginx-ingress-controller的metrics</p><p><img src="/2020/08/26/k8s/media/28a8d1fc8edc5fad645b1dada6f8cc4d.png"></p><p>如何验证默认后端（nginx-ingress-default-backend）效果呢？</p><p>首先我们在本地hosts文件中建立一个整个k8s集群中没有定义ingress，不存在的域名并指向nginx-ingress-controller，访问测试即可</p><p><img src="/2020/08/26/k8s/media/58ed9d31362103f188537195c5a8dda3.png"></p><p>访问测试</p><p><img src="/2020/08/26/k8s/media/b6e69420019de9c1fcdc79edb8fc1af3.png"></p><h6 id><a href="#" class="headerlink" title></a></h6><p>也可以参考我已经修改好的charts，参考地址：<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/nginx-ingress">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/nginx-ingress</a></p><h6 id="-1"><a href="#-1" class="headerlink" title></a></h6><h2 id="nginx-ingress-controller-nginx功能"><a href="#nginx-ingress-controller-nginx功能" class="headerlink" title="nginx-ingress-controller nginx功能"></a>nginx-ingress-controller nginx功能</h2><p>nginx-ingress-controller 并不像traefik一样提供WEB界面功能</p><p>nginx-ingress-controller-ingress.yaml<br>参考地址：<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/nginx-ingress/ingress.yaml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/nginx-ingress/ingress.yaml</a></p><p>验证效果，这个返回的页面是nginx-ingress-controller返回的，并不是后端返回的</p><p><img src="/2020/08/26/k8s/media/c8bc815bef8e22e91dbb8e2779559c28.png"></p><h2 id="nginx-ingress-controller的四层代理"><a href="#nginx-ingress-controller的四层代理" class="headerlink" title="nginx-ingress-controller的四层代理"></a>nginx-ingress-controller的四层代理</h2><p>nginx从1.9.0开始，新增加了一个stream模块，用来实现四层协议的转发、代理或者负载均衡等。可以配置TCP或者UDP来实现这个功能</p><h3 id="代理集群的coredns"><a href="#代理集群的coredns" class="headerlink" title="代理集群的coredns"></a>代理集群的coredns</h3><p>只需要在values.yaml文件中增加UDP协议端口：coredns的service名：service_IP</p><p><img src="/2020/08/26/k8s/media/0c70598ca20e83a5f36f4444db4bdec4.png"></p><p>重新建立nginx-ingress-controller后，查看是否成功开启UDP 53端口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -lnup | grep 53</span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/821127b3b80f30705b270f49349d209f.png"></p><p>集群完整的域名如何获取？</p><p>我们使用一个pod去ping另一个pod的service名称，kube-dns会解析出完整的域名，就可以修改其中的service名和对应的namespace即可</p><p><img src="/2020/08/26/k8s/media/82e7d16fd70b1d760463183eeafd14c6.png"></p><p>验证效果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nslookup -qt=A kubernetes.default.svc.cluster.local 192.168.137.13</span><br><span class="line">nslookup -qt=A kube-dns.kube-system.svc.cluster.local 192.168.137.13</span><br><span class="line">nslookup -qt=A prometheus-k8s.monitoring.svc.cluster.local 192.168.137.13</span><br></pre></td></tr></table></figure><p>windows CMD验证</p><p><img src="/2020/08/26/k8s/media/7671a9d3a5beb90673783537c32d0cf7.png"></p><p>linux 验证</p><p><img src="/2020/08/26/k8s/media/e08988c2ffa569bce75b7972fcb2fac8.png"></p><h1 id="Metrics-server"><a href="#Metrics-server" class="headerlink" title="Metrics-server"></a>Metrics-server</h1><p>kubernetes Metrics Server是资源使用数据的集群范围聚合器，是Heapster的后继者。度量服务器通过汇集来自kubernetes.summary_api的数据来收集节点和pod的CPU和内存使用情况。摘要API是一种内存高效的API，用于将数据从Kubelet / cAdvisor传递到度量服务器。</p><p>从 v1.8 开始，资源使用情况的度量（如容器的 CPU 和内存使用）可以通过 Metrics API获取。注意：</p><ul><li>  Metrics API 只可以查询当前的度量数据，并不保存历史数据</li><li>  Metrics API URI 为 /apis/metrics.k8s.io/，在 k8s.io/metrics 维护</li><li>  必须部署 metrics-server 才能使用该 API，metrics-server 通过调用 Kubelet Summary API 获取数据</li><li>  在新版本的kubernetes中 Pod CPU使用率不在来源于heapster,而是来自于metrics-server</li><li>  支持metrics-server必须在api-server中添加如下参数</li></ul><p>设置apiserver相关参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">--requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem </span><br><span class="line">--proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.pem </span><br><span class="line">--proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client-key.pem </span><br><span class="line">--requestheader-allowed-names=aggregator </span><br><span class="line">--requestheader-group-headers=X-Remote-Group </span><br><span class="line">--requestheader-extra-headers-prefix=X-Remote-Extra- </span><br><span class="line">--requestheader-username-headers=X-Remote-User </span><br></pre></td></tr></table></figure><h2 id="部署metrics-server"><a href="#部署metrics-server" class="headerlink" title="部署metrics-server"></a>部署metrics-server</h2><p>此处将metrics-server放到prometheus-operator后面是因为在prometheus-operator建立的时候就建立了一个以prometheus-adapter为支持的<strong>api：metrics.k8s.io</strong>，这个api建立后就可以使用kubectl top命令了，但是prometheus-operator方式建立的<strong>api：metrics.k8s.io</strong>仅仅支持kubectltop pod，并不支持kubectl top node。<br>为了后续的<strong>api：custom.metrics.k8s.io</strong>使用prometheus-adapter来作服务支撑和使用metrics-server来建立<strong>api：metrics.k8s.io</strong>，我们需要移除prometheus-operator以prometheus-adapter为支持的<strong>api：metrics.k8s.io</strong></p><p>将prometheus-adapter的所有yaml文件归档到一个文件目录中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir prometheus-adapter</span><br><span class="line">mv prometheus-adapter-* prometheus-adapter/</span><br><span class="line">kubectl delete -f prometheus-adapter/</span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/1eb6a6f5a131ff6287126ff61259575d.png"></p><p>然后部署metrics-server，具体部署yaml参考地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Metrics-server1.12/Metrics-server1.12.yaml</span><br></pre></td></tr></table></figure><h2 id="验证效果-4"><a href="#验证效果-4" class="headerlink" title="验证效果"></a>验证效果</h2><p>获取命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl api-versions | grep metrics.k8s.io</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl top pod --all-namespaces</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl top node</span></span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/12031d891104780e561b9742041e8b3f.png"></p><p>查看nodes metrics：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get --raw &quot;/apis/metrics.k8s.io/v1beta1/nodes&quot; | jq .</span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/d213f882d29ff203408d0021dcbc860c.png"></p><h1 id="HPA"><a href="#HPA" class="headerlink" title="HPA"></a>HPA</h1><p>此栏目已经迁移到<a href="https://huisebug.github.io/2019/08/28/k8s-hpa/">k8s-HPA</a> </p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这是一个k8s集群，从零开始搭建服务器环境，keepalived+haproxy实现集群高可用VIP；glusterfs搭建实现可持续存储；k8s1.12版本集群；efk日志系统；prometheus-operator告警系统；HPA v2横向pod扩容；k8s1.11+以上的版本部署基本没什么区别，无非就是优化了一些参数的配置和增加一些功能，舍弃一些api，此文档适用后续发布的其他版本的部署，提供一些部署思路。&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes" scheme="https://huisebug.github.io/categories/Kubernetes/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="hpav2" scheme="https://huisebug.github.io/tags/hpav2/"/>
    
    <category term="hpa" scheme="https://huisebug.github.io/tags/hpa/"/>
    
  </entry>
  
  <entry>
    <title>Jenkins持续集成到Kubernetes集群</title>
    <link href="https://huisebug.github.io/2020/06/03/jenkins-CICD-k8s/"/>
    <id>https://huisebug.github.io/2020/06/03/jenkins-CICD-k8s/</id>
    <published>2020-06-03T09:04:01.000Z</published>
    <updated>2021-07-07T08:50:19.883Z</updated>
    
    <content type="html"><![CDATA[<p>这是一个k8s集群结合jenkins做持续集成，以及jenkins流水线在k8s集群中运行。java编译后、nodejs编译后docker镜像制作。</p><span id="more"></span><h1 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h1><p>K8s集群1.15.10</p><p>Docker-ce 19.03</p><p>nfs卷</p><p>服务器信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">192.168.137.100:k8s-master/k8s-node</span><br><span class="line">192.168.137.101:k8s-master/k8s-node</span><br><span class="line">192.168.137.102:k8s-master/k8s-node</span><br><span class="line">192.168.137.5:nfs服务、docker私有镜像仓库</span><br></pre></td></tr></table></figure><h1 id="nfs服务部署"><a href="#nfs服务部署" class="headerlink" title="nfs服务部署"></a>nfs服务部署</h1><p>nfsinstall.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">docker run -d --restart=always --name nfs_server --privileged -p 2049:2049 \</span><br><span class="line">-v /data/nfs:/nfsroot \</span><br><span class="line">-h nfsserver \</span><br><span class="line">-e SHARED_DIRECTORY=/nfsroot \</span><br><span class="line">huisebug/nfs-server:latest</span><br></pre></td></tr></table></figure><p>执行脚本即可在服务器上部署一个nfs服务</p><h1 id="docker私有镜像仓库"><a href="#docker私有镜像仓库" class="headerlink" title="docker私有镜像仓库"></a>docker私有镜像仓库</h1><p>创建私有仓库带有简单的密码认证,用户名admin，密码huisebug</p><p>dockerregistryinstall.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">dockerregistryinstall()&#123;</span><br><span class="line">mkdir -p /data/dockerregistry/auth</span><br><span class="line">docker run --entrypoint htpasswd registry:2 -Bbn admin huisebug  &gt; /data/dockerregistry/auth/htpasswd</span><br><span class="line"></span><br><span class="line">docker run -d -p 80:5000 --restart=always --name registry \</span><br><span class="line">   -v /data/dockerregistry/auth/:/auth \</span><br><span class="line">   -e &quot;REGISTRY_AUTH=htpasswd&quot; \</span><br><span class="line">   -e &quot;REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm&quot; \</span><br><span class="line">   -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \</span><br><span class="line">   -v /data/dockerregistry/data:/var/lib/registry \</span><br><span class="line">   registry:2</span><br><span class="line">&#125;</span><br><span class="line">dockerregistryinstall</span><br></pre></td></tr></table></figure><h1 id="Jenkins部署"><a href="#Jenkins部署" class="headerlink" title="Jenkins部署"></a>Jenkins部署</h1><h2 id="新建jenkins命名空间"><a href="#新建jenkins命名空间" class="headerlink" title="新建jenkins命名空间"></a>新建jenkins命名空间</h2><p>jenkins-ns.yaml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">   name: jenkins</span><br><span class="line">   labels:</span><br><span class="line">     name: jenkins</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="持久化存储jenkins和maven"><a href="#持久化存储jenkins和maven" class="headerlink" title="持久化存储jenkins和maven"></a>持久化存储jenkins和maven</h2><p>准备好2个nfs数据卷用于jenkins持久化和maven仓库持久化（此处我是使用1个，采用不同目录进行区分）</p><p>因为jenkins的kubernetes插件功能不是很完善，所以这里的pvc在pod中是不支持subpath的，所以需要在pv的path下写好对应的路径</p><p>登录到nfs所在机器192.168.137.5,建立pv所需的路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /data/nfs</span><br><span class="line">mkdir -p jenkins/home</span><br><span class="line">mkdir -p jenkins/maven</span><br></pre></td></tr></table></figure><h3 id="jenkins-pv-pvc"><a href="#jenkins-pv-pvc" class="headerlink" title="jenkins-pv-pvc"></a>jenkins-pv-pvc</h3><p>jenkins-pv-pvc.yaml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins</span><br><span class="line">  labels:</span><br><span class="line">    app: jenkins</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 20Gi</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  persistentVolumeReclaimPolicy: Retain</span><br><span class="line">  volumeMode: Filesystem</span><br><span class="line">  nfs:</span><br><span class="line">    server: 192.168.137.5</span><br><span class="line">    path: /jenkins/home</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: jenkins</span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: jenkins</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 20Gi</span><br><span class="line">  storageClassName: &quot;&quot;</span><br><span class="line">  volumeMode: Filesystem</span><br><span class="line">  volumeName: jenkins</span><br></pre></td></tr></table></figure><h3 id="maven-pv-pvc"><a href="#maven-pv-pvc" class="headerlink" title="maven-pv-pvc"></a>maven-pv-pvc</h3><p>maven-pv-pvc.yaml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: maven</span><br><span class="line">  labels:</span><br><span class="line">    app: maven</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 20Gi</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  nfs:</span><br><span class="line">    server: 192.168.137.5</span><br><span class="line">    path: /jenkins/maven</span><br><span class="line">  persistentVolumeReclaimPolicy: Retain</span><br><span class="line">  volumeMode: Filesystem</span><br><span class="line">---</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: maven</span><br><span class="line">  name: maven</span><br><span class="line">  namespace: jenkins</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 20Gi</span><br><span class="line">  storageClassName: &quot;&quot;</span><br><span class="line">  volumeMode: Filesystem</span><br><span class="line">  volumeName: maven</span><br></pre></td></tr></table></figure><h2 id="Jenkins-rbac"><a href="#Jenkins-rbac" class="headerlink" title="Jenkins-rbac"></a>Jenkins-rbac</h2><p>jenkins需要建立pod权限以调用k8s集群建立jenkins构建节点pod</p><p>jenkins-rbac.yaml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: jenkins</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: jenkins</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;extensions&quot;, &quot;apps&quot;]</span><br><span class="line">    resources: [&quot;deployments&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;, &quot;delete&quot;, &quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;patch&quot;, &quot;update&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;services&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;, &quot;delete&quot;, &quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;patch&quot;, &quot;update&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;pods&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;pods/exec&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;pods/log&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;,&quot;list&quot;,&quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;secrets&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;]</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: jenkins</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: jenkins</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: jenkins</span><br></pre></td></tr></table></figure><h2 id="Jenkins"><a href="#Jenkins" class="headerlink" title="Jenkins"></a>Jenkins</h2><p>jenkins.yaml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: jenkins</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: jenkins</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 10</span><br><span class="line">      serviceAccount: jenkins</span><br><span class="line">      containers:</span><br><span class="line">      - name: jenkins</span><br><span class="line">        image: jenkins/jenkins:lts</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">          name: web</span><br><span class="line">          protocol: TCP</span><br><span class="line">        - containerPort: 50000</span><br><span class="line">          name: agent</span><br><span class="line">          protocol: TCP</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1000m</span><br><span class="line">            memory: 1Gi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 500m</span><br><span class="line">            memory: 512Mi</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /login</span><br><span class="line">            port: 8080</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">          failureThreshold: 12 </span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /login</span><br><span class="line">            port: 8080</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">          failureThreshold: 12</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: jenkinshome</span><br><span class="line">          subPath: jenkins</span><br><span class="line">          mountPath: /var/jenkins_home</span><br><span class="line">        env:</span><br><span class="line">        - name: LIMITS_MEMORY</span><br><span class="line">          valueFrom:</span><br><span class="line">            resourceFieldRef:</span><br><span class="line">              resource: limits.memory</span><br><span class="line">              divisor: 1Mi</span><br><span class="line">        - name: JAVA_OPTS</span><br><span class="line">          value: -Xmx$(LIMITS_MEMORY)m -XshowSettings:vm -Dhudson.slaves.NodeProvisioner.initialDelay=0 -Dhudson.slaves.NodeProvisioner.MARGIN=50 -Dhudson.slaves.NodeProvisioner.MARGIN0=0.85 -Duser.timezone=Asia/Shanghai</span><br><span class="line">      securityContext:</span><br><span class="line">        fsGroup: 1000</span><br><span class="line">      volumes:</span><br><span class="line">      - name: jenkinshome</span><br><span class="line">        persistentVolumeClaim:</span><br><span class="line">          claimName: jenkins</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: jenkins</span><br><span class="line">  labels:</span><br><span class="line">    app: jenkins</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    app: jenkins</span><br><span class="line">  ports:</span><br><span class="line">  - name: web</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: web</span><br><span class="line">    nodePort: 30000</span><br><span class="line">  - name: agent</span><br><span class="line">    port: 50000</span><br><span class="line">    targetPort: agent</span><br></pre></td></tr></table></figure><p>注意事项：</p><p>如果jenkins提示权限问题</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/5662511e2ac0f43bd196847ced4cf757.png"></p><p>因为jenkins镜像中使用的账户是jenkins，而非root用户，所以需要将jenkins使用的pv下的path路径加上jenkins中提交的subpath重新授权</p><p>登录到nfs所在机器192.168.137.5</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /data/nfs/jenkins/home</span><br><span class="line">chown -R 1000 jenkins/</span><br></pre></td></tr></table></figure><p><img src="/2020/06/03/jenkins-CICD-k8s/media/c4d5c6811ce0bf8b6cf9ee70bc3b69b0.png"></p><p>授权后重新部署jenkins</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -f Jenkins.yaml &amp;&amp; kubectl apply -f Jenkins.yaml</span><br></pre></td></tr></table></figure><h2 id="get-k8s-admin-cert"><a href="#get-k8s-admin-cert" class="headerlink" title="get_k8s_admin_cert"></a>get_k8s_admin_cert</h2><p>jenkins构建节点的docker镜像内置了kubectl客户端，用于job完成打包到部署的流程，所以需要执行get_k8s_admin_cert.sh脚本获取k8s管理员证书建立secret</p><p>因为jenkins的kubernetes插件功能不是很完善，所以这里的secret的key不能单独指定</p><p>get_k8s_admin_cert.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">建立jenkins构建节点pod中kubectl客户端所需的k8s管理员证书</span></span><br><span class="line">kubectl -n jenkins create secret generic  k8sadmin --from-file=/root/.kube/config</span><br></pre></td></tr></table></figure><h1 id="验证访问"><a href="#验证访问" class="headerlink" title="验证访问"></a>验证访问</h1><p>访问<a href="http://192.168.137.101:30000/">http://192.168.137.101:30000</a></p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/9a868befb4f79a4e93891051dfcaeabc.png"></p><p>准备完毕</p><h1 id="Jenkins配置"><a href="#Jenkins配置" class="headerlink" title="Jenkins配置"></a>Jenkins配置</h1><h2 id="安装kubernetes插件"><a href="#安装kubernetes插件" class="headerlink" title="安装kubernetes插件"></a>安装kubernetes插件</h2><p>因为网络原因需要将jenkins代理设置位置：<a href="http://jenkins访问url/pluginManager/advanced">http://jenkins访问url/pluginManager/advanced</a></p><p>将升级站点处的url地址修改为：<a href="http://mirror.xmission.com/jenkins/updates/current/update-center.json">http://mirror.xmission.com/jenkins/updates/current/update-center.json</a></p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/0c4b674450b636a558782580d1a9c3f9.png"></p><p>然后点击提交</p><p>切换到插件管理进行插件安装</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/20d9bd3e6f8985401213b8ab9b108451.png"></p><p>等待安装完毕即可</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/5b77afb8002eb93c0e995894a2fee144.png"></p><h2 id="配置kubernetes节点信息"><a href="#配置kubernetes节点信息" class="headerlink" title="配置kubernetes节点信息"></a>配置kubernetes节点信息</h2><p>系统管理–节点管理—Configure Clouds</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/534ec5d1523e40cb87661456ac900d86.png"></p><p>安装了kubernetes插件这里就会出现kubernetes可选项</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/8cbab7e483c0b48ccc1da1ceada128d8.png"></p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/3aebc8174620eaa633a1aadcef5f6b1d.png"></p><h2 id="Kubernetes-Cloud-details配置"><a href="#Kubernetes-Cloud-details配置" class="headerlink" title="Kubernetes Cloud details配置"></a>Kubernetes Cloud details配置</h2><p>点击Kubernetes Cloud details按钮即可开始配置</p><p>名称：kubernetes</p><p>kubernetes地址：<a href="https://kubernetes.default.svc.cluster.local/">https://kubernetes.default.svc.cluster.local</a></p><p>kubernetes命名空间：jenkins 即 jenkins服务所在的命名空间namespace</p><p>配置完毕后点击连接测试，查看是否可以连接成功</p><p>jenkins地址：<a href="http://jenkins.jenkins.svc.cluster.local:8080/">http://jenkins.jenkins.svc.cluster.local:8080</a><br>即jenkins服务在k8s集群中使用完整的svc名称的访问地址</p><p>其他项默认即可</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/05c0f00a2a1fdae7abb1fe0d5ad64ac2.png"></p><h2 id="Pod-Templates配置"><a href="#Pod-Templates配置" class="headerlink" title="Pod Templates配置"></a>Pod Templates配置</h2><p>因为我们需要定义一些功能，所以需要自定义jenkins构建节点pod的配置</p><p>点击添加pod模板后再点击Pod Template details按钮即可开始配置</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/255b9d0e92403bcaa35bf6c2d7762565.png"></p><p>注意事项：</p><ul><li><p>  命令空间填写和jenkins同一命名空间</p></li><li><p>  标签列表就是后续job调用这个pod运行的标签，后续需要写到job中去</p></li><li><p>  容器列表名称的pod模板中的容器名必须为jnlp，这样就只会有一个容器</p></li><li><p>  容器镜像必须是以jenkins/jnlp-slave镜像为基础的镜像，可以以此为基础镜像增加服务，此处我的镜像增加了mvn、nodejs、docker、kubectl</p></li><li><p>  如果pod模板中容器名称不是jnlp，那么就会认为是一个额外的容器，就会默认建立一个容器名为jnlp使用镜像jenkins/jnlp-slave的容器。然后将模板中的容器名称作为pod中的第2个容器的容器名，镜像使用你填写的镜像</p></li><li><p>  pipline工作中调用的容器永远是容器名为jnlp的容器</p></li></ul><h2 id="卷挂载（非必须的）"><a href="#卷挂载（非必须的）" class="headerlink" title="卷挂载（非必须的）"></a>卷挂载（非必须的）</h2><p><img src="/2020/06/03/jenkins-CICD-k8s/media/2546fa4d2ca83fd0f937f9c0f872ea68.png"></p><p>注意事项：</p><ul><li>  docker打包的时候需要将调度的k8s-node的docker.sock挂载到容器中，便于容器中的docker服务进行打包，然后推送镜像到私有仓库，推送镜像需要认证私有仓库信息。认证的时候是根据挂载的docker.sock的，所以k8s-node的docker服务需要将私有仓库信息添加到insecure-registries中，比如此处我的私有镜像仓库为172.168.137.5</li></ul><p><img src="/2020/06/03/jenkins-CICD-k8s/media/86d752d8beaf8302ba796150af848ece.png"></p><ul><li><p>  java项目打包的时候会下载很多依赖插件，所以此处需要将maven的仓库持久化存储，加快打包效率，注意我的jenkinsbuild镜像中使用的是root用户运行的，所以maven的仓库数据是放在/root/.m2的。</p></li><li><p>  secret认证是用于kubectl客户端连接k8s集群，便于k8s镜像发布，secret的创建在之前的get_k8s_admin_cert.sh已经建立</p></li><li><p>  其他的配置默认即可，例如：代理的空闲存活时间（分）代表jenkins构建节点pod的存活时间，一般默认即可</p></li></ul><h1 id="Jenkins自由风格Job验证"><a href="#Jenkins自由风格Job验证" class="headerlink" title="Jenkins自由风格Job验证"></a>Jenkins自由风格Job验证</h1><p><img src="/2020/06/03/jenkins-CICD-k8s/media/4ac012a55e42313cad69db7cd10d06ab.png"></p><h2 id="限制项目的运行节点"><a href="#限制项目的运行节点" class="headerlink" title="限制项目的运行节点"></a>限制项目的运行节点</h2><p><img src="/2020/06/03/jenkins-CICD-k8s/media/ea3186c54da301f079b0e4a46b8cd0cd.png"></p><p>此处就对应之前pod template中的标签列表</p><h2 id="构建操作，执行一个shell"><a href="#构建操作，执行一个shell" class="headerlink" title="构建操作，执行一个shell"></a>构建操作，执行一个shell</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br><span class="line">mvn -v</span><br><span class="line">nodejs -v</span><br><span class="line">docker info</span><br><span class="line">kubectl get pod -n jenkins</span><br></pre></td></tr></table></figure><p><img src="/2020/06/03/jenkins-CICD-k8s/media/f694753eb556140c75e4247ea4e8efb5.png"></p><p>保存后点击构建</p><p>会出现下面的状态，正在创建pod以获取输出，如果一直没有变化，那么就是pod建立失败，需要查看一下kubernetes节点配置是否正确，大多数原因是因为pod的配置</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/37357509031ed3b1d088ed8ed26a049a.png"></p><p>等待建立构建的pod，可以在终端查看到</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/0b077fc9098d1f93860c632af3ab0fb0.png"></p><p>pod建立成功将会在jenkins控制台进行输出，也会输出pod的建立yaml，可以根据来排查问题</p><p>成功获取到了上述shell执行结果</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/2d3e57dca1b2d82746ea73672cacdc20.png"></p><h1 id="Jenkins认证凭证"><a href="#Jenkins认证凭证" class="headerlink" title="Jenkins认证凭证"></a>Jenkins认证凭证</h1><p><img src="/2020/06/03/jenkins-CICD-k8s/media/b8b0051d9be94b8ee731ba1d50353e8b.png"></p><p>添加好jenkins认证凭证，可以用于后续的job拉取git仓库代码、pipline中使用git函数利用凭证的ID字段进行认证，上面的ID是可以手动定义的，后续将会用到</p><h1 id="Jenkins-Pipline"><a href="#Jenkins-Pipline" class="headerlink" title="Jenkins Pipline"></a>Jenkins Pipline</h1><p>构建定制化的持续集成是必须的工作，pipline的使用就不过多介绍,此处介绍本次使用的pipline过程</p><p>参考github地址：<a href="https://github.com/huisebug/jenk8s-pipline.git">https://github.com/huisebug/jenk8s-pipline.git</a> 和 <a href="https://github.com/huisebug/jenk8s-install.git">https://github.com/huisebug/jenk8s-install.git</a></p><ul><li><p>  build.groovy：流水线pipline文件</p></li><li><p>  dockerfile：java环境Dockerfile，nodejs项目Dockerfile，jenkins构建节点的Dockerfile</p></li><li><p>  jenkins：jenkins部署到k8s中的yaml</p></li><li><p>  projects：存放需要打包的项目的信息，文件目录名区分项目，project_info文件中写入：项目名称#服务名称=服务的git代码地址</p></li><li><p>  scripts：存放docker镜像制作的脚本</p></li></ul><h2 id="build-groovy"><a href="#build-groovy" class="headerlink" title="build.groovy"></a>build.groovy</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line">//当前的jenkins管理节点</span><br><span class="line">node &#123;</span><br><span class="line">//配置全局变量</span><br><span class="line">env.project = env.JOB_BASE_NAME.split(&quot;\\+&quot;)[0]</span><br><span class="line">env.app_name = env.JOB_BASE_NAME.split(&quot;\\+&quot;)[1]</span><br><span class="line">env.branch = env.JOB_BASE_NAME.split(&quot;\\+&quot;)[2]</span><br><span class="line">println(env.project)</span><br><span class="line">println(env.app_name)</span><br><span class="line">println(env.branch)</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line">因为jenkins的版本原因，使用git方式拉取了jenkinsfile的项目所有源代码文件</span><br><span class="line">在系统workspace目录下建立了一个job名+@script的文件目录来存放jenkinsfile源代码所有文件</span><br><span class="line">pipline执行的时候是以job名称为文件目录名称下</span><br><span class="line">类似于</span><br><span class="line">[root@localhost workspace]# tree</span><br><span class="line">.</span><br><span class="line">├── jenkins+java+master</span><br><span class="line">│   └── java-ci</span><br><span class="line">├── jenkins+java+master@script</span><br><span class="line">│   ├── build.groovy</span><br><span class="line">│   └── projects</span><br><span class="line">│       └── jenkins</span><br><span class="line">│           └── project_info</span><br><span class="line">*/</span><br><span class="line"></span><br><span class="line">//jenkinsfile的源代码存放目录名称,此处是:job名称+@script</span><br><span class="line">env.tempsuffix = &#x27;@script&#x27;</span><br><span class="line">env.JenkinsfileREPO = env.JOB_BASE_NAME + env.tempsuffix</span><br><span class="line"></span><br><span class="line">//获取代码仓库地址,作为全局变量提供给打包k8s节点使用</span><br><span class="line">if (env.app_name == &#x27;&#x27;) &#123;</span><br><span class="line">env.app_name = &#x27;none&#x27;</span><br><span class="line">env.git_repository = sh returnStdout: true, script: &#x27;cat ../&#x27;+env.JenkinsfileREPO+&#x27;/projects/&#x27;+env.project+&#x27;project_info|grep &#x27;+env.project+&#x27;_repo|awk -F &quot;=&quot; \&#x27;&#123;print $2&#125;\&#x27;&#x27;</span><br><span class="line">env.ci_dir =  env.project+&#x27;-ci&#x27;</span><br><span class="line"></span><br><span class="line">&#125; else &#123;</span><br><span class="line">env.git_repository = sh returnStdout: true, script: &#x27;cat ../&#x27;+env.JenkinsfileREPO+&#x27;/projects/&#x27;+env.project+&#x27;/project_info|grep &#x27;+env.project+&#x27;#&#x27;+env.app_name+&#x27;_repo|awk -F &quot;=&quot; \&#x27;&#123;print $2&#125;\&#x27;&#x27;    </span><br><span class="line">env.ci_dir =  env.app_name+&#x27;-ci&#x27;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//CI打包k8s节点，这里就是匹配之前的pod template中的标签列表</span><br><span class="line">node(&#x27;jenkinsbuildnode&#x27;) &#123;</span><br><span class="line">echo &quot;本次build的项目的源代码地址: &quot;+env.git_repository</span><br><span class="line">        </span><br><span class="line">stage(&#x27;CI打包&#x27;) &#123;</span><br><span class="line">    /*tools &#123;</span><br><span class="line">                maven &#x27;3.6.2&#x27;</span><br><span class="line">                jdk &#x27;1.8.0_242&#x27;</span><br><span class="line">                nodejs &#x27;10.19.0&#x27;</span><br><span class="line">npm &#x27;5.8.0&#x27;</span><br><span class="line">            &#125;*/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">script &#123;       </span><br><span class="line">                    /*判断是否推送到生产仓库</span><br><span class="line">                    try &#123;</span><br><span class="line">                        timeout(time: 70, unit: &#x27;SECONDS&#x27;) &#123;</span><br><span class="line">                            def userInput = input(id: &#x27;userInput&#x27;, ok: &#x27;确定&#x27;, message: &#x27;是否推送镜像到生产环境&#x27;, parameters: [booleanParam(defaultValue:  false, description: &#x27;&#x27;, name: &#x27;发布到master仓库&#x27;)])</span><br><span class="line">                            //println userInput</span><br><span class="line">                            println userInput.getClass()</span><br><span class="line">                            if(userInput==true)&#123;</span><br><span class="line">                                env.to_master_registry = &quot;go&quot;</span><br><span class="line">                            &#125;else &#123;</span><br><span class="line">                                env.to_master_registry = &quot;&quot;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">catch(Exception ex) &#123;</span><br><span class="line">                        println(&quot;Catching  exception&quot;)</span><br><span class="line">                        echo &#x27;do nothing, continue.......&#x27;</span><br><span class="line">                        env.to_master_registry = &quot;&quot;</span><br><span class="line">                    &#125;*/</span><br><span class="line">dir(env.ci_dir) &#123;</span><br><span class="line">                        echo &quot;拉取git代码&quot;</span><br><span class="line">                        checkout([$class: &#x27;GitSCM&#x27;, branches: [[name: branch]], doGenerateSubmoduleConfigurations: false, extensions: [[$class: &#x27;CleanBeforeCheckout&#x27;]], submoduleCfg: [], userRemoteConfigs: [[credentialsId: &#x27;huisebug&#x27;, url: git_repository]]])                    </span><br><span class="line">                        git_commit_hash = sh (script: &quot;git log -n 1 --pretty=format:&#x27;%H&#x27;&quot;, returnStdout: true)</span><br><span class="line">                        env.git_commit_hash = &#x27;-&#x27;+git_commit_hash[0..4]                        </span><br><span class="line"></span><br><span class="line">                        project = sh returnStdout: true, script: &#x27;cat app.info|grep devlang|awk -F &quot;:&quot; \&#x27;&#123;print $2&#125;\&#x27;&#x27;</span><br><span class="line">                        project = project.replace(&quot;\r&quot;,&quot;&quot;).replace(&quot; &quot;, &quot;&quot;).replace(&#x27;\n&#x27;, &#x27;&#x27;)</span><br><span class="line">                        echo &quot;项目类型是&quot;+project</span><br><span class="line">                        println project.class</span><br><span class="line"></span><br><span class="line">                        def version = sh returnStdout: true, script: &#x27;&#x27;&#x27; cat app.info |grep version|awk -F &quot;:&quot; &#x27;&#123;print $2&#125;&#x27; &#x27;&#x27;&#x27;</span><br><span class="line">                        env.ver = version.replace(&quot;\r&quot;, &quot;&quot;).replace(&quot;\n&quot;, &quot;&quot;).replace(&quot; &quot;, &quot;&quot;)</span><br><span class="line">                        echo &#x27;打包版本&#x27;+env.ver</span><br><span class="line">                        </span><br><span class="line">if (project == &#x27;NodeJs&#x27;||project == &#x27;nodejs&#x27;) &#123;</span><br><span class="line">                            sh &#x27;mkdir -pv target&#x27;</span><br><span class="line">                            //sh &#x27;find ./ -type f -exec dos2unix -q &#123;&#125; \\;&#x27;</span><br><span class="line">                            sh &#x27;npm cache clean --force&#x27;</span><br><span class="line">                            sh &#x27;npm config set registry http://registry.npm.taobao.org/&#x27;</span><br><span class="line">                            sh &#x27;cnpm install&#x27;</span><br><span class="line">                            if (env.JOB_BASE_NAME.tokenize(&#x27;+&#x27;)[3] == &#x27;dev&#x27;) &#123;</span><br><span class="line">                                sh &#x27;npm run build:dev&#x27;</span><br><span class="line">                            &#125; else if (env.JOB_BASE_NAME.tokenize(&#x27;+&#x27;)[3] == &#x27;test&#x27;) &#123;</span><br><span class="line">                                sh &#x27;npm run build:test&#x27;</span><br><span class="line">                            &#125; else if (env.JOB_BASE_NAME.tokenize(&#x27;+&#x27;)[3] == &#x27;sg&#x27;) &#123;</span><br><span class="line">                                sh &#x27;npm run build:sg&#x27;                               </span><br><span class="line">                            &#125; else &#123;</span><br><span class="line">                                sh &#x27;npm run build&#x27;</span><br><span class="line">                            &#125;</span><br><span class="line">                            sh &#x27;mv dist target/&#x27;</span><br><span class="line">                            echo project+&#x27;nodejs container Preparing....&#x27;</span><br><span class="line"></span><br><span class="line">                        &#125; else if (project == &#x27;Java&#x27;||project == &#x27;java&#x27;) &#123;</span><br><span class="line">                            echo project+&#x27; java container Preparing....&#x27;</span><br><span class="line">                            sh &#x27;mvn -Dmaven.test.skip=true clean package&#x27;   </span><br><span class="line"></span><br><span class="line">                        &#125; else if (project == &#x27;Python&#x27;||project == &#x27;python&#x27;) &#123;</span><br><span class="line">                            sh &#x27;mkdir -pv target&#x27;</span><br><span class="line">                            //sh &#x27;find ./ -type f -exec dos2unix -q &#123;&#125; \\;&#x27;</span><br><span class="line">                            sh &#x27;cp -rf `ls|grep -v app.info|grep -v target|xargs` target/&#x27;</span><br><span class="line">                            echo project+&#x27; python container Preparing....&#x27;</span><br><span class="line"></span><br><span class="line">                        &#125; else if (project == &#x27;HTML&#x27;||project == &#x27;html&#x27;) &#123;</span><br><span class="line">                            sh &#x27;mkdir -pv target/dist&#x27;</span><br><span class="line">                            //sh &#x27;find ./ -type f -exec dos2unix -q &#123;&#125; \\;&#x27;</span><br><span class="line">                            sh &#x27;cp -rf `ls|grep -v app.info|grep -v target|xargs` target/dist&#x27;</span><br><span class="line">                            echo project+&#x27; html container Preparing....&#x27;</span><br><span class="line"></span><br><span class="line">                        &#125; else &#123;</span><br><span class="line">                            sh &#x27;mkdir -pv target/dist&#x27;</span><br><span class="line">                            //sh &#x27;find ./ -type f -exec dos2unix -q &#123;&#125; \\;&#x27;</span><br><span class="line">                            sh &#x27;cp -rf `ls|grep -v app.info|grep -v target|xargs` target/dist&#x27;</span><br><span class="line">                            echo project+&#x27; html container Preparing....&#x27;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">stage(&#x27;docker镜像制作&#x27;) &#123;</span><br><span class="line">            script &#123;</span><br><span class="line">                sh &#x27;mkdir -pv dockerbuild&#x27;</span><br><span class="line">                dir(&#x27;dockerbuild&#x27;) &#123;</span><br><span class="line">                    deleteDir()</span><br><span class="line">                    echo &quot;拉取docker镜像制作脚本，代码分支一般默认是master&quot;,凭证信息credentialsId: &#x27;huisebug&#x27; ，直接使用jenkins存放的凭证的ID</span><br><span class="line">env.git_repo = &#x27;https://github.com/huisebug/jenk8s-pipline.git&#x27;</span><br><span class="line">                    checkout([$class: &#x27;GitSCM&#x27;, branches: [[name: &#x27;master&#x27;]], doGenerateSubmoduleConfigurations: false, extensions: [[$class: &#x27;CleanBeforeCheckout&#x27;]], submoduleCfg: [], userRemoteConfigs: [[credentialsId: &#x27;huisebug&#x27;, url: git_repo]]])                    </span><br><span class="line"></span><br><span class="line">                    println(&#x27;需要执行打包的容器名称有:&#x27;)</span><br><span class="line">                    sh &#x27;bash scripts/dockerbuild.sh&#x27;+&#x27; &#x27;+env.WORKSPACE+&#x27;/&#x27;+env.ci_dir+&#x27; &#x27;+env.app_name+&#x27; &#x27;+env.ver+&#x27; &#x27;+env.git_commit_hash+&#x27; &#x27;+env.project+&#x27; &#x27;+project+&#x27; &#x27;+env.to_master_registry</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>  pipline会首先在jenkins本地node执行build.groovy，根据当前的job名称来获取项目名称+服务名称+代码分支，例如：jenkins+java+master，然后去匹配projects文件目录中的信息，最终得到此次job的运行的服务名，git地址，分支信息传递到整个pipline的全局变量中</p></li><li><p>  然后运行在jenkins构建节点pod中运行拉取项目服务代码、项目服务编译、docker镜像制作</p></li></ul><h2 id="scripts-dockerbuild-sh"><a href="#scripts-dockerbuild-sh" class="headerlink" title="scripts/ dockerbuild.sh"></a>scripts/ dockerbuild.sh</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">脚本参数，<span class="variable">$1</span>:ci路径   <span class="variable">$2</span>:服务名称   <span class="variable">$3</span>:打包版本   <span class="variable">$4</span>:代码哈希  <span class="variable">$5</span> 项目名称  <span class="variable">$6</span>:项目类型(Html  or  Java  or  Nodejs)  <span class="variable">$7</span>:生产仓库(可选)</span></span><br><span class="line"><span class="meta">#</span><span class="bash">私库地址</span></span><br><span class="line">REGISTRY=&#x27;192.168.137.5&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">私库存放服务的仓库名</span></span><br><span class="line">WAREHOUSE_NAME=&#x27;huisebug&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">项目的CI目录名称</span></span><br><span class="line">if [ x$2 != xnone ];then</span><br><span class="line">app_name=$5-$2</span><br><span class="line">ci_dir=$2-ci</span><br><span class="line">else</span><br><span class="line">app_name=$5</span><br><span class="line">ci_dir=$5-ci</span><br><span class="line">fi</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">java项目镜像制作</span></span><br><span class="line">java()</span><br><span class="line">&#123;</span><br><span class="line">BASE_IMAGE=$&#123;REGISTRY&#125;/base/openjdk:8</span><br><span class="line">APP_HOME=/data/projects/$&#123;app_name&#125;</span><br><span class="line">EXE_CMD=&quot;java -jar&quot;</span><br><span class="line">EXE_BIN=$&#123;APP_HOME&#125;/bin/$&#123;app_name&#125;.jar</span><br><span class="line">EXE_CONF=&#x27;-Dlog_host=$&#123;log_host&#125;&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">jvm内存参数，临时设置，后续可以传参自定义</span></span><br><span class="line">EXE_LEVEL=&quot;-Xms1680M -Xmx1680M -Xmn1260M -Xss1M&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash">jvm其他参数调优</span></span><br><span class="line">EXE_OPTION=&quot;-server -XX:SurvivorRatio=8 -XX:+UseConcMarkSweepGC -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=512m -XX:MaxDirectMemorySize=1g -XX:+ExplicitGCInvokesConcurrent -XX:CMSInitiatingOccupancyFraction=80 -XX:-UseCMSInitiatingOccupancyOnly -Dsun.rmi.dgc.server.gcInterval=2592000000 -Dsun.rmi.dgc.client.gcInterval=2592000000 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=$&#123;APP_HOME&#125;/log/java.hprof -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cat &gt; run.sh &lt;&lt; \EOF</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">环境变量当前主机名</span></span><br><span class="line">log_host=`hostname`</span><br><span class="line"><span class="meta">#</span><span class="bash">jvm内存参数设置</span></span><br><span class="line">java_mem=`env|grep java_mem|awk -F &#x27;=&#x27; &#x27;&#123;print $2&#125;&#x27;`</span><br><span class="line"><span class="meta">#</span><span class="bash">服务日志是否在控制台输出，即容器即生命</span></span><br><span class="line">log_echo=`env|grep log_echo|awk -F &#x27;=&#x27; &#x27;&#123;print $2&#125;&#x27;`</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">判断变量java_mem是否为空，不存在即为空</span></span><br><span class="line">if [[ x$java_mem == x ]];then</span><br><span class="line">EXE_LEVEL=&quot;&quot;</span><br><span class="line">else</span><br><span class="line">EXE_LEVEL=$java_mem</span><br><span class="line">fi</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">额外的jvm参数</span></span><br><span class="line">if [[ x$general_para != x ]];then</span><br><span class="line">EXT_OPTION=$general_para</span><br><span class="line">else</span><br><span class="line">EXT_OPTION=&quot;&quot;</span><br><span class="line">fi</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">程序的参数</span></span><br><span class="line">if [[ x$application_para != x ]];then</span><br><span class="line">APP_OPTION=$application_para</span><br><span class="line">else</span><br><span class="line">APP_OPTION=&quot;&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">echo &quot;if [[ x\$&#123;log_echo&#125; != x ]];then&quot; &gt;&gt; run.sh</span><br><span class="line">echo $&#123;EXE_CMD&#125; \$&#123;EXE_LEVEL&#125; $&#123;EXE_OPTION&#125;  \$&#123;EXT_OPTION&#125; $&#123;EXE_CONF&#125; $&#123;EXE_BIN&#125; \$&#123;APP_OPTION&#125; &gt;&gt; run.sh</span><br><span class="line">echo &quot;else&quot; &gt;&gt; run.sh</span><br><span class="line">EXE_CMD=&quot;nohup java -jar&quot; </span><br><span class="line">echo $&#123;EXE_CMD&#125; \$&#123;EXE_LEVEL&#125; $&#123;EXE_OPTION&#125;  \$&#123;EXT_OPTION&#125; $&#123;EXE_CONF&#125; $&#123;EXE_BIN&#125; \$&#123;APP_OPTION&#125; &gt;&gt; run.sh</span><br><span class="line">echo &quot;tail -f /dev/null&quot; &gt;&gt; run.sh</span><br><span class="line">echo &quot;fi&quot; &gt;&gt; run.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">准备好将要复制到镜像中的服务文件</span></span><br><span class="line">rm -rf source</span><br><span class="line">mkdir -p source</span><br><span class="line">mv ../$&#123;ci_dir&#125;/target/* source/</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">生成Dockerfile</span></span><br><span class="line">cat &gt; Dockerfile &lt;&lt; EOF</span><br><span class="line"></span><br><span class="line">FROM $&#123;BASE_IMAGE&#125; </span><br><span class="line">RUN for dir in data bin log conf; do mkdir -p $&#123;APP_HOME&#125;/\$dir; done </span><br><span class="line">ADD source $&#123;APP_HOME&#125;/bin</span><br><span class="line">COPY run.sh /root/run.sh</span><br><span class="line">CMD [&quot;/bin/bash&quot;,&quot;/root/run.sh&quot;]</span><br><span class="line"></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">web项目镜像制作，包含nodejs和html</span></span><br><span class="line">web()</span><br><span class="line">&#123;</span><br><span class="line"><span class="meta">#</span><span class="bash">此处只做拷贝文件，启动方式按照<span class="variable">$BASE_IMAGE</span>的说明进行</span></span><br><span class="line">BASE_IMAGE=$&#123;REGISTRY&#125;/base/nginx:1.19</span><br><span class="line">APP_HOME=/usr/share/nginx/html</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">准备好将要复制到镜像中的服务文件，nodejs打包后的项目html全部放在dist目录下，在前面的pipline中已经将dist目录移动到了target目录中</span></span><br><span class="line">rm -rf source</span><br><span class="line">mkdir -p source</span><br><span class="line">mv ../$&#123;ci_dir&#125;/target/dist/* source/</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">生成dockerfile</span></span><br><span class="line">cat &gt; Dockerfile &lt;&lt; EOF</span><br><span class="line">FROM $&#123;BASE_IMAGE&#125;</span><br><span class="line">ADD source $&#123;APP_HOME&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if [[ x$6 == xJava || x$6 == xjava  ]];then</span><br><span class="line">java $1 $2 $3 $4 $5 $6 $7 $8</span><br><span class="line">else</span><br><span class="line">web $1 $2 $3 $4 $5 $6 $7 $8</span><br><span class="line">fi</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">####################main()###################</span></span></span><br><span class="line">cat Dockerfile</span><br><span class="line">container_name=$&#123;app_name&#125;</span><br><span class="line">time_char=`date &quot;+%y%m%d%H%M&quot;`</span><br><span class="line">container_version=$3-$&#123;time_char&#125;$4</span><br><span class="line"><span class="meta">#</span><span class="bash">私有仓库的认证</span></span><br><span class="line">docker login 192.168.137.5 -u admin -p huisebug &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">docker build -t $&#123;REGISTRY&#125;/$&#123;WAREHOUSE_NAME&#125;/$&#123;container_name&#125;:$&#123;container_version&#125; .</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">生成镜像文件<span class="comment">#推送镜像文件</span></span></span><br><span class="line">docker push  $&#123;REGISTRY&#125;/$&#123;WAREHOUSE_NAME&#125;/$&#123;container_name&#125;:$&#123;container_version&#125;</span><br><span class="line"></span><br><span class="line">if [[ x$7 == xgo ]];then</span><br><span class="line">echo &#x27;开始推送到生产镜像仓库&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">生产镜像仓库地址</span></span><br><span class="line">proimagerepo=192.168.137.5</span><br><span class="line"><span class="meta">#</span><span class="bash">生产镜像仓库认证</span></span><br><span class="line">docker login $&#123;proimagerepo&#125; -u admin -p huisebug &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">docker tag $&#123;REGISTRY&#125;/$&#123;WAREHOUSE_NAME&#125;/$&#123;container_name&#125;:$&#123;container_version&#125; $&#123;proimagerepo&#125;/$&#123;WAREHOUSE_NAME&#125;/$&#123;container_name&#125;:$&#123;container_version&#125;</span><br><span class="line">docker push $&#123;proimagerepo&#125;/$&#123;WAREHOUSE_NAME&#125;/$&#123;container_name&#125;:$&#123;container_version&#125;</span><br><span class="line">echo &#x27;清除多余的标签&#x27;</span><br><span class="line">docker rmi $&#123;proimagerepo&#125;/$&#123;WAREHOUSE_NAME&#125;/$&#123;container_name&#125;:$&#123;container_version&#125;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><ul><li><p>  java的镜像制作过程中会需要项目的pom.xml中定义构建后的jar包名称，比如此处我的项目jar名称就应该为jenkins-java.jar。</p></li><li><p>  java的docker镜像制作是可以后续在建立时使用docker的变量方式进行java传参和jar包传参；变量general_para是java参数传递，变量application_para是jar参数传递。</p></li><li><p>  nodejs编译后就是web页面，所以nodejs和web直接copy到nginx容器即可</p></li><li><p>  使用的基础docker镜像是存放在私有镜像仓库的，所以需要推送镜像到私有镜像仓库，参考java和nginx基础镜像Dockerfile：<a href="https://github.com/huisebug/jenk8s-install.git">https://github.com/huisebug/jenk8s-install.git</a></p></li></ul><h2 id="projects"><a href="#projects" class="headerlink" title="projects"></a>projects</h2><p>此处我准备的项目名称为：jenkins 所以建立了一个jenkins文件目录</p><p>jenkins文件目录下存放了project_info文件<br>project_info</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">jenkins#</span><span class="bash">java_repo=https://github.com/huisebug/jenk8s-java.git</span></span><br><span class="line"><span class="meta">jenkins#</span><span class="bash">nodejs_repo=https://github.com/huisebug/jenk8s-nodejs.git</span></span><br><span class="line"><span class="meta">jenkins#</span><span class="bash">html_repo=https://github.com/huisebug/jenk8s-html.git</span></span><br></pre></td></tr></table></figure><ul><li><p>  前面的jenkins代表项目名称，要求这个名称和文件目录名称相同</p></li><li><p>  java_repo/nodejs_repo/html_repo将会去掉repo取名作为服务名称</p></li><li><p>  git地址是对应的服务的git地址</p></li></ul><h2 id="调试项目介绍"><a href="#调试项目介绍" class="headerlink" title="调试项目介绍"></a>调试项目介绍</h2><p>此处我准备了三个不同类型的项目</p><p>java：<a href="https://github.com/huisebug/jenk8s-java.git">https://github.com/huisebug/jenk8s-java.git</a></p><p>nodejs：<a href="https://github.com/huisebug/jenk8s-nodejs.git">https://github.com/huisebug/jenk8s-nodejs.git</a></p><p>html：<a href="https://github.com/huisebug/jenk8s-html.git">https://github.com/huisebug/jenk8s-html.git</a></p><p>三个项目中都会有一个特殊的配置文件app.info</p><h3 id="项目中特殊的配置文件app-info"><a href="#项目中特殊的配置文件app-info" class="headerlink" title="项目中特殊的配置文件app.info"></a>项目中特殊的配置文件app.info</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">devlang:java </span><br><span class="line">version:1.1 </span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">devlang:html </span><br><span class="line">version:1.1</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">devlang:nodejs </span><br><span class="line">version:1.1</span><br></pre></td></tr></table></figure><ul><li><p>  需要给项目中的代码根目录建立一个app.info文件，便于流水线识别这是什么类型的项目而调用对应的方法进行打包和制作docker镜像</p></li><li><p>  devlang ：开发语言</p></li><li><p>  version ：版本号</p></li></ul><h1 id="Jenkins流水线Job验证"><a href="#Jenkins流水线Job验证" class="headerlink" title="Jenkins流水线Job验证"></a>Jenkins流水线Job验证</h1><h2 id="job名称格式"><a href="#job名称格式" class="headerlink" title="job名称格式"></a>job名称格式</h2><p><img src="/2020/06/03/jenkins-CICD-k8s/media/629a853f5ea978a15ce182f1ca228828.png"></p><p>注意job名称，以”+”号分割，分别是项目名称+服务名称+代码分支</p><h2 id="流水线配置"><a href="#流水线配置" class="headerlink" title="流水线配置"></a>流水线配置</h2><p><img src="/2020/06/03/jenkins-CICD-k8s/media/7351b532308f352de91be0b66d46ea4d.png"></p><p>使用git的方式拉取jenkinsfile（即build.groovy），轻量级检出的选项不勾选，勾选就会直接读取build.groovy的内容，不会clone整个项目代码，就无法获取到其他的项目代码</p><p>后续的项目我们只需要在建立job的时候复制然后建立就行了</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/62effa702063273bed79898bece3b9ee.png"></p><p>java项目执行结果</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/xiaoguo.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这是一个k8s集群结合jenkins做持续集成，以及jenkins流水线在k8s集群中运行。java编译后、nodejs编译后docker镜像制作。&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes" scheme="https://huisebug.github.io/categories/Kubernetes/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="jenkins" scheme="https://huisebug.github.io/tags/jenkins/"/>
    
    <category term="devops" scheme="https://huisebug.github.io/tags/devops/"/>
    
    <category term="pipline" scheme="https://huisebug.github.io/tags/pipline/"/>
    
    <category term="java" scheme="https://huisebug.github.io/tags/java/"/>
    
    <category term="nodejs" scheme="https://huisebug.github.io/tags/nodejs/"/>
    
    <category term="dockerfile" scheme="https://huisebug.github.io/tags/dockerfile/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes-hpav2横向自动扩容</title>
    <link href="https://huisebug.github.io/2019/08/28/k8s-hpa/"/>
    <id>https://huisebug.github.io/2019/08/28/k8s-hpa/</id>
    <published>2019-08-28T02:04:01.000Z</published>
    <updated>2021-07-07T08:50:30.212Z</updated>
    
    <content type="html"><![CDATA[<p>这是一个k8s集群验证hpav2的功能</p><span id="more"></span><h1 id="HPA"><a href="#HPA" class="headerlink" title="HPA"></a>HPA</h1><ul><li>  Horizo​​ntal Pod Autoscaler根据观察到的CPU利用率自动调整复制控制器，部署或副本集中的pod数量（或者，使用自定义度量标准支持，根据其他一些应用程序提供的度量标准）。请注意，Horizo​​ntal PodAutoscaling不适用于无法缩放的对象，例如DaemonSet。</li><li>  Horizo​​ntal Pod Autoscaler实现为Kubernetes API资源和控制器。资源确定控制器的行为。控制器会定期调整复制控制器或部署中的副本数，以使观察到的平均CPU利用率与用户指定的目标相匹配</li></ul><h2 id="Horizo​​ntal-Pod-Autoscaler如何工作？"><a href="#Horizo​​ntal-Pod-Autoscaler如何工作？" class="headerlink" title="Horizo​​ntal Pod Autoscaler如何工作？"></a>Horizo​​ntal Pod Autoscaler如何工作？</h2><p><img src="/2019/08/28/k8s-hpa/media/7f0680b00596a3a68743a5b0b041da00.png"></p><p>Horizo​​ntal Pod<br>Autoscaler实现为控制循环，其周期由控制器管理器的–horizontal-pod-autoscaler-sync-period标志控制（默认值为15秒）。<br>在每个期间，控制器管理器根据每个Horizo​​ntalPodAutoscaler定义中指定的度量查询资源利用率。控制器管理器从资源指标API（针对每个窗格资源指标）或自定义指标API（针对所有其他指标）获取指标。</p><ul><li>  对于每个pod资源指标（如CPU），控制器从Horizo​​ntalPodAutoscaler所针对的每个pod获取资源指标API中的指标。然后，如果设置了目标利用率值，则控制器将利用率值计算为每个容器中容器上的等效资源请求的百分比。如果设置了目标原始值，则直接使用原始度量标准值。然后，控制器在所有目标pod中获取利用率的平均值或原始值（取决于指定的目标类型），并产生用于缩放所需副本数量的比率。<br>请注意，如果某些pod的容器没有设置相关的资源请求，则不会定义pod的CPU利用率，并且autoscaler不会对该度量标准采取任何操作。有关自动调节算法如何工作的更多信息，请参阅下面的算法详细信息部分。</li><li>  对于每个pod自定义指标，控制器的功能与每个pod资源指标类似，不同之处在于它适用于原始值，而不是使用值。</li><li>  对于对象度量和外部度量，将获取单个度量，该度量描述相关对象。将该度量与目标值进行比较，以产生如上所述的比率。在autoscaling/v2beta2API版本中，可以选择在进行比较之前将此值除以pod的数量。<br>所述Horizo​​ntalPodAutoscaler通常由一系列的API聚集（的获取度量metrics.k8s.io，custom.metrics.k8s.io和external.metrics.k8s.io）。该metrics.k8s.ioAPI通常是通过度量服务器，其需要单独启动提供。有关说明，请参阅<br>metrics-server。Horizo​​ntalPodAutoscaler还可以直接从Heapster获取指标。</li></ul><p><font color="red" size="5">从Kubernetes 1.11开始，不推荐从Heapster获取指标。</font></p><h2 id="算法细节"><a href="#算法细节" class="headerlink" title="算法细节"></a>算法细节</h2><p>从最基本的角度来看，Horizo​​ntal Pod Autoscaler控制器根据所需度量值与当前度量值之间的比率进行操作：<br>desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]</p><ul><li>  例如，如果当前度量标准值是200m，并且期望值是100m，则副本的数量将加倍，因为200.0 / 100.0 == 2.0如果当前值是50m，则我们将副本的数量减半，因为50.0 / 100.0 == 0.5。如果比率足够接近1.0（在全局可配置的容差范围内，从–horizontal-pod-autoscaler-tolerance标志，默认为0.1），我们将跳过缩放。</li><li>  当指定a targetAverageValue或时targetAverageUtilization，currentMetricValue通过在Horizo​​ntalPodAutoscaler的比例目标中获取所有Pod的给定度量的平均值来计算。在检查容差和决定最终值之前，我们考虑了容量准备和缺失指标。</li><li>  设置了删除时间戳的所有Pod（即正在关闭的Pod）和所有失败的Pod都将被丢弃。</li><li>  如果特定的Pod缺少指标，则将其留待以后使用;具有缺失指标的窗格将用于调整最终缩放量。</li><li>  在CPU上进行扩展时，如果任何pod尚未准备好（即它仍在初始化），或者pod的最新度量标准点在它准备就绪之前，那么该pod也会被搁置。</li><li>  由于技术限制，在确定是否预留某些CPU指标时，Horizo​​ntalPodAutoscaler控制器无法准确确定容器第一次准备就绪。相反，它认为Pod尚未准备就绪，如果它尚未准备就绪，并且在它启动后的一个简短，可配置的时间窗口内转换为未准备好。此值使用–horizontal-pod-autoscaler-initial-readiness-delay标志配置，默认值为30秒。一旦pod准备就绪，它会认为任何转换都准备好成为第一个，如果它在启动后的较长的可配置时间内发生的话。此值使用–horizontal-pod-autoscaler-cpu-initialization-period标志配置，默认值为5分钟。</li><li>  所述currentMetricValue / desiredMetricValue然后碱比例是利用剩余的pod没有预留或从上方丢弃计算。</li><li>  如果有任何缺失的指标，我们会更加保守地重新计算平均值，假设这些容量在缩小的情况下消耗100％的期望值，并且在放大的情况下消耗0％。这可以抑制任何潜在规模的大小。</li><li>  此外，如果存在任何尚未准备好的播客，并且我们会扩大规模而不考虑丢失的指标或尚未准备好的播客，我们保守地假设尚未准备好的播客正在消耗所需指标的0％，进一步抑制了规模扩大的程度。</li><li>  考虑到尚未准备好的广告连播和缺少指标后，我们会重新计算使用率。如果新比率反转了比例方向，或者在公差范围内，我们会跳过缩放比例。否则，我们使用新的比例进行扩展。</li><li>  请注意，即使使用新的使用率，也会通过Horizo​​ntalPodAutoscaler状态报告平均利用率的原始值，而不考虑尚未准备好的容器或缺少指标。</li><li>  如果在Horizo​​ntalPodAutoscaler中指定了多个度量标准，则对每个度量标准进行此计算，然后选择所需的最大副本计数。如果任何这些度量标准无法转换为所需的副本计数（例如，由于从度量标准API获取度量标准时出错），则会跳过缩放。</li><li>  最后，在HPA扩展目标之前，记录比例建议。控制器会在可配置窗口中考虑所有建议，从该窗口中选择最高建议。可以使用–horizontal-pod-autoscaler-downscale-stabilization-window标志配置此值，默认为5分钟。这意味着缩放将逐渐发生，平滑快速波动的度量值的影响。</li></ul><p>使用Horizo​​ntal Pod自动缩放器管理一组副本的比例时，由于所评估的度量标准的动态特性，副本数量可能会不断波动。这有时被称为颠簸。<br>从v1.6开始，集群运营商可以通过调整作为kube-controller-manager组件标志公开的全局HPA设置来缓解此问题：<br>从v1.12开始，新的算法更新消除了对高级延迟的需求。<br>–horizontal-pod-autoscaler-downscale-delay：此选项的值是一个持续时间，指定自动缩放器必须等待多长时间才能在当前完成后执行另一个缩减操作。默认值为5分钟（5m0s）。<br>注意：调整这些参数值时，集群操作员应了解可能的后果。如果延迟（冷却）值设置得太长，可能会有人抱怨Horizo​​ntal Pod Autoscaler没有响应工作负载变化。但是，如果延迟值设置得太短，副本集的比例可能会像往常一样保持颠簸。</p><h2 id="支持的API"><a href="#支持的API" class="headerlink" title="支持的API"></a>支持的API</h2><p>默认情况下，Horizo​​ntalPodAutoscaler控制器从一系列API中检索指标。为了使其能够访问这些API，集群管理员必须确保：<br>1.该API汇聚层启用。<br>2.相应的API已注册：</p><ul><li>  对于资源指标，这是metrics.k8s.ioAPI，通常由metrics-server提供。它可以作为群集插件启动。</li><li>  对于自定义指标，这是custom.metrics.k8s.ioAPI。它由度量标准解决方案供应商提供的“适配器”API服务器提供。检查您的指标管道或已知解决方案列表。如果您想自己编写，请查看样板文件以开始使用。</li><li>  对于外部指标，这是external.metrics.k8s.ioAPI。它可能由上面提供的自定义指标适配器提供。</li></ul><p>3.–horizontal-pod-autoscaler-use-rest-clients=true已经在1.12+版本取消。将此设置为false会切换到基于Heapster的自动缩放，不推荐使用。</p><p>官方参考文档链接地址：<br><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/</a></p><h1 id="HPA-V1"><a href="#HPA-V1" class="headerlink" title="HPA V1"></a>HPA V1</h1><p>v2已经集成v1的功能，所以这里就不演示v1了。<br>最简易的v1，例如下面命令或者yaml示例</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=103</span><br><span class="line"></span><br><span class="line">或</span><br><span class="line"></span><br><span class="line">apiVersion: autoscaling/v1</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: php-apache</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: apps/v1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: php-apache</span><br><span class="line">  minReplicas: 1</span><br><span class="line">  maxReplicas: 10</span><br><span class="line">  targetCPUUtilizationPercentage: 50</span><br></pre></td></tr></table></figure><h1 id="HPA-V2"><a href="#HPA-V2" class="headerlink" title="HPA V2 "></a>HPA V2 </h1><p>下面是之前提到的版本问题HPA无法获取内存情况</p><p>参考地址：<br><a href="https://github.com/kubernetes/kubernetes/issues/74704">https://github.com/kubernetes/kubernetes/issues/74704</a></p><p>k8s从v1.7版本开始，对支持自定义指标的HPA架构进行了重新设计，引入了api server aggregation层、<strong>custom metrics server</strong>等组件来实现自定义业务指标的采集、保存和查询、再提供给HPA控制器进行扩缩容决策，称为HPA V2版本；</p><p>首先我们需要给kube-controller-manager服务增加三个参数：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--horizontal-pod-autoscaler-sync-period=30s \</span><br><span class="line">--horizontal-pod-autoscaler-downscale-delay=3m0s \</span><br><span class="line">--horizontal-pod-autoscaler-upscale-delay=3m0s</span><br></pre></td></tr></table></figure><p>分别是同步时间，缩容时间，扩容时间</p><p>参考修改后的kube-controller-manager服务service文件地址：<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/kube-controller-manager">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/kube-controller-manager</a></p><h2 id="概念解析"><a href="#概念解析" class="headerlink" title="概念解析"></a>概念解析</h2><ul><li>  cluster-autoscaler：kubernetes社区中负责节点水平伸缩的组件，目前处在GA阶段（General Availability，即正式发布的版本）。</li><li>  HPA：kubernetes社区中负责Pod水平伸缩的组件，是所有伸缩组件中历史最悠久的，目前支持autoscaling/v1、autoscaling/v2beta1与autoscaling/v2beta2，其中autoscaling/v1只支持CPU一种伸缩指标，在autoscaling/v2beta1中增加支持custommetrics，在autoscaling/v2beta2中增加支持external metrics。（获取命令kubectlapi-versions）</li><li>  cluster-proportional-autoscaler：根据集群的节点数目，水平调整Pod数目的组件，目前处在GA阶段。</li><li>  vetical-pod-autoscaler：根据Pod的资源利用率、历史数据、异常事件，来动态调整负载的Request值的组件，主要关注在有状态服务、单体应用的资源伸缩场景，目前处在beta阶段。</li><li>  addon-resizer：根据集群中节点的数目，纵向调整负载的Request的组件，目前处在beta阶段。</li></ul><h1 id="Resouce类型（CPU、内存）"><a href="#Resouce类型（CPU、内存）" class="headerlink" title="Resouce类型（CPU、内存）"></a>Resouce类型（CPU、内存）</h1><h2 id="运行原理简析"><a href="#运行原理简析" class="headerlink" title="运行原理简析"></a>运行原理简析</h2><h3 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h3><p>spec.template.spec.containers.resources.requests来为基数，</p><p>根据pod消耗的cpu数量相加求出平均值，然后除以基数，即可得到当前百分比（current），超出设置的目标百分比（target）就进行扩容，低于就缩容。</p><p><img src="/2019/08/28/k8s-hpa/media/e0767d3a988919bd6df3cf47df01c173.png"></p><p>内存根据pod消耗的内存值相加求出平均值就是当前值（current），与目标值（target）相比较，高于就扩容。</p><h3 id="缩容"><a href="#缩容" class="headerlink" title="缩容"></a>缩容</h3><p>如果是CPU的原因进行了扩容后，CPU消耗百分比低于目标百分比将会进行缩容，此时平均消耗内存除以目标值，如果接近于1就不进行完全的缩容。</p><h2 id="实践探究"><a href="#实践探究" class="headerlink" title="实践探究"></a>实践探究</h2><h3 id="部署mysql-rc服务"><a href="#部署mysql-rc服务" class="headerlink" title="部署mysql-rc服务"></a>部署mysql-rc服务</h3><p>yaml文件参考地址：<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/mysql-rc">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/mysql-rc</a></p><p>注意，使用hpav2的type: Resource，必须在pod中定义spec.template.spec.containers.resource，否则没有具体的基数。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ReplicationController</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql-rc</span><br><span class="line">  labels:</span><br><span class="line">    name: mysql-rc</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    name: mysql-pod</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels: </span><br><span class="line">        name: mysql-pod</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: mysql</span><br><span class="line">        image: mysql</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 3306</span><br><span class="line">        env:</span><br><span class="line">        - name: MYSQL_ROOT_PASSWORD</span><br><span class="line">          value: &quot;mysql&quot;</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 200m</span><br><span class="line">            memory: 500Mi</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 400m</span><br><span class="line">            memory: 1000Mi</span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: autoscaling/v2beta1</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql-rc</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: v1</span><br><span class="line">    kind: ReplicationController</span><br><span class="line">    name: mysql-rc</span><br><span class="line">  minReplicas: 1</span><br><span class="line">  maxReplicas: 3</span><br><span class="line">  metrics:</span><br><span class="line">  - type: Resource</span><br><span class="line">    resource:</span><br><span class="line">      name: cpu</span><br><span class="line">      targetAverageUtilization: 80</span><br><span class="line">  - type: Resource</span><br><span class="line">    resource:</span><br><span class="line">      name: memory</span><br><span class="line">      targetAverageValue: 1300Mi</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>| <strong>指示</strong>                                                                                                                                                                                                                                                                                                                                                                                                                 | <strong>描述</strong>                                                                                             |<br>| apiVersion: autoscaling/v2beta1                                                                                                                                                                                                                                                                                                                                                                                          | autoscaling正在使用的Kubernetes API组的版本。此示例清单使用beta版本，因此启用了按CPU和内存进行扩展。 |<br>| name: mysql-rc                                                                                                                                                                                                                                                                                                                                                                                                           | 表示HPA正在为mysql-rc部署执行自动扩展。                                                              |<br>| minReplicas: 1                                                                                                                                                                                                                                                                                                                                                                                                           | 表示运行的最小副本数不能低于1。                                                                      |<br>| maxReplicas: 3                                                                                                                                                                                                                                                                                                                                                                                                           | 表示部署中最大副本数不能超过3。                                                                      |<br>| targetAverageUtilization: 80                                                                                                                                                                                                                                                                                                                                                                                             | 表示当平均运行pod使用超过其请求CPU的80％时，部署将扩展pod。                                          |<br>| targetAverageValue: 1300Mi                                                                                                                                                                                                                                                                                                                                                                                               | 表示当平均运行pod使用超过1300Mi的内存时，部署将扩展pod。   </p><h3 id="验证效果"><a href="#验证效果" class="headerlink" title="验证效果"></a>验证效果</h3><h4 id="部署结果"><a href="#部署结果" class="headerlink" title="部署结果"></a>部署结果</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod -l name=mysql-pod</span><br></pre></td></tr></table></figure><p><img src="/2019/08/28/k8s-hpa/media/e84b6ee5dfdb84f117bd0030afcaeb5f.png"></p><h5 id="CPU扩容"><a href="#CPU扩容" class="headerlink" title="CPU扩容"></a>CPU扩容</h5><p>进入pod中的容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -it mysql-rc-9ngzn bash</span><br></pre></td></tr></table></figure><p>使CPU满载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i in `seq 1 $(cat /proc/cpuinfo |grep &quot;physical id&quot; |wc -l)`; do dd if=/dev/zero of=/dev/null &amp; done</span><br></pre></td></tr></table></figure><p><img src="/2019/08/28/k8s-hpa/media/f3cfd616d9da177d177ee2c9fc2ae2a3.png"></p><p>查看扩容情况，之前设定了扩容间隔时间（–horizontal-pod-autoscaler-upscale-delay=3m0s）为3m0s。所以我们需要等待大约3分钟<br><img src="/2019/08/28/k8s-hpa/media/6a37a76bf2228f35abf66c662b09611d.png"></p><h4 id="CPU缩容"><a href="#CPU缩容" class="headerlink" title="CPU缩容"></a>CPU缩容</h4><p>杀死容器进程1，即会重启容器，然后增压CPU的循环将会停止，以达到CPU的负载就会将下来，然后进行缩容。</p><h5 id="内存不影响的情况下"><a href="#内存不影响的情况下" class="headerlink" title="内存不影响的情况下"></a>内存不影响的情况下</h5><p><img src="/2019/08/28/k8s-hpa/media/7a6bbf5f1793439be51cd86d098d4c93.png"></p><h5 id="内存影响情况下"><a href="#内存影响情况下" class="headerlink" title="内存影响情况下"></a>内存影响情况下</h5><p>此处的mysql稳定运行消耗的内存为500Mi左右，为了让其缩容比例接近于1，以达到扩容后，就算CPU消耗降下来，因为内存的current/target接近于1，不会完全的缩容到最小副本数。</p><p>参考yaml文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete hpa mysql-rc</span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/hpav2/mysql-rc/mysql-rc-hpa-memory.yaml</span><br></pre></td></tr></table></figure><p>测试步骤和上面一样</p><p><img src="/2019/08/28/k8s-hpa/media/cbfc06e94c4af731f1f9f7207ba61625.png"><br><img src="/2019/08/28/k8s-hpa/media/89b5f8e71844f2be704b4a02db5cc63f.png"><br><img src="/2019/08/28/k8s-hpa/media/7f6102e660fd454b4c74346e6564f26c.png"><br><img src="/2019/08/28/k8s-hpa/media/361a0e5c157f9d9b7588e62b5183451c.png"></p><p>此时pod数量将会缩减为2，所以目标值的设定可以经过实际场景数据进行分析。</p><p><img src="/2019/08/28/k8s-hpa/media/4ac56712cd4cdd18505e63a4d45a776b.png"></p><h1 id="建立custom-metrics-k8s-io"><a href="#建立custom-metrics-k8s-io" class="headerlink" title="建立custom.metrics.k8s.io"></a>建立custom.metrics.k8s.io</h1><p>使用HPA v2的type为Resource时，使用的api是metrics.k8s.io，这个是由metrics-server提供的，使用type为Pods和Object时就需要使用的api是custom.metrics.k8s.io；</p><p>其中的Custom Metrics Server由prometheus-adapter服务实现，这个服务在之前我们安装prometheus-operator服务的时候已经安装。但是其安装的是api：metrics.k8s.io，并且其功能并不满足。所以在metrics-server1.12已经将api：metrics.k8s.io移除并使用metrics-server来提供支持。并且移除了prometheus-adapter服务。</p><p>github上有位大神的方法可以成功，参考地址如下，并且里面也有metrics-server的安装和验证</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/stefanprodan/k8s-prom-hpa</span><br></pre></td></tr></table></figure><p>需要修改镜像地址为可拉取的，可参考我修改了的</p><p><a href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/k8s-prom-hpa">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/k8s-prom-hpa</a></p><h2 id="部署Prometheus和Prometheus-adapter"><a href="#部署Prometheus和Prometheus-adapter" class="headerlink" title="部署Prometheus和Prometheus-adapter"></a>部署Prometheus和Prometheus-adapter</h2><p>创建monitoring命名空间：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f ./namespaces.yaml</span><br></pre></td></tr></table></figure><p>在monitoring命名空间中部署Prometheus v2 ：</p><p>如果要部署到GKE，可能会收到错误消息：<em>Error from server (Forbidden): error when<br>creating</em> 这将帮助您解决该问题：<a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/troubleshooting.md">GKE上的RBAC</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f ./prometheus</span><br></pre></td></tr></table></figure><p>生成Prometheus适配器所需的TLS证书：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make certs</span><br></pre></td></tr></table></figure><p>部署Prometheus自定义指标API适配器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f ./custom-metrics-api</span><br></pre></td></tr></table></figure><h2 id="验证并访问api"><a href="#验证并访问api" class="headerlink" title="验证并访问api"></a>验证并访问api</h2><p>查看pod是否成功建立</p><p><img src="/2019/08/28/k8s-hpa/media/e2613a17ecea2aff0b5b982038589066.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl api-versions | grep custom.metrics.k8s.io</span><br></pre></td></tr></table></figure><p><img src="/2019/08/28/k8s-hpa/media/b5822b1159f9adaf1733c956398a013a.png"></p><p>安装jQuery插件，并将请求值转换为json</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum install epel-release</span><br><span class="line">yum list jq</span><br><span class="line">yum -y install jq</span><br></pre></td></tr></table></figure><p>获取monitoring命名空间中所有pod的FS使用情况：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/monitoring/pods/*/fs_usage_bytes&quot; | jq .</span><br></pre></td></tr></table></figure><p><img src="/2019/08/28/k8s-hpa/media/7b630add30ef6652a3943add8fced092.png"></p><h1 id="Pods类型"><a href="#Pods类型" class="headerlink" title="Pods类型"></a>Pods类型</h1><h2 id="创建Pod"><a href="#创建Pod" class="headerlink" title="创建Pod"></a>创建Pod</h2><p>依照项目k8s-prom-hpa中的podinfo文件目录下来验证。</p><p>podinfo在default命名空间中创建NodePort服务和部署</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f ./podinfo/podinfo-svc.yaml ./podinfo/podinfo-dep.yaml</span><br></pre></td></tr></table></figure><p>该podinfo应用程序公开名为的自定义指标http_requests_total。Prometheus适配器删除_total后缀并将度量标记为计数器度量标准。</p><p>从自定义指标API获取每秒的总请求数：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests&quot; | jq .</span><br></pre></td></tr></table></figure><p><img src="/2019/08/28/k8s-hpa/media/534de9c48f8f9c01da152a780cdc9368.png"></p><p>注意：如果想让prometheus从pod抓取数据，需要在template声明annotations:中添加 prometheus.io/scrape: ‘true’</p><p><img src="/2019/08/28/k8s-hpa/media/a12b92db5f89feb0b1cbe01263e182bf.png"></p><h2 id="创建HPA"><a href="#创建HPA" class="headerlink" title="创建HPA"></a>创建HPA</h2><p>podinfo如果请求数超过每秒10个，将扩展部署</p><p>podinfo在default命名空间中部署HPA</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f ./podinfo/podinfo-hpa-custom.yaml</span><br></pre></td></tr></table></figure><p><img src="/2019/08/28/k8s-hpa/media/e27e33e324df09caaaf3748ccd82891c.png"></p><p>注意：不加m的时候，单位的默认是n<em>1000m，即上面是10</em>1000m=10000m</p><h2 id="安装-hey-压力测试工具"><a href="#安装-hey-压力测试工具" class="headerlink" title="安装 hey 压力测试工具"></a>安装 hey 压力测试工具</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run -dit -v /usr/local/bin:/go/bin golang:1.8 go get github.com/rakyll/hey</span><br><span class="line">export APP_ENDPOINT=$(kubectl get svc podinfo -o template --template &#123;&#123;.spec.clusterIP&#125;&#125;); echo $&#123;APP_ENDPOINT&#125;</span><br><span class="line">export APP_ENDPOINT_PORT=$(kubectl get svc podinfo -o yaml | grep port: | awk -F &#x27;: &#x27; &#x27;&#123;print $2&#125;&#x27;); </span><br><span class="line">echo $&#123;APP_ENDPOINT_PORT&#125;</span><br><span class="line">hey -n 10000 -q 5 -c 5</span><br></pre></td></tr></table></figure><p>执行请求测试后，可以看到pod数量变为了3</p><p><img src="/2019/08/28/k8s-hpa/media/721ddf000a950a306e7419edc8437ea4.png"></p><p>增大请求，pod数量变成4，停止后等待大约3分钟就可以缩容到最小副本数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hey -n 50000 -q 5 -c 5</span><br></pre></td></tr></table></figure><p><img src="/2019/08/28/k8s-hpa/media/77b90e208755d72445e6ac4a5f952679.png"><br><img src="/2019/08/28/k8s-hpa/media/f859ae2a4ddc72caba3a0fede4f0c88a.png"></p><h1 id="Object类型"><a href="#Object类型" class="headerlink" title="Object类型"></a>Object类型</h1><p>可支持的数据来源可以是service、endpoint等。</p><p>暂时无法实现，分析原因是需要在prometheus-adapter的配置文件中定义service，因为prometheus可以在service中声明annotations:中添加<br>prometheus.io/scrape: ‘true’</p><p>是可以获取到的metrics的，可是在api就无法访问到。</p><p>参考yaml：<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/sample-metrics">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/sample-metrics</a></p><h1 id="失败"><a href="#失败" class="headerlink" title="失败"></a>失败</h1><h2 id="部署prometheus"><a href="#部署prometheus" class="headerlink" title="部署prometheus"></a>部署prometheus</h2><p>参考我的values-hpav2.yaml文件地址：<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/prometheus">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/prometheus</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> helm delete prometheushpa --purge</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> helm install --name prometheushpa --namespace kube-system \</span></span><br><span class="line"><span class="bash">/root/charts/stable/prometheus \</span></span><br><span class="line"><span class="bash">-f /root/charts/stable/prometheus/values-hpav2.yaml</span></span><br></pre></td></tr></table></figure><p>部署的服务如下：</p><ul><li>  关闭告警插件alertmanager</li><li>  关闭节点收集器nodeexporter</li><li>  关闭网关pushgateway</li><li>  开启deployment收集器kubestatemetrics</li><li>  开启prometheus</li></ul><p>注意：我为方便与prometheus-operator安装prometheus区分，将服务命名为prometheushpa</p><h2 id="部署prometheus-adapter"><a href="#部署prometheus-adapter" class="headerlink" title="部署prometheus-adapter"></a>部署prometheus-adapter</h2><p>参考我的values-hpav2.yaml文件地址：<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/prometheus-adapter">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/prometheus-adapter</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> helm delete prometheus-adapter --purge</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> helm install --name prometheus-adapter --namespace kube-system \</span></span><br><span class="line"><span class="bash">/root/charts/stable/prometheus-adapter \</span></span><br><span class="line"><span class="bash">-f /root/charts/stable/prometheus-adapter/values-hpav2.yaml</span></span><br></pre></td></tr></table></figure><p>注意：prometheus-adapter去连接prometheushpa时，helm方式安装的prometheus的service port是80，不是9090；当然你可以修改为9090</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这是一个k8s集群验证hpav2的功能&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes" scheme="https://huisebug.github.io/categories/Kubernetes/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="hpav2" scheme="https://huisebug.github.io/tags/hpav2/"/>
    
    <category term="hpa" scheme="https://huisebug.github.io/tags/hpa/"/>
    
    <category term="custom.metrics" scheme="https://huisebug.github.io/tags/custom-metrics/"/>
    
    <category term="custom-metrics-api" scheme="https://huisebug.github.io/tags/custom-metrics-api/"/>
    
  </entry>
  
  <entry>
    <title>Prometheus-Operator监控k8s</title>
    <link href="https://huisebug.github.io/2019/08/27/Prometheus-Operator/"/>
    <id>https://huisebug.github.io/2019/08/27/Prometheus-Operator/</id>
    <published>2019-08-27T05:53:02.000Z</published>
    <updated>2021-07-09T02:57:20.670Z</updated>
    
    <content type="html"><![CDATA[<p>这是一篇Prometheus-Operator监控k8s服务的部署、监控原理讲解、监控配置、告警模板配置。</p><span id="more"></span><p>初始条件：</p><ul><li>  K8s1.12+集群</li></ul><h1 id="Prometheus-operator"><a href="#Prometheus-operator" class="headerlink" title="Prometheus-operator "></a>Prometheus-operator </h1><h2 id="安装monitoring-coreos-com-v1-api（prometheus-operator）"><a href="#安装monitoring-coreos-com-v1-api（prometheus-operator）" class="headerlink" title="安装monitoring.coreos.com/v1 api（prometheus-operator）"></a>安装monitoring.coreos.com/v1 api（prometheus-operator）</h2><p>此处我们直接使用yaml文件方式安装，不使用helm安装，helm安装会缺少一些服务，不方便我们更深了解</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/coreos/prometheus-operator.git</span><br></pre></td></tr></table></figure><p>已迁移到如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/coreos/kube-prometheus.git</span><br><span class="line">cd prometheus-operator/contrib/kube-prometheus</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/a75024b7d92536cdc10f54c8545a1c3b.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod --all-namespaces</span><br><span class="line">kubectl api-versions</span><br></pre></td></tr></table></figure><p>如果整个集群是否设置tain，解除即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i in api.huisebug.com node1.huisebug.com node2.huisebug.com; do kubectl taint nodes $i node-role.kubernetes.io/master:NoSchedule-; done</span><br></pre></td></tr></table></figure><h2 id="集群状态"><a href="#集群状态" class="headerlink" title="集群状态"></a>集群状态</h2><p>查看是否建立并启动好所有容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod --all-namespaces -o wide 或者 kubectl get pod -n monitoring</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/7fa22fa29b2998022851543d251dae94.png"></p><h2 id="问题汇总"><a href="#问题汇总" class="headerlink" title="问题汇总"></a>问题汇总</h2><ul><li>  <strong>Prometheus can’t access node-exporter and kube-state-metrics</strong></li></ul><p>解决地址</p><p><a href="https://github.com/coreos/prometheus-operator/issues/2330">https://github.com/coreos/prometheus-operator/issues/2330</a></p><ul><li>  alertmanager无法建立pod，主要是服务器环境硬件跟不上</li></ul><p>解决地址</p><p><a href="https://github.com/coreos/prometheus-operator/issues/1902">https://github.com/coreos/prometheus-operator/issues/1902</a></p><p><a href="https://github.com/coreos/prometheus-operator/issues/965">https://github.com/coreos/prometheus-operator/issues/965</a></p><ul><li>  prometheus的storage.local.retention（数据存储时间）设置更长时间</li></ul><p>解决地址</p><p><a href="https://github.com/coreos/prometheus-operator/issues/732">https://github.com/coreos/prometheus-operator/issues/732</a></p><p>只需在prometheus-prometheus.yaml文件中增加一行配置</p><p><img src="/2019/08/27/Prometheus-Operator/media/2fb7331e54c312463c5817ed4293e280.png"></p><p>主要介绍一下alertmanager无法建立pod的问题的解决方法，默认的alertmanager重启嗅探如下：</p><p><img src="/2019/08/27/Prometheus-Operator/media/5e37cd97ee9a0ad417dc44df9fbb2f64.png"></p><p>即嗅探10*10=100秒以后就会重启，显然模拟环境是无法在100秒内成功启动alertmanager，所以这里我们需要给配置文件alertmanager-alertmanager.yaml添加paused:true参数，添加步骤如下：</p><ol><li> 首先已经建立所有的prometheus-operator服务（kubectl apply -f manifests/ ）</li><li> 然后给alertmanager-alertmanager.yaml添加paused: true参数; paused: true参数解释参考地址：<a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerspec">https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerspec</a><br><img src="/2019/08/27/Prometheus-Operator/media/629d37574ccd3a5996ba69138c53299e.png"></li><li> 重载配置文件alertmanager-alertmanager.yaml （kubectl apply -f alertmanager-alertmanager.yaml）</li><li> 将配置文件alertmanager-alertmanager.yaml建立后生成的statefulset转储到文件中（kubectl get statefulsets alertmanager-main -n monitoring -o yaml &gt; alertmanager-main-statefulsets.yaml）</li><li> 在集群中删除alertmanager（kubectl delete -f alertmanager-main-statefulsets.yaml）</li><li> 修改文件alertmanager-main-statefulsets.yaml的嗅探失败次数，修改为30次，即30*10=300秒，也可根据你的实际环境来调节<br><img src="/2019/08/27/Prometheus-Operator/media/a5f43f5ed4d781c892b25e579d5eea8f.png"></li><li> 再次创建，即可成功建立alertmanager</li></ol><h1 id="解析prometheus-operator原理"><a href="#解析prometheus-operator原理" class="headerlink" title="解析prometheus-operator原理"></a>解析prometheus-operator原理</h1><p>新加了两个apiversion，获取命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl api-versions</span><br></pre></td></tr></table></figure><p>metrics.k8s.io/v1beta1<br>monitoring.coreos.com/v1</p><ul><li>  <strong>monitoring.coreos.com/v1是作为四个资源对象的组而添加到api中</strong></li><li>  <strong>metrics.k8s.io/v1beta1是使用资源对象APIService进行建立的，这个apiversion需关联到service对应的pod prometheus-adapter建立成功才会成功建立，所以记住需要使用kubectl api-versions命令关注是否建立成功</strong></li></ul><p>使用自定义资源定义CustomResourceDefinitions（CRD）新增4个kind：</p><ol><li> Prometheus，它定义了所需的Prometheus部署。运营商始终确保正在运行与资源定义匹配的部署。</li><li> ServiceMonitor，以声明方式指定应如何监控服务组。操作员根据定义自动生成Prometheus刮削配置。</li><li> PrometheusRule，它定义了一个所需的Prometheus规则文件，该文件可以由包含Prometheus警报和记录规则的Prometheus实例加载。</li><li> Alertmanager，它定义了所需的Alertmanager部署。运营商始终确保正在运行与资源定义匹配的部署。</li></ol><p><strong>获取命令如下</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get Servicemonitor --all-namespaces</span><br><span class="line">kubectl get Prometheus --all-namespaces</span><br></pre></td></tr></table></figure><h2 id="yaml类型"><a href="#yaml类型" class="headerlink" title="yaml类型"></a>yaml类型</h2><ol><li> 0prometheus-operator*：建立整个prometheus-operator的CRD、提供服务支持的pod运行</li><li> alertmanager*：prometheus所需的告警处理，alertmanager告警的pod建立</li><li> grafana*：监控页面展示</li><li> kube-state-metrics*：增加对deployment建立的pod、statefulset建立的pod等资源对象的metrics数据，参考地址：<a href="https://github.com/kubernetes/kube-state-metrics">https://github.com/kubernetes/kube-state-metrics</a></li><li> node-exporter*：运行k8s集群的node的服务器数据抓取</li><li> prometheus-adapter：为<strong>metrics.k8s.io/v1beta1</strong>提供metrics数据支持，获取集群的node和pod的CPU、内存使用情况，但是在这里只能获取到pod的，无法获取node的，后续将会在<strong>HPA V2</strong>章节讲解如何解决。</li><li> prometheus*：prometheus监控系统的pod建立，为需要添加到监控列表的服务增加metrics</li></ol><h2 id="简易原理"><a href="#简易原理" class="headerlink" title="简易原理"></a>简易原理</h2><p>数据源是从prometheus获取的，那么prometheus是如何获取k8s中这些数据的呢？</p><ol><li> 资源类型ServiceMonitor来定义添加的要被监控的k8s的服务service，然后使用operator工具来定义一个资源类型Prometheus进行筛选ServiceMonitor进行服务监控。PrometheusRule和Alertmanager进行规制和告警设置。</li></ol><h2 id="资源类型Prometheus"><a href="#资源类型Prometheus" class="headerlink" title="资源类型Prometheus"></a>资源类型Prometheus</h2><p>这里我们来深度解析下这个资源类型为Prometheus</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get Prometheus --all-namespaces -o yaml &gt; k8s-prometheus.yaml</span><br></pre></td></tr></table></figure><p>去除一些不必要的信息后，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">items:</span><br><span class="line">- apiVersion: monitoring.coreos.com/v1</span><br><span class="line">  kind: Prometheus</span><br><span class="line">  metadata:</span><br><span class="line">    labels:</span><br><span class="line">      prometheus: k8s</span><br><span class="line">    name: k8s</span><br><span class="line">    namespace: monitoring</span><br><span class="line">  spec:</span><br><span class="line">    alerting:</span><br><span class="line">      alertmanagers:</span><br><span class="line">      - name: alertmanager-main</span><br><span class="line">        namespace: monitoring</span><br><span class="line">        port: web</span><br><span class="line">    baseImage: quay.io/prometheus/prometheus</span><br><span class="line">    nodeSelector:</span><br><span class="line">      beta.kubernetes.io/os: linux</span><br><span class="line">    replicas: 2</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: 400Mi</span><br><span class="line">    ruleSelector:</span><br><span class="line">      matchLabels:</span><br><span class="line">        prometheus: k8s</span><br><span class="line">        role: alert-rules</span><br><span class="line">    securityContext:</span><br><span class="line">      fsGroup: 2000</span><br><span class="line">      runAsNonRoot: true</span><br><span class="line">      runAsUser: 1000</span><br><span class="line">    serviceAccountName: prometheus-k8s</span><br><span class="line">    serviceMonitorNamespaceSelector: &#123;&#125;</span><br><span class="line">    serviceMonitorSelector: &#123;&#125;</span><br><span class="line">    version: v2.5.0</span><br><span class="line">kind: List</span><br><span class="line">metadata:</span><br><span class="line">  resourceVersion: &quot;&quot;</span><br><span class="line">  selfLink: &quot;&quot;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">kind: List</span><br><span class="line">metadata:</span><br><span class="line">  resourceVersion: &quot;&quot;</span><br><span class="line">  selfLink: &quot;&quot;</span><br><span class="line">这一段是定义多个，不必在意。</span><br><span class="line"></span><br><span class="line">- apiVersion: monitoring.coreos.com/v1</span><br><span class="line">  kind: Prometheus</span><br><span class="line">  metadata:</span><br><span class="line">    labels:</span><br><span class="line">      prometheus: k8s</span><br><span class="line">    name: k8s</span><br><span class="line">    namespace: monitoring</span><br><span class="line">这是基础的头定义，定义了api类型，资源对象类型，标签，名称，命名空间。</span><br><span class="line"></span><br><span class="line">alerting:</span><br><span class="line">      alertmanagers:</span><br><span class="line">      - name: alertmanager-main</span><br><span class="line">        namespace: monitoring</span><br><span class="line">        port: web</span><br><span class="line">这是定义告警方式的alertmanager的k8s服务service名称（kubectl get svc –all-namespaces）获取查看，web是定义service时spec.ports下的一个名称name（kubectl get svc alertmanager-main -n monitoring -o yaml）获取查看。</span><br><span class="line"></span><br><span class="line">baseImage: quay.io/prometheus/prometheus</span><br><span class="line">    nodeSelector:</span><br><span class="line">      beta.kubernetes.io/os: linux</span><br><span class="line">    replicas: 2</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: 400Mi</span><br><span class="line">设置prometheus建立pod时的参数，基础镜像，节点选择，副本因子数，资源配额。</span><br><span class="line"></span><br><span class="line">ruleSelector:</span><br><span class="line">      matchLabels:</span><br><span class="line">        prometheus: k8s</span><br><span class="line">        role: alert-rules</span><br><span class="line">规则选择器，依照标签来选择规则列表，获取规则列表可以（kubectl get prometheusrule prometheus-k8s-rules -n monitoring），prometheus-operator添加api时候定义了一个新的资源类型PrometheusRule，如何查看添加哪些新的kind呢，我是依照安装prometheus-operator时候会建立资源对象推断出的</span><br><span class="line"></span><br><span class="line">    securityContext:</span><br><span class="line">      fsGroup: 2000</span><br><span class="line">      runAsNonRoot: true</span><br><span class="line">      runAsUser: 1000</span><br><span class="line">安全上下文配置，这里我猜测应该是基于容器container的安全上下文配置。具体参考官方</span><br><span class="line">https://www.kubernetes.org.cn/security-context-psp</span><br><span class="line"></span><br><span class="line">serviceAccountName: prometheus-k8s</span><br><span class="line">RBAC认证账户</span><br><span class="line"></span><br><span class="line">serviceMonitorNamespaceSelector: &#123;&#125;</span><br><span class="line">服务监控（ServiceMonitor）命名空间选择，这里是匹配所有；</span><br><span class="line"></span><br><span class="line">serviceMonitorSelector: &#123;&#125;</span><br><span class="line">对（ServiceMonitor）进行筛选，这里也是匹配所有</span><br><span class="line"></span><br><span class="line">version: v2.5.0</span><br><span class="line">镜像的版本号，即prometheus的版本号</span><br><span class="line"></span><br><span class="line">建立的pod是使用statefulsets部署的</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="资源类型PrometheusRule"><a href="#资源类型PrometheusRule" class="headerlink" title="资源类型PrometheusRule"></a>资源类型PrometheusRule</h2><p>专门用于prometheus的rules配置,执行以下命令来参照修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get prometheusrule prometheus-k8s-rules -n monitoring -o yaml</span><br></pre></td></tr></table></figure><p>后续将会在企业微信告警配置中讲解</p><h2 id="资源类型ServiceMonitor"><a href="#资源类型ServiceMonitor" class="headerlink" title="资源类型ServiceMonitor"></a>资源类型ServiceMonitor</h2><p>kubectl get Servicemonitor –all-namespaces<br>以下举例三个不同namespace下的service进行说明</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">kubectl get Servicemonitor kube-apiserver -n monitoring -o yaml &gt; kube-apiserver-servicemonitor.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: monitoring.coreos.com/v1</span><br><span class="line">kind: ServiceMonitor</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: apiserver</span><br><span class="line">  name: kube-apiserver</span><br><span class="line">  namespace: monitoring</span><br><span class="line">spec:</span><br><span class="line">  endpoints:</span><br><span class="line">  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token</span><br><span class="line">    interval: 30s</span><br><span class="line">    metricRelabelings:</span><br><span class="line">    - action: drop</span><br><span class="line">      regex: etcd_(debugging|disk|request|server).*</span><br><span class="line">      sourceLabels:</span><br><span class="line">      - __name__</span><br><span class="line">    port: https</span><br><span class="line">    scheme: https</span><br><span class="line">    tlsConfig:</span><br><span class="line">      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt</span><br><span class="line">      serverName: kubernetes</span><br><span class="line">  jobLabel: component</span><br><span class="line">  namespaceSelector:</span><br><span class="line">    matchNames:</span><br><span class="line">    - default</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      component: apiserver</span><br><span class="line">      provider: kubernetes</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">kubectl get Servicemonitor node-exporter -n monitoring -o yaml &gt; node-exporter-servicemonitor.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: monitoring.coreos.com/v1</span><br><span class="line">kind: ServiceMonitor</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: node-exporter</span><br><span class="line">  name: node-exporter</span><br><span class="line">  namespace: monitoring</span><br><span class="line">spec:</span><br><span class="line">  endpoints:</span><br><span class="line">  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token</span><br><span class="line">    interval: 30s</span><br><span class="line">    port: https</span><br><span class="line">    scheme: https</span><br><span class="line">    tlsConfig:</span><br><span class="line">      insecureSkipVerify: true</span><br><span class="line">  jobLabel: k8s-app</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: node-exporter</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">kubectl get Servicemonitor coredns -n monitoring -o yaml &gt; coredns-servicemonitor.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: monitoring.coreos.com/v1</span><br><span class="line">kind: ServiceMonitor</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: coredns</span><br><span class="line">  name: coredns</span><br><span class="line">  namespace: monitoring</span><br><span class="line">spec:</span><br><span class="line">  endpoints:</span><br><span class="line">  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token</span><br><span class="line">    interval: 15s</span><br><span class="line">    port: metrics</span><br><span class="line">  namespaceSelector:</span><br><span class="line">    matchNames:</span><br><span class="line">    - kube-system</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: kube-dns</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">endpoints:   #端点配置</span><br><span class="line">  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token  #serviceaccount自动建立的token</span><br><span class="line">    interval: 30s       #间隔时间</span><br><span class="line">    metricRelabelings:    </span><br><span class="line">    - action: drop</span><br><span class="line">      regex: etcd_(debugging|disk|request|server).*</span><br><span class="line">      sourceLabels:</span><br><span class="line">      - __name__</span><br><span class="line">    port: https        #端点配置中的port定义名称，使用kubectl get ep ep名称 -o yaml 可以查看到 </span><br><span class="line">scheme: https       #访问metrics的方式，一般是http和https，但是源代码是没有固定参数，可以任意传字符串，所以这里还是写http或者https</span><br><span class="line">tlsConfig: #在抓取端点时使用的TLS配置</span><br><span class="line"></span><br><span class="line">namespaceSelector:        #命名空间选择器</span><br><span class="line">    matchNames:     #当监控的pod与servicemonitor不在同一个命名空间就需要进行筛选，使用any： true匹配所有命名空间，不然就写matchNames来匹配</span><br><span class="line">    - default</span><br><span class="line"></span><br><span class="line">selector:        #service标签选择器，kubectl get svc kubernetes -o yaml进行查看，注意svc必须在metadata.label下声明才能进行匹配。</span><br><span class="line">    matchLabels:</span><br><span class="line">      component: apiserver</span><br><span class="line">      provider: kubernetes</span><br></pre></td></tr></table></figure><p>官方解析地址如下：<br><a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md">https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md</a></p><h2 id="资源对象Alertmanagers"><a href="#资源对象Alertmanagers" class="headerlink" title="资源对象Alertmanagers"></a>资源对象Alertmanagers</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">kubectl get alertmanagers main -n monitoring -o yaml &gt; main-alertmanagers.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">items:</span><br><span class="line">- apiVersion: monitoring.coreos.com/v1</span><br><span class="line">  kind: Alertmanager</span><br><span class="line">  metadata:</span><br><span class="line">    labels:</span><br><span class="line">      alertmanager: main</span><br><span class="line">    name: main</span><br><span class="line">    namespace: monitoring</span><br><span class="line">  spec:</span><br><span class="line">    baseImage: quay.io/prometheus/alertmanager</span><br><span class="line">    nodeSelector:</span><br><span class="line">      beta.kubernetes.io/os: linux</span><br><span class="line">    replicas: 3</span><br><span class="line">    securityContext:</span><br><span class="line">      fsGroup: 2000</span><br><span class="line">      runAsNonRoot: true</span><br><span class="line">      runAsUser: 1000</span><br><span class="line">    serviceAccountName: alertmanager-main</span><br><span class="line">    version: v0.15.3</span><br><span class="line">kind: List</span><br><span class="line">metadata:</span><br><span class="line">  resourceVersion: &quot;&quot;</span><br><span class="line">  selfLink: &quot;&quot;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">- apiVersion: monitoring.coreos.com/v1</span><br><span class="line">  kind: Alertmanager</span><br><span class="line">  metadata:</span><br><span class="line">    labels:</span><br><span class="line">      alertmanager: main</span><br><span class="line">    name: main</span><br><span class="line">    namespace: monitoring</span><br><span class="line">这是基础的头定义，定义了api类型，资源对象类型，标签，名称，命名空间。</span><br><span class="line">spec:</span><br><span class="line">baseImage: quay.io/prometheus/alertmanager</span><br><span class="line">    nodeSelector:</span><br><span class="line">      beta.kubernetes.io/os: linux</span><br><span class="line">    replicas: 3</span><br><span class="line">    securityContext:</span><br><span class="line">      fsGroup: 2000</span><br><span class="line">      runAsNonRoot: true</span><br><span class="line">      runAsUser: 1000</span><br><span class="line">        </span><br><span class="line">设置prometheus建立pod时的参数，基础镜像，节点选择，副本因子数，安全上下文。</span><br><span class="line">安全上下文配置，这里我猜测应该是基于容器container的安全上下文配置。具体参考官方https://www.kubernetes.org.cn/security-context-psp</span><br><span class="line"></span><br><span class="line">serviceAccountName: alertmanager-main</span><br><span class="line">RBAC认证账户</span><br><span class="line"></span><br><span class="line">version: v0.15.3</span><br><span class="line">镜像的版本号，即alertmanager的版本号</span><br><span class="line"></span><br><span class="line">建立的pod是使用statefulsets部署的，会建立一个无头服务service：</span><br><span class="line">serviceName: alertmanager-operated</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>使用命令kubectl get statefulsets alertmanager-main -n monitoring -o yaml可以查看到；</strong><br><img src="/2019/08/27/Prometheus-Operator/media/d169d095774698249244fe235ae40837.png"></p><h1 id="验证效果"><a href="#验证效果" class="headerlink" title="验证效果"></a>验证效果</h1><p>查看建立的service，我们需要访问prometheus数据源和grafana的监控展示web-UI界面，如何访问呢，可以使用service的Nodeport方式，显然这是非常lose的，这里我使用traefik代理方式（traefik安装就不赘述了），建立ingress</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">第一步，查看service</span><br><span class="line">kubectl get svc --all-namespaces</span><br><span class="line"></span><br><span class="line">第二步，查看ingress</span><br><span class="line">kubectl get ingress --all-namespaces</span><br><span class="line"></span><br><span class="line">新建一个ingress在命名空间monitoring下的</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus-k8s-grafana</span><br><span class="line">  namespace: monitoring</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">    - host: prometheus.huisebug.com</span><br><span class="line">      http:</span><br><span class="line">        paths:</span><br><span class="line">          - backend:</span><br><span class="line">              serviceName: prometheus-k8s</span><br><span class="line">              servicePort: 9090</span><br><span class="line">            path: /</span><br><span class="line">    - host: grafana.huisebug.com</span><br><span class="line">      http:</span><br><span class="line">        paths:</span><br><span class="line">          - backend:</span><br><span class="line">              serviceName: grafana</span><br><span class="line">              servicePort: 3000</span><br><span class="line">            path: /</span><br></pre></td></tr></table></figure><p>给客户机的hosts添加本地解析记录<br>192.168.137.13 prometheus.huisebug.com grafana.huisebug.com</p><h2 id="grafana-huisebug-com"><a href="#grafana-huisebug-com" class="headerlink" title="grafana.huisebug.com"></a>grafana.huisebug.com</h2><p>grafana.huisebug.com需要密码验证用户名admin 密码admin并提示修改初始密码</p><p>grafana已经添加了k8s集群的监控</p><p><img src="/2019/08/27/Prometheus-Operator/media/c94d7f6a843e9df190914a2abca78c2e.png"></p><h2 id="prometheus-huisebug-com"><a href="#prometheus-huisebug-com" class="headerlink" title="prometheus.huisebug.com"></a>prometheus.huisebug.com</h2><p><img src="/2019/08/27/Prometheus-Operator/media/650ae2d1f247399562f4fb6ce90f9515.png"></p><p>从上面可以看到没有获取到kube-controller-manager、kube-scheduler的metric，接下来就解决这个问题，因为这2个服务不是使用kubeadm方式部署的是使用二进制部署的，所以无法获取到。使用kubeadm方式部署的服务都是存在于namespace:kube-system中，例如：</p><p><img src="/2019/08/27/Prometheus-Operator/media/d37b4268d6fb43051831ca9581da35bd.png"></p><h1 id="metrics"><a href="#metrics" class="headerlink" title="metrics"></a>metrics</h1><h2 id="解决scheduler和controller-manager的metric"><a href="#解决scheduler和controller-manager的metric" class="headerlink" title="解决scheduler和controller-manager的metric"></a>解决scheduler和controller-manager的metric</h2><p>想要解决上述问题，先来查看整个集群的service和endpoint<br><img src="/2019/08/27/Prometheus-Operator/media/9e6546ab25a4f4d1cce61a3413baa3db.png"><br>可以看到：</p><ul><li>  service中不存在kube-controller-manager、kube-scheduler</li><li>  endpoint中都有</li></ul><p>所以我们需要建立对应的service和修改endpoint的参数</p><p>注意新建立service的port名称要与serviceMonitor配置文件对应的上<br><img src="/2019/08/27/Prometheus-Operator/media/b3309bb846a64600ecc3293dfeaba2ab.png"></p><p>参考地址,记得修改其中的地址为你的服务器node_ip地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Prometheus-operator/metrics/scheduler_controller-manager/metics_services.yaml</span><br></pre></td></tr></table></figure><p>如果建立后还是无法访问记得使用命令netstat -lntp查看端口是否设置是所有网卡可以访问，如果是127.0.0.1，就修改服务的启动参数，设置为0.0.0.0</p><p>上述修改完毕后，就可以再次查看是否成功获取到<br><img src="/2019/08/27/Prometheus-Operator/media/d79e172a8e29f7deddbe41ea5223e99f.png"></p><h2 id="mysql-metrics"><a href="#mysql-metrics" class="headerlink" title="mysql-metrics"></a>mysql-metrics</h2><p>此处我们建立一个mysql服务的metric验证</p><h3 id="首先建立一个mysql服务"><a href="#首先建立一个mysql服务" class="headerlink" title="首先建立一个mysql服务"></a>首先建立一个mysql服务</h3><p>参考地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Prometheus-operator/metrics/mysql-metrics/mysql-metric.yaml</span><br></pre></td></tr></table></figure><p>测试是否成功建立mysql服务命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it mysql sh -c &#x27;exec mysql -h192.168.137.13 -P32306 -pmysql -uroot&#x27;</span><br></pre></td></tr></table></figure><p>如果能登录，说明mysql服务已经建立<br><img src="/2019/08/27/Prometheus-Operator/media/79bbd7c67dd266a4e97e04147ac8aa07.png"></p><h3 id="使用helm方式安装mysql-exporter"><a href="#使用helm方式安装mysql-exporter" class="headerlink" title="使用helm方式安装mysql-exporter"></a>使用helm方式安装mysql-exporter</h3><p>找到到之前安装traefik的官方charts目录；<br>进入到stable/prometheus-mysql-exporter目录下就是mysql-exporter的chart了；<br>values.yaml文件中要指定mysql服务的用户和密码。然后再安装<br><img src="/2019/08/27/Prometheus-Operator/media/ffd56c5b5235fb6b8e27c715f6d36dd5.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm install -f values.yaml --name mysql-exporter .</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/99ce34a2797daee5e18254382881bf5d.png"></p><h3 id="访问mysql-exporter是否有metric生成"><a href="#访问mysql-exporter是否有metric生成" class="headerlink" title="访问mysql-exporter是否有metric生成"></a>访问mysql-exporter是否有metric生成</h3><p>这里同样采用traefik代理来访问，建立ingress，上述两个pod都是建立在default命名空间的。</p><p>ingress yaml文件参考地址：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/Prometheus-operator/metrics/mysql-metrics/mysql-metric.yaml</span><br></pre></td></tr></table></figure><p>本地解析添加<br>192.168.137.13 mysqlexporter.huisebug.com</p><p>验证访问<a href="http://mysqlexporter.huisebug.com/metrics">http://mysqlexporter.huisebug.com/metrics</a><br><img src="/2019/08/27/Prometheus-Operator/media/1e0514c33f9c540cdfc6880f42a3d971.png"></p><h3 id="ServiceMonitor资源对象建立"><a href="#ServiceMonitor资源对象建立" class="headerlink" title="ServiceMonitor资源对象建立"></a>ServiceMonitor资源对象建立</h3><p>查看service的对应标签，用于servicemonitor</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc mysql-exporter-prometheus-mysql-exporter -o yaml| grep -1 labels</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/20f49c74e35290151302f7657fac6d16.png"></p><p>参考yaml地址<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/Prometheus-operator/metrics/mysql-metrics/mysql-metric.yaml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/Prometheus-operator/metrics/mysql-metrics/mysql-metric.yaml</a></p><p><font color="red" size="6">记住servicemonitor必须建立在monitor命名空间下</font></p><h3 id="Prometheus资源对象建立"><a href="#Prometheus资源对象建立" class="headerlink" title="Prometheus资源对象建立"></a>Prometheus资源对象建立</h3><p>promethes资源对象在安装<strong>monitoring.coreos.com/v1</strong> api的时候建立了一个匹配所有servicemonitot的prometheus资源对象，所以这里我就不建立了，可以参考以下地址：<br><a href="https://github.com/coreos/prometheus-operator/blob/master/contrib/kube-prometheus/manifests/prometheus-prometheus.yaml">https://github.com/coreos/prometheus-operator/blob/master/contrib/kube-prometheus/manifests/prometheus-prometheus.yaml</a></p><h3 id="验证效果-1"><a href="#验证效果-1" class="headerlink" title="验证效果"></a>验证效果</h3><p>再次访问prometheus的target页面查看是否成功获取</p><p><img src="/2019/08/27/Prometheus-Operator/media/0ce7ae823851da215e533b770df42b93.png"></p><p>成功建立mysql-metric</p><h2 id="traefik-metrics"><a href="#traefik-metrics" class="headerlink" title="traefik-metrics"></a>traefik-metrics</h2><h3 id="metrics建立"><a href="#metrics建立" class="headerlink" title="metrics建立"></a>metrics建立</h3><p>之前建立traefik的时候开启dashboard.enabled=true,dashboard.domain=traefik.huisebug.com,metrics.prometheus.enabled=true<br>这样我们就可以访问到traefik的metrics</p><p>访问地址：<a href="http://traefik.huisebug.com/metrics">http://traefik.huisebug.com/metrics</a></p><p><img src="/2019/08/27/Prometheus-Operator/media/45a04fd447062832133a6db5fa9f7135.png"></p><h3 id="ServiceMonitor资源对象建立-1"><a href="#ServiceMonitor资源对象建立-1" class="headerlink" title="ServiceMonitor资源对象建立"></a>ServiceMonitor资源对象建立</h3><p>参考yaml文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Prometheus-operator/metrics/traefik-metrics/traefik-metrics.yaml</span><br></pre></td></tr></table></figure><h3 id="验证效果-2"><a href="#验证效果-2" class="headerlink" title="验证效果"></a>验证效果</h3><p><img src="/2019/08/27/Prometheus-Operator/media/6412727c854d8dec8afa4048d4372318.png"></p><h1 id="alertmanager告警"><a href="#alertmanager告警" class="headerlink" title="alertmanager告警"></a>alertmanager告警</h1><p>alertmanager也是提供了web访问控制台的，先使用ingress进行访问</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Prometheus-operator/alertmanager/alertmanager-ingress.yaml</span><br></pre></td></tr></table></figure><p>记得本地添加解析记录</p><p>访问效果</p><p><img src="/2019/08/27/Prometheus-Operator/media/83a70cb03cfb322a21960c1fe68311b4.png"></p><p>可以看到告警信息这么多，无法找到接收者，所以堆积了。</p><p>查看alertmanager的配置参数</p><p><img src="/2019/08/27/Prometheus-Operator/media/e4b8bd6bf0c5a44e1066f84b7e374a75.png"></p><p>下面演示如何用钉钉群组来接收告警信息</p><h2 id="钉钉webhook报警2-0版本8060"><a href="#钉钉webhook报警2-0版本8060" class="headerlink" title="钉钉webhook报警2.0版本8060"></a>钉钉webhook报警2.0版本8060</h2><h3 id="安装钉钉插件"><a href="#安装钉钉插件" class="headerlink" title="安装钉钉插件"></a>安装钉钉插件</h3><h4 id="物理机安装"><a href="#物理机安装" class="headerlink" title="物理机安装"></a>物理机安装</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/timonwong/prometheus-webhook-dingtalk/releases/download/v0.3.0/prometheus-webhook-dingtalk-0.3.0.linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">tar解压后启动</span><br><span class="line"></span><br><span class="line">nohup ./prometheus-webhook-dingtalk --ding.profile=&quot;ops_dingding=钉钉机器人处复制webhook&quot; 2&gt;&amp;1 1&gt;dingding.log &amp;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>物理方式安装就不过多介绍</p><h4 id="Docker单机容器部署"><a href="#Docker单机容器部署" class="headerlink" title="Docker单机容器部署"></a>Docker单机容器部署</h4><h5 id="钉钉webhook-容器建立"><a href="#钉钉webhook-容器建立" class="headerlink" title="钉钉webhook 容器建立"></a>钉钉webhook 容器建立</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --restart=always -p 8060:8060 --name=prometheus-webhook-dingtalk timonwong/prometheus-webhook-dingtalk --ding.profile=&quot;ops_dingding=钉钉机器人处复制webhook&quot;</span><br></pre></td></tr></table></figure><h5 id="修改alertmanager-yaml内容"><a href="#修改alertmanager-yaml内容" class="headerlink" title="修改alertmanager.yaml内容"></a>修改alertmanager.yaml内容</h5><p>此处需要修改/root/prometheus-operator-history/contrib/kube-prometheus/manifests/alertmanager-secret.yaml文件，其中的data数据是经过base64加密的，复制出来随便去一个base64解码即可看到。或者运行下面的命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat alertmanager-secret.yaml | grep alertmanager.yaml | cut -f 2- -d &quot;:&quot; | tr -d &quot; &quot; | base64 –d</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/ce16fd528480ca5dc6accbe565813805.png"></p><p>我们需要将secret中的alertmanager.yaml修改为如下的内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">  resolve_timeout: 1h</span><br><span class="line">route:</span><br><span class="line">  receiver: webhook</span><br><span class="line">  group_wait: 30s</span><br><span class="line">  group_interval: 30s</span><br><span class="line">  repeat_interval: 10m</span><br><span class="line">  group_by: [alertname]</span><br><span class="line">  routes:</span><br><span class="line">  - receiver: webhook</span><br><span class="line">    group_wait: 30s</span><br><span class="line">    match:</span><br><span class="line">      team: node</span><br><span class="line">receivers:</span><br><span class="line">- name: webhook</span><br><span class="line">  webhook_configs:</span><br><span class="line">  - url: http://192.168.137.13:8060/dingtalk/ops_dingding/send</span><br><span class="line">    send_resolved: true</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>上述的地址记得修改为你的服务器IP地址</p><p>首先在同一级目录下建立一个alertmanager.yaml文件，内容就是如上</p><p>运行如下命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ALT_S=$(cat alertmanager-secret.yaml | grep alertmanager.yaml | cut -f 2- -d &quot;:&quot; | tr -d &quot; &quot;)</span><br><span class="line">ALT_D=$(cat alertmanager.yaml | base64 | tr -d &quot;n&quot; )</span><br><span class="line">sed -i &quot;s/$ALT_S/$ALT_D/&quot; ./alertmanager-secret.yaml</span><br></pre></td></tr></table></figure><p>即可完成替换，然后重新建立secret，从之前的alertmanager-main-statefulsets.yaml文件中可以看出secret是挂载到卷的，不是环境变量的方式，这样是支持热更新的</p><p><img src="/2019/08/27/Prometheus-Operator/media/669cdd83d0ce476a8c71ed066323249f.png"></p><h1 id="执行修改后的secret文件"><a href="#执行修改后的secret文件" class="headerlink" title="执行修改后的secret文件"></a>执行修改后的secret文件</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f alertmanager-secret.yaml</span><br></pre></td></tr></table></figure><p>查看是否修改成功</p><p><img src="/2019/08/27/Prometheus-Operator/media/7ee31846b6d89f96cc4906984556d0e6.png"></p><h4 id="集群方式部署"><a href="#集群方式部署" class="headerlink" title="集群方式部署"></a>集群方式部署</h4><p>都失败了，暂时放弃这种想法了。</p><h5 id="hostnetwork方式访问"><a href="#hostnetwork方式访问" class="headerlink" title="hostnetwork方式访问"></a>hostnetwork方式访问</h5><p>参考yaml地址<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/Prometheus-operator/alertmanager/hostnetwork%E6%96%B9%E5%BC%8F">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/Prometheus-operator/alertmanager/hostnetwork%E6%96%B9%E5%BC%8F</a></p><h5 id="service方式访问"><a href="#service方式访问" class="headerlink" title="service方式访问"></a>service方式访问</h5><p>这种方式会存在问题</p><p>alertmanager的日志</p><p><img src="/2019/08/27/Prometheus-Operator/media/7bd2e8ce4126f01a2e22c66c7409552c.png"></p><p>钉钉插件日志</p><p><img src="/2019/08/27/Prometheus-Operator/media/e4c36138aa9877ebd76c385982790997.png"></p><p>参考yaml文件</p><p><a href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/Prometheus-operator/alertmanager/dd_webhook-deployment-svc.yaml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/Prometheus-operator/alertmanager/dd_webhook-deployment-svc.yaml</a></p><p>需要将其中的webhook设置成你的</p><h6 id="修改alertmanager-yaml内容-1"><a href="#修改alertmanager-yaml内容-1" class="headerlink" title="修改alertmanager.yaml内容"></a>修改alertmanager.yaml内容</h6><p>此处需要修改/root/prometheus-operator-history/contrib/kube-prometheus/manifests/alertmanager-secret.yaml文件，其中的data数据是经过base64加密的，为了不与上面混淆，将其复制并命名为alertmanager-secret-k8sdd.yaml</p><p>我们需要将secret中的alertmanager.yaml修改为如下的内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">  resolve_timeout: 1h</span><br><span class="line">route:</span><br><span class="line">  receiver: webhook</span><br><span class="line">  group_wait: 30s</span><br><span class="line">  group_interval: 30s</span><br><span class="line">  repeat_interval: 10m</span><br><span class="line">  group_by: [alertname]</span><br><span class="line">  routes:</span><br><span class="line">  - receiver: webhook</span><br><span class="line">    group_wait: 30s</span><br><span class="line">    match:</span><br><span class="line">      team: node</span><br><span class="line">receivers:</span><br><span class="line">- name: webhook</span><br><span class="line">  webhook_configs:</span><br><span class="line">  - url: http://dd-webhook:8060/dingtalk/ops_dingding/send</span><br><span class="line">    send_resolved: true</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>首先在同一级目录下建立一个alertmanager-k8sdd.yaml文件，内容就是如上，修改了地址为service名称<br>运行如下命令，记住alertmanager-k8sdd.yaml不能存在注释符号</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ALT_S=$(cat alertmanager-secret-k8sdd.yaml | grep alertmanager.yaml | cut -f 2- -d &quot;:&quot; | tr -d &quot; &quot;)</span><br><span class="line">ALT_D=$(cat alertmanager-k8sdd.yaml | base64 | tr -d &quot;n&quot; )</span><br><span class="line">sed -i &quot;s/$ALT_S/$ALT_D/&quot; ./alertmanager-secret-k8sdd.yaml</span><br></pre></td></tr></table></figure><p>上述方法我感觉是钉钉插件镜像的代码问题，镜像作者也很久未更新了，所以暂时放弃service方式访问</p><h2 id="企业微信报警"><a href="#企业微信报警" class="headerlink" title="企业微信报警"></a>企业微信报警</h2><p>后续的所有操作都可在以下GitHub地址找到<br><a href="https://github.com/huisebug/Prometheus-Operator">https://github.com/huisebug/Prometheus-Operator</a><br>后续演示的还是基于官方的github来操作</p><h3 id="建立企业微信账户"><a href="#建立企业微信账户" class="headerlink" title="建立企业微信账户"></a>建立企业微信账户</h3><p>step 1: 访问网站<a href="https://work.weixin.qq.com/">https://work.weixin.qq.com/</a> 注册企业微信账号（不需要企业认证）。<br>step 2: 访问apps 创建应用，点击 创建应用按钮 -&gt; 填写应用信息：</p><p><img src="/2019/08/27/Prometheus-Operator/media/2167460f7929e9f8d6b804602d053162.png"></p><p>如果有则忽略</p><h3 id="修改alertmanager-yaml内容-2"><a href="#修改alertmanager-yaml内容-2" class="headerlink" title="修改alertmanager.yaml内容"></a>修改alertmanager.yaml内容</h3><p>此处需要修改kube-prometheus/manifests/alertmanager-secret.yaml文件，其中的data数据是经过base64加密的，复制出来随便去一个base64解码即可看到。或者运行下面的命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat alertmanager-secret.yaml | grep alertmanager.yaml | cut -f 2- -d &quot;:&quot; | tr -d &quot; &quot; | base64 –d</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/ce16fd528480ca5dc6accbe565813805.png"></p><p>我们需要将secret中的alertmanager.yaml修改为如下的内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">  resolve_timeout: 1h</span><br><span class="line">route:</span><br><span class="line">  group_by: [&#x27;alertname&#x27;]</span><br><span class="line">  receiver: &#x27;wechat&#x27;</span><br><span class="line">  group_wait: 30s</span><br><span class="line">  group_interval: 30s</span><br><span class="line">  repeat_interval: 10m</span><br><span class="line"></span><br><span class="line">receivers:</span><br><span class="line">- name: &#x27;wechat&#x27;</span><br><span class="line">  wechat_configs:</span><br><span class="line">  - corp_id: &#x27;企业ID&#x27;</span><br><span class="line">    to_party: &#x27;组ID&#x27;</span><br><span class="line">    agent_id: &#x27;应用ID&#x27;</span><br><span class="line">    api_secret: &#x27;应用密钥&#x27;</span><br><span class="line">    #是否发送恢复信息</span><br><span class="line">    send_resolved: true</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>  corp_id: 企业微信账号唯一 ID， 可以在”我的企业”中查看。</li><li>  to_party: 需要发送的组(部门ID)。即<br><img src="/2019/08/27/Prometheus-Operator/media/a5971d9b5aec90857f23a49be18077d8.png"></li><li>  agent_id: 第三方企业应用的 ID，可以在自己创建的第三方企业应用详情页面查看。</li><li>  api_secret:第三方企业应用的密钥，可以在自己创建的第三方企业应用详情页面查看。<br><img src="/2019/08/27/Prometheus-Operator/media/09f014e63904cbd46b50e91e05cf679b.png"><br>将上面修改好的内容进行base64加密，然后copy到alertmanager-secret.yaml中，然后在alertmanager web页面进行查看</li></ul><p><img src="/2019/08/27/Prometheus-Operator/media/53c9c0f213c0f6922474fae21b5d5048.png"></p><h3 id="建立资源类型PrometheusRule"><a href="#建立资源类型PrometheusRule" class="headerlink" title="建立资源类型PrometheusRule"></a>建立资源类型PrometheusRule</h3><p>默认的在建立<a href="https://github.com/huisebug/Prometheus-Operator">Prometheus-Operator</a>时候会建立官方推荐的一些告警规则，为便于测试，可暂时删除prometheus-rules.yaml中的资源类型PrometheusRule规则</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl --kubeconfig=./kubeconfig/sre.kubeconfig delete -f kube-prometheus/manifests/prometheus-rules.yaml</span><br></pre></td></tr></table></figure><p>然后执行我新建的一些简单告警规则资源类型PrometheusRule（服务状态规则，mysql服务状态规则）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl --kubeconfig=./kubeconfig/sre.kubeconfig apply -f kube-prometheus/manifests/new/mysql-rules.yaml</span><br><span class="line">kubectl --kubeconfig=./kubeconfig/sre.kubeconfig apply -f kube-prometheus/manifests/new/state-rules.yaml</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/c601e3e34c2264ac2a41bb0ce001479f.png"></p><h3 id="建立对应的告警目标服务"><a href="#建立对应的告警目标服务" class="headerlink" title="建立对应的告警目标服务"></a>建立对应的告警目标服务</h3><p>根据上面我们建立的告警规则，我们需要建立mysql服务，mysql监控服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl --kubeconfig=./kubeconfig/sre.kubeconfig apply -f kube-prometheus/manifests/new/mysql-rc.yaml</span><br><span class="line">kubectl --kubeconfig=./kubeconfig/sre.kubeconfig apply -fkube-prometheus/manifests/new/mysql-exporter.yaml</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/fa3d85ccd71b489a0321dde1c4f61e60.png"></p><p>在prometheus web页面查看</p><p><img src="/2019/08/27/Prometheus-Operator/media/eeb84e617080aab07e887b6019149aa5.png"></p><h3 id="验证告警效果"><a href="#验证告警效果" class="headerlink" title="验证告警效果"></a>验证告警效果</h3><p>注意验证方法，MySQL服务自身是没有提供metrics，是依靠mysql-exporter这个服务去收集的，所以当MySQL服务无法访问的时候才能实现mysql告警，MySQL-exporter服务停止是服务停止告警，此处我需要将mysql服务的service配置删除，已达到服务无法访问的效果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl.exe --kubeconfig=./kubeconfig/sre.kubeconfig get svc</span><br><span class="line">kubectl.exe --kubeconfig=./kubeconfig/sre.kubeconfig delete svc mysql-svc</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/859214249b15ec19e6978a18afffb9cd.png"></p><p>30秒后就可以在两个控制台分别看到如下</p><p><img src="/2019/08/27/Prometheus-Operator/media/5d43c62772602e2c81ec114cb7f27e73.png"></p><p><img src="/2019/08/27/Prometheus-Operator/media/475bccb776141ea2327393e1b47eeea0.png"></p><p>企业微信客户端收到错误告警如下</p><p><img src="/2019/08/27/Prometheus-Operator/media/cb724f3a8955de683842e386b32611a3.png"></p><p>重新建立svc，验证恢复告警</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl --kubeconfig=./kubeconfig/sre.kubeconfig apply -f kube-prometheus/manifests/new/mysql-rc.yaml</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/0a9bb4b8c8f002f2129e10f3de548ac6.png"></p><p>总结：</p><ul><li>  告警信息太多，太杂乱</li><li>  错误告警和恢复告警基本内容差不多，无法区别</li><li>  需要自定义告警模板来便于快速定位</li></ul><h3 id="建立自定义告警模板"><a href="#建立自定义告警模板" class="headerlink" title="建立自定义告警模板"></a>建立自定义告警模板</h3><h4 id="alertmanager-temp-configmap-yaml"><a href="#alertmanager-temp-configmap-yaml" class="headerlink" title="alertmanager-temp-configmap.yaml"></a>alertmanager-temp-configmap.yaml</h4><p><img src="/2019/08/27/Prometheus-Operator/media/9faa737a345f90cacdad094721bb64a0.png"></p><p>上面的模板是使用的gotemplate语法编写，其中包含了错误告警和恢复告警，具体编写请参考以下链接<br><a href="https://github.com/songjiayang/prometheus_practice/issues/12">https://github.com/songjiayang/prometheus_practice/issues/12</a></p><h4 id="修改alertmanager-yaml内容-3"><a href="#修改alertmanager-yaml内容-3" class="headerlink" title="修改alertmanager.yaml内容"></a>修改alertmanager.yaml内容</h4><p>我们需要将secret中的alertmanager.yaml修改为如下的内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">  resolve_timeout: 1h</span><br><span class="line">route:</span><br><span class="line">  group_by: [&#x27;alertname&#x27;]</span><br><span class="line">  receiver: &#x27;wechat&#x27;</span><br><span class="line">  group_wait: 30s</span><br><span class="line">  group_interval: 30s</span><br><span class="line">  repeat_interval: 10m</span><br><span class="line"></span><br><span class="line">receivers:</span><br><span class="line">- name: &#x27;wechat&#x27;</span><br><span class="line">  wechat_configs:</span><br><span class="line">  - corp_id: &#x27;企业ID&#x27;</span><br><span class="line">    to_party: &#x27;组ID&#x27;</span><br><span class="line">    agent_id: &#x27;应用ID&#x27;</span><br><span class="line">    api_secret: &#x27;应用密钥&#x27;</span><br><span class="line">    #是否发送恢复信息</span><br><span class="line">    send_resolved: true</span><br><span class="line"><span class="meta">#</span><span class="bash">告警模板</span></span><br><span class="line">templates: </span><br><span class="line">- &#x27;/etc/alertmanager/configmaps/alertmanager-temp/temp.yaml&#x27;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>  templates：指定告警模板的存放位置，注意此处的路径是固定格式的/etc/alertmanager/configmaps/+configmap名称+configmap中的键名，即我在new目录下的alertmanager-temp-configmap.yaml，如下：</li></ul><p><img src="/2019/08/27/Prometheus-Operator/media/1fee93d395693c7b04f1a47da3bea884.png"></p><p><img src="/2019/08/27/Prometheus-Operator/media/dd755c752ef27bee10ea902010d5999e.png"></p><p>将上面修改好的内容进行base64加密，然后copy到alertmanager-secret.yaml中，然后在alertmanager web页面进行查看</p><p><img src="/2019/08/27/Prometheus-Operator/media/4f1cddad3f49bcfa1aa08e5381026785.png"></p><h4 id="alertmanager服务添加configmap挂载"><a href="#alertmanager服务添加configmap挂载" class="headerlink" title="alertmanager服务添加configmap挂载"></a>alertmanager服务添加configmap挂载</h4><p>alertmanager服务的建立是statefulset建立的，是依据alertmanager-alertmanager.yaml文件建立的，需要增加自定义模板的configmap挂载，并且默认是挂载到/etc/alertmanager/configmaps/下的，可参照官方介绍<a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerspec">https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerspec</a></p><p>修改如下：</p><p><img src="/2019/08/27/Prometheus-Operator/media/a47fde5295cd130d94772a0223d818cc.png"></p><p>所以需要重新建立alertmanager服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl --kubeconfig=./kubeconfig/sre.kubeconfig apply -f kube-prometheus/manifests/alertmanager-alertmanager.yaml</span><br></pre></td></tr></table></figure><p>再次验证告警效果，同样的删除mysql的svc。企业微信效果如下</p><p><img src="/2019/08/27/Prometheus-Operator/media/8d5ab49247e0cc6611fe0f1dda7ba020.png"></p><p><img src="/2019/08/27/Prometheus-Operator/media/d02e70173936e3774b30232d105fc185.png"></p><h2 id="企业微信-QQmail同时报警"><a href="#企业微信-QQmail同时报警" class="headerlink" title="企业微信/QQmail同时报警"></a>企业微信/QQmail同时报警</h2><p>配置文件如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">  resolve_timeout: 1h</span><br><span class="line">  smtp_smarthost: &#x27;smtp.qq.com:587&#x27;</span><br><span class="line">  smtp_from: &#x27;发件人&#x27;     </span><br><span class="line">  smtp_auth_username: &#x27;发件人&#x27;   </span><br><span class="line">  smtp_auth_password: &#x27;授权码&#x27;  </span><br><span class="line"></span><br><span class="line">route:</span><br><span class="line">  group_by: [&#x27;alertname&#x27;] </span><br><span class="line">  group_wait: 30s</span><br><span class="line">  group_interval: 30s</span><br><span class="line">  repeat_interval: 10m</span><br><span class="line">  receiver: &#x27;huisebug&#x27;</span><br><span class="line">  routes:</span><br><span class="line">  - receiver: &#x27;wechat&#x27;</span><br><span class="line">    continue: true</span><br><span class="line">  - receiver: &#x27;huisebug&#x27;</span><br><span class="line">    continue: true</span><br><span class="line"></span><br><span class="line">receivers:</span><br><span class="line">- name: &#x27;huisebug&#x27;  </span><br><span class="line">  email_configs:</span><br><span class="line">  - to: &#x27;收件人&#x27;</span><br><span class="line">    send_resolved: true</span><br><span class="line">- name: &#x27;wechat&#x27;</span><br><span class="line">  wechat_configs:</span><br><span class="line">  - corp_id: &#x27;&#x27;</span><br><span class="line">    to_party: &#x27;8&#x27;</span><br><span class="line">    agent_id: &#x27;1000009&#x27;</span><br><span class="line">    api_secret: &#x27;&#x27;</span><br><span class="line">    send_resolved: true</span><br><span class="line">    </span><br><span class="line">templates: </span><br><span class="line">- &#x27;/etc/alertmanager/configmaps/alertmanager-temp/temp.yaml&#x27;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;这是一篇Prometheus-Operator监控k8s服务的部署、监控原理讲解、监控配置、告警模板配置。&lt;/p&gt;</summary>
    
    
    
    <category term="Prometheus" scheme="https://huisebug.github.io/categories/Prometheus/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="prometheus" scheme="https://huisebug.github.io/tags/prometheus/"/>
    
    <category term="alertmanager" scheme="https://huisebug.github.io/tags/alertmanager/"/>
    
    <category term="grafana" scheme="https://huisebug.github.io/tags/grafana/"/>
    
    <category term="mysql-export" scheme="https://huisebug.github.io/tags/mysql-export/"/>
    
    <category term="钉钉告警" scheme="https://huisebug.github.io/tags/%E9%92%89%E9%92%89%E5%91%8A%E8%AD%A6/"/>
    
    <category term="企业微信告警" scheme="https://huisebug.github.io/tags/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E5%91%8A%E8%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>mysql5.7主从复制及mycat1.6读写分离</title>
    <link href="https://huisebug.github.io/2019/05/24/mysql-mycat/"/>
    <id>https://huisebug.github.io/2019/05/24/mysql-mycat/</id>
    <published>2019-05-24T08:04:01.000Z</published>
    <updated>2021-07-09T02:56:31.796Z</updated>
    
    <content type="html"><![CDATA[<p>本文档介绍如何在三台服务器利用docker搭建mysql5.7三机主从复制及mycat1.6实现数据库读写分离。</p><span id="more"></span><h1 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h1><p>本文档介绍如何在三台服务器利用docker搭建mysql5.7三机主从复制及mycat1.6实现数据库读写分离。</p><h1 id="环境需求"><a href="#环境需求" class="headerlink" title="环境需求"></a>环境需求</h1><ul><li><p>  Docker</p></li><li><p>  三台服务器 Centos 7 ，服务器名称使用IP地址末尾代替，分别为182,181,180.</p></li></ul><p>唯一要求</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ulimit -n 根据实际进行设置</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h1><p>docker的安装方式请参照官方地址：<a href="https://docs.docker.com/install/linux/docker-ce/centos/">https://docs.docker.com/install/linux/docker-ce/centos/</a></p><p>其他系统请选择合适的方式安装。</p><h1 id="MySQL主从搭建"><a href="#MySQL主从搭建" class="headerlink" title="MySQL主从搭建"></a>MySQL主从搭建</h1><p>本方案的设计是1台主节点182，2台从节点181、180</p><p>182下的mysql.cnf</p><p>[mysqld]</p><p>#唯一id，如果开启主从功能则每台MySQL节点不能相同</p><p>server-id = 1</p><p>#开启mysql-log-bin功能</p><p>log-bin=mysql-bin</p><p>180下的mysql.cnf</p><p>[mysqld]</p><p>#唯一id，如果开启主从功能则每台MySQL节点不能相同</p><p>server-id = 2</p><p>#开启mysql-log-bin功能</p><p>log-bin=mysql-bin</p><p>181下的mysql.cnf</p><p>[mysqld]</p><p>#唯一id，如果开启主从功能则每台MySQL节点不能相同</p><p>server-id = 3</p><p>#开启mysql-log-bin功能</p><p>log-bin=mysql-bin</p><h2 id="MySQL主节点"><a href="#MySQL主节点" class="headerlink" title="MySQL主节点"></a>MySQL主节点</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">182节点</span><br><span class="line"></span><br><span class="line">docker run -d --restart=always --name mysql5.7master \</span><br><span class="line"></span><br><span class="line">-v /etc/localtime:/etc/localtime \</span><br><span class="line"></span><br><span class="line">-e MYSQL_ROOT_PASSWORD=&#x27;密码&#x27; \</span><br><span class="line"></span><br><span class="line">-p 3309:3306 \</span><br><span class="line"></span><br><span class="line">-v /data/mysql5.7master/backupdir:/var/lib/backupdir \</span><br><span class="line"></span><br><span class="line">-v /data/mysql5.7master/datadir:/var/lib/mysql \</span><br><span class="line"></span><br><span class="line">-v /data/mysql5.7master/mysql.cnf:/etc/mysql/conf.d/mysql.cnf \</span><br><span class="line"></span><br><span class="line">mysql:5.7</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="简单介绍"><a href="#简单介绍" class="headerlink" title="简单介绍"></a>简单介绍</h3><ul><li><p>  -d：后台运行；</p></li><li><p>  --restart=always：允许跟随docker服务启动而启动；</p></li><li><p>  --name mysql5.7master：docker容器名称，从节点宿主机同理；</p></li><li><p>  -v /etc/localtime:/etc/localtime：将服务器宿主机的时间挂载到容器中，实现时区同步；</p></li><li><p>  -e MYSQL_ROOT_PASSWORD=’密码’：MySQL服务root用户密码，修改你想要的密码即可；</p></li><li><p>  -p 3309:3306：将宿主机的3309端口映射到容器的3306端口，即MySQL服务端口号，以便于外部访问；</p></li><li><p>  -v /data/mysql5.7master/backupdir:/var/lib/backupdir：其目录中存放的是将要进行数据库还原的.sql文件，从节点宿主机同理；</p></li><li><p>  -v /data/mysql5.7master/datadir:/var/lib/mysql：MySQL服务的根目录，从节点宿主机同理；</p></li><li><p>  -v /data/mysql5.7master/mysql.cnf:/etc/mysql/conf.d/mysql.cnf：指定MySQL服务的配置文件，从节点宿主机同理；</p></li><li><p>  mysql:5.7：MySQL官方5.7 docker镜像。</p></li></ul><h2 id="MySQL从节点"><a href="#MySQL从节点" class="headerlink" title="MySQL从节点"></a>MySQL从节点</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">180节点</span><br><span class="line"></span><br><span class="line">docker run -d --restart=always --name mysql5.7slave1 \</span><br><span class="line"></span><br><span class="line">-v /etc/localtime:/etc/localtime \</span><br><span class="line"></span><br><span class="line">-e MYSQL_ROOT_PASSWORD=&#x27;密码&#x27; \</span><br><span class="line"></span><br><span class="line">-p 3309:3306 \</span><br><span class="line"></span><br><span class="line">-v /data/mysql5.7slave1/backupdir:/var/lib/backupdir \</span><br><span class="line"></span><br><span class="line">-v /data/mysql5.7slave1/datadir:/var/lib/mysql \</span><br><span class="line"></span><br><span class="line">-v /data/mysql5.7slave1/mysql.cnf:/etc/mysql/conf.d/mysql.cnf \</span><br><span class="line"></span><br><span class="line">mysql:5.7</span><br><span class="line"></span><br><span class="line">181节点</span><br><span class="line"></span><br><span class="line">docker run -d --restart=always --name mysql5.7slave2 \</span><br><span class="line"></span><br><span class="line">-v /etc/localtime:/etc/localtime \</span><br><span class="line"></span><br><span class="line">-e MYSQL_ROOT_PASSWORD=&#x27;密码&#x27; \</span><br><span class="line"></span><br><span class="line">-p 3309:3306 \</span><br><span class="line"></span><br><span class="line">-v /data/mysql5.7slave2/backupdir:/var/lib/backupdir \</span><br><span class="line"></span><br><span class="line">-v /data/mysql5.7slave2/datadir:/var/lib/mysql \</span><br><span class="line"></span><br><span class="line">-v /data/mysql5.7slave2/mysql.cnf:/etc/mysql/conf.d/mysql.cnf \</span><br><span class="line"></span><br><span class="line">mysql:5.7</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="进入Docker容器命令"><a href="#进入Docker容器命令" class="headerlink" title="进入Docker容器命令"></a>进入Docker容器命令</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">182节点</span><br><span class="line"></span><br><span class="line">docker exec -it mysql5.7master bash</span><br><span class="line"></span><br><span class="line">180节点</span><br><span class="line"></span><br><span class="line">docker exec -it mysql5.7slave1 bash</span><br><span class="line"></span><br><span class="line">181节点</span><br><span class="line"></span><br><span class="line">docker exec -it mysql5.7slave2 bash</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="使用MySQL镜像客户端命令直接进入MySQL控制台"><a href="#使用MySQL镜像客户端命令直接进入MySQL控制台" class="headerlink" title="使用MySQL镜像客户端命令直接进入MySQL控制台"></a>使用MySQL镜像客户端命令直接进入MySQL控制台</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">182节点</span><br><span class="line"></span><br><span class="line">docker run -it --link mysql5.7master:mysql --rm mysql:5.7 sh -c &#x27;exec mysql -h&quot;$MYSQL_PORT_3306_TCP_ADDR&quot; -P&quot;$MYSQL_PORT_3306_TCP_PORT&quot; -uroot -p&quot;$MYSQL_ENV_MYSQL_ROOT_PASSWORD&quot;&#x27;</span><br><span class="line"></span><br><span class="line">180节点</span><br><span class="line"></span><br><span class="line">docker run -it --link mysql5.7slave1:mysql --rm mysql:5.7 sh -c &#x27;exec mysql -h&quot;$MYSQL_PORT_3306_TCP_ADDR&quot; -P&quot;$MYSQL_PORT_3306_TCP_PORT&quot; -uroot -p&quot;$MYSQL_ENV_MYSQL_ROOT_PASSWORD&quot;&#x27;</span><br><span class="line"></span><br><span class="line">181节点</span><br><span class="line"></span><br><span class="line">docker run -it --link mysql5.7slave2:mysql --rm mysql:5.7 sh -c &#x27;exec mysql -h&quot;$MYSQL_PORT_3306_TCP_ADDR&quot; -P&quot;$MYSQL_PORT_3306_TCP_PORT&quot; -uroot -p&quot;$MYSQL_ENV_MYSQL_ROOT_PASSWORD&quot;&#x27;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="建立主从复制账户replicate"><a href="#建立主从复制账户replicate" class="headerlink" title="建立主从复制账户replicate"></a>建立主从复制账户replicate</h2><p>相当于给从节点建立一个MySQL服务账户，使其有权限可以进行数据拉取。所以仅需在主节点182执行如下SQL即可。</p><p># 连接MySQL控制台</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --link mysql5.7master:mysql --rm mysql:5.7 sh -c &#x27;exec mysql -h&quot;\$MYSQL_PORT_3306_TCP_ADDR&quot; -P&quot;\$MYSQL_PORT_3306_TCP_PORT&quot; -uroot -p&quot;\$MYSQL_ENV_MYSQL_ROOT_PASSWORD&quot;&#x27;</span><br></pre></td></tr></table></figure><p>SQL如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">create user &#x27;replicate&#x27;@&#x27;%&#x27; identified by &#x27;123456abc&#x27;;</span><br><span class="line"></span><br><span class="line">grant replication slave on *.* to &#x27;replicate&#x27;@&#x27;%&#x27;;</span><br><span class="line"></span><br><span class="line">flush privileges;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>查看主节点mysql状态，将其master_log_file、master_log_pos记录下来提供给从节点进行主从建立。</p><p>SQL如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">show master status;</span><br></pre></td></tr></table></figure><h2 id="从节点开启主从复制功能"><a href="#从节点开启主从复制功能" class="headerlink" title="从节点开启主从复制功能"></a>从节点开启主从复制功能</h2><p>SQL如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">stop slave;</span><br><span class="line"></span><br><span class="line">change master to</span><br><span class="line"></span><br><span class="line">master_host=&#x27;10.10.10.235&#x27;,</span><br><span class="line"></span><br><span class="line">master_port=3309,</span><br><span class="line"></span><br><span class="line">master_password=&#x27;123456abc&#x27;,</span><br><span class="line"></span><br><span class="line">master_user=&#x27;replicate&#x27;,</span><br><span class="line"></span><br><span class="line">master_log_file=&#x27;mysql-bin.000003&#x27;,</span><br><span class="line"></span><br><span class="line">master_log_pos=749;</span><br><span class="line"></span><br><span class="line">start slave;</span><br><span class="line"></span><br><span class="line">show slave status\G;</span><br></pre></td></tr></table></figure><h3 id="简单介绍-1"><a href="#简单介绍-1" class="headerlink" title="简单介绍"></a>简单介绍</h3><ul><li><p>  stop slave;：停止从功能；</p></li><li><p>  change master to</p></li></ul><p>master_host=’10.10.10.235’,</p><p>master_port=3309,</p><p>master_password=’123456abc’,</p><p>master_user=’replicate’,</p><p>master_log_file=’mysql-bin.000003’,</p><p>master_log_pos=749;</p><ol><li><p> master_host=’10.10.10.235’：MySQL主节点所在服务器IP地址，此处因为我的是三台物理服务器，此处即写的是宿主机的IP地址，如果是只有一台宿主机，可以写docker网络中的主节点IP地址；</p></li><li><p> master_port=3309：MySQL主节点所在服务器映射MySQL容器的端口号，如果是同一台宿主机，此处可以写容器中MySQL服务的3306端口；</p></li><li><p> master_password=’123456abc’,master_user=’replicate’：主从复制账户名和密码；</p></li><li><p> master_log_file=’mysql-bin.000003’,master_log_pos=749;：在主节点使用SQL命令show master status; 获取到的主节点信息。每次主节点产生变动都会变化，所以如果主从复制发生建立失败都要重新修改两个对应的值。</p></li></ol><ul><li><p>  stop slave;：开启从功能；</p></li><li><p>  show slave status\G;：查看是否建立主从复制成功，确保出现两个YES</p></li></ul><p><img src="/2019/05/24/mysql-mycat/media/e21d87fc668d517d2583b1522eb5aa81.png"></p><p>然后就可以建立新数据库，查看是否同步</p><p>SQL如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">create DATABASE `box-education`;</span><br><span class="line"></span><br><span class="line">create DATABASE boxtest;</span><br><span class="line"></span><br><span class="line">create DATABASE db_benefit;</span><br></pre></td></tr></table></figure><p>数据库名称使用横杠要加上``号</p><p>主从复制即搭建完成</p><h1 id="Mycat1-6读写分离"><a href="#Mycat1-6读写分离" class="headerlink" title="Mycat1.6读写分离"></a>Mycat1.6读写分离</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">docker run -d --restart=always --name=mycat1.6 \</span><br><span class="line"></span><br><span class="line">-v /etc/localtime:/etc/localtime \</span><br><span class="line"></span><br><span class="line">-v /data/mycat1.6/schema.xml:/usr/local/mycat/conf/schema.xml \</span><br><span class="line"></span><br><span class="line">-v /data/mycat1.6/server.xml:/usr/local/mycat/conf/server.xml \</span><br><span class="line"></span><br><span class="line">-v /data/mycat1.6/rule.xml:/usr/local/mycat/conf/rule.xml \</span><br><span class="line"></span><br><span class="line">-p 3306:8066 -p 9066:9066 \</span><br><span class="line"></span><br><span class="line">mycat:1.6.6.1</span><br></pre></td></tr></table></figure><h3 id="简单介绍-2"><a href="#简单介绍-2" class="headerlink" title="简单介绍"></a>简单介绍</h3><ul><li><p>  -d：后台运行；</p></li><li><p>  --restart=always：允许跟随docker服务启动而启动；</p></li><li><p>  --name mycat1.6：docker容器名称；</p></li><li><p>  -v /etc/localtime:/etc/localtime：将服务器宿主机的时间挂载到容器中，实现时区同步；</p></li><li><p>  -v /data/mycat1.6/schema.xml:/usr/local/mycat/conf/schema.xml：编写MySQL数据库的连接信息，对应的逻辑数据库指向MySQL的真实数据库；</p></li><li><p>  -v /data/mycat1.6/server.xml:/usr/local/mycat/conf/server.xml：mycat逻辑数据库名称以对应MySQL中的真实数据库，mycat管理员账户和密码；</p></li><li><p>  -v /data/mycat1.6/rule.xml:/usr/local/mycat/conf/rule.xml：mycat读写分离规则；</p></li><li><p>  -p 3306:8066：mycat提供数据库功能的管理端口，在连接数据库客户端看来，这个就是数据库端口；</p></li><li><p>  -p 9066:9066：mycat管理自身的数据库功能端口；</p></li><li><p>  mycat:1.6.6.1：mycat镜像名称，注意，官方提供了Dockerfile进行制作镜像，具体的Dockerfile如下:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cat &gt; Dockerfile &lt;&lt; EOF</span><br><span class="line"></span><br><span class="line">FROM openjdk:8-jdk-stretch</span><br><span class="line"></span><br><span class="line">ENV JAVA_OPTS=&quot;-Xmx2048m -XX:MetaspaceSize=1024m -XX:MaxMetaspaceSize=1536m -Xss2m&quot;</span><br><span class="line"></span><br><span class="line">ENV CATALINA_OPTS=&quot;-Djava.awt.headless=true&quot;</span><br><span class="line"></span><br><span class="line">ADD http://dl.mycat.io/1.6.6.1/Mycat-server-1.6.6.1-release-20181031195535-linux.tar.gz /usr/local</span><br><span class="line"></span><br><span class="line">RUN cd /usr/local &amp;&amp; tar -zxvf Mycat-server-1.6.6.1-release-20181031195535-linux.tar.gz &amp;&amp; ls -lna</span><br><span class="line"></span><br><span class="line">VOLUME /usr/local/mycat/conf</span><br><span class="line"></span><br><span class="line">VOLUME /usr/local/mycat/logs</span><br><span class="line"></span><br><span class="line">EXPOSE 8066 9066</span><br><span class="line"></span><br><span class="line">CMD [&quot;/usr/local/mycat/bin/mycat&quot;, &quot;console&quot;]</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li></ul><p>读写分离即安装完成</p><h1 id="结尾语"><a href="#结尾语" class="headerlink" title="结尾语"></a>结尾语</h1><p>如果需要MySQL服务的mysql.cnf和mycat的schema.xml、server.xml和rule.xml的具体配置请按照主页的联系方式进行沟通。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文档介绍如何在三台服务器利用docker搭建mysql5.7三机主从复制及mycat1.6实现数据库读写分离。&lt;/p&gt;</summary>
    
    
    
    <category term="Mysql" scheme="https://huisebug.github.io/categories/Mysql/"/>
    
    
    <category term="docker" scheme="https://huisebug.github.io/tags/docker/"/>
    
    <category term="mysql5.7" scheme="https://huisebug.github.io/tags/mysql5-7/"/>
    
    <category term="mycat1.6" scheme="https://huisebug.github.io/tags/mycat1-6/"/>
    
    <category term="读写分离" scheme="https://huisebug.github.io/tags/%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/"/>
    
  </entry>
  
  <entry>
    <title>国内安装knative v0.6.0使用dockerhub镜像</title>
    <link href="https://huisebug.github.io/2019/05/23/knative/"/>
    <id>https://huisebug.github.io/2019/05/23/knative/</id>
    <published>2019-05-23T06:04:01.000Z</published>
    <updated>2021-07-09T03:00:14.342Z</updated>
    
    <content type="html"><![CDATA[<p>本文是在基于官方knative的基础上进行安装，因为官方yaml中涉及太多的gcr.io镜像和配置文件中也包含gcr.io，便于国内用户进行研究学习而进行替换为可拉取镜像的yaml文件。其中主要是替换镜像的操作。</p><span id="more"></span><p>直接使用替换好的官方yaml文件请移步<a href="https://github.com/huisebug/knative-yaml">knative-image</a></p><p>Knative</p><p><a href="https://github.com/knative/">knative官方github</a><br><a href="https://knative.dev/docs/install/knative-with-any-k8s/">knative官方安装方式</a></p><p>##官方下载yaml地址<br><a href="https://github.com/knative/serving/releases/download/v0.6.0/serving.yaml">https://github.com/knative/serving/releases/download/v0.6.0/serving.yaml</a><br><a href="https://github.com/knative/build/releases/download/v0.6.0/build.yaml">https://github.com/knative/build/releases/download/v0.6.0/build.yaml</a><br><a href="https://github.com/knative/eventing/releases/download/v0.6.0/release.yaml">https://github.com/knative/eventing/releases/download/v0.6.0/release.yaml</a><br><a href="https://github.com/knative/serving/releases/download/v0.6.0/monitoring.yaml">https://github.com/knative/serving/releases/download/v0.6.0/monitoring.yaml</a><br><a href="https://github.com/knative/eventing-sources/releases/download/v0.6.0/eventing-sources.yaml">https://github.com/knative/eventing-sources/releases/download/v0.6.0/eventing-sources.yaml</a><br><a href="https://raw.githubusercontent.com/knative/serving/v0.6.0/third_party/config/build/clusterrole.yaml">https://raw.githubusercontent.com/knative/serving/v0.6.0/third_party/config/build/clusterrole.yaml</a></p><p>#yaml文件下载py脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF | python -</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">packyamllist=[&#x27;serving:serving&#x27;,</span><br><span class="line">                       &#x27;build:build&#x27;,</span><br><span class="line">                       &#x27;eventing:release&#x27;,</span><br><span class="line">                       &#x27;serving:monitoring&#x27;,</span><br><span class="line">                       &#x27;eventing-sources:eventing-sources&#x27;</span><br><span class="line">                       ]</span><br><span class="line">yamlurl=[&#x27;https://raw.githubusercontent.com/knative/serving/v0.6.0/third_party/config/build/clusterrole.yaml&#x27;]</span><br><span class="line">for urlfile in packyamllist:</span><br><span class="line">  ls=urlfile.split(&#x27;:&#x27;)</span><br><span class="line">  pack=ls[0]</span><br><span class="line">  yamlname=ls[1]</span><br><span class="line">  yamlurl.append(&quot;https://github.com/knative/%s/releases/download/v0.6.0/%s.yaml&quot; % (pack, yamlname))</span><br><span class="line">for url in yamlurl:</span><br><span class="line">  os.system(&#x27;wget %s&#x27; % url)</span><br><span class="line">  </span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>#抓取yaml中的所有镜像地址，建立image文件夹及生成Dockerfile</p><p>##build.yaml</p><p>###其中包含对应镜像列表<br>[‘gcr.io/knative-releases/github.com/knative/build/cmd/creds-init@sha256:101f537b53b895b28b84ac3c74ede7d250845e24c51c26516873d8ccb23168ce’]<br>[‘gcr.io/knative-releases/github.com/knative/build/cmd/git-init@sha256:ce2c17308e9cb81992be153861c359a0c9e5f69c501a490633c8fe54ec992d53’]<br>[‘gcr.io/cloud-builders/gcs-fetcher’]<br>[‘gcr.io/knative-releases/github.com/knative/build/cmd/nop@sha256:50e2be042298f24800b9840a9aef831a5fe4d89d9a8edea5e0559cdedf32369d’]<br>[‘gcr.io/knative-releases/github.com/knative/build/cmd/creds-init@sha256:101f537b53b895b28b84ac3c74ede7d250845e24c51c26516873d8ccb23168ce’]<br>[‘gcr.io/knative-releases/github.com/knative/build/cmd/git-init@sha256:ce2c17308e9cb81992be153861c359a0c9e5f69c501a490633c8fe54ec992d53’]<br>[‘gcr.io/knative-releases/github.com/knative/build/cmd/nop@sha256:50e2be042298f24800b9840a9aef831a5fe4d89d9a8edea5e0559cdedf32369d’]<br>[‘gcr.io/knative-releases/github.com/knative/build/cmd/controller@sha256:6a762848a46786cb481f5870787133e0d5e15615f8d54a5ba50d86b8315a58eb’]<br>[‘gcr.io/knative-releases/github.com/knative/build/cmd/webhook@sha256:8f0bbc50b63f368c9959acab87838c6986691c28d424847459f3526bf97f8a3e’]</p><p>###py内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF | python -</span><br><span class="line"></span><br><span class="line">import os,re</span><br><span class="line"></span><br><span class="line">pwd=os.getcwd()</span><br><span class="line">with open(&#x27;build.yaml&#x27;, &#x27;r&#x27;) as myfile:</span><br><span class="line">  reg=&#x27;gcr.io/.*&#x27;</span><br><span class="line">  imgre=re.compile(reg)</span><br><span class="line">  for yamlline in myfile:</span><br><span class="line">    myimage = re.findall(imgre, yamlline)</span><br><span class="line">    if myimage != []:</span><br><span class="line">      #print(myimage)</span><br><span class="line">      branch=myimage[0].split(&#x27;knative&#x27;)[-1].split(&#x27;@&#x27;)[0].replace(&#x27;/&#x27;,&#x27;-&#x27;).lstrip(&#x27;-&#x27;)</span><br><span class="line">      #print(branch)</span><br><span class="line">      if os.path.isdir(r&#x27;%s/build/%s&#x27; % (pwd, branch)):</span><br><span class="line">        pass</span><br><span class="line">      else:</span><br><span class="line">        os.makedirs(r&#x27;%s/build/%s&#x27; % (pwd, branch))</span><br><span class="line">      os.system(&#x27;echo %s &gt; %s/build/%s/Dockerfile&#x27; % (&#x27;FROM&#x27;+&#x27; &#x27;+myimage[0], pwd, branch))</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>##clusterrole.yaml没有涉及到镜像</p><p>##eventing-sources.yaml</p><p>其中包含对应镜像列表<br>[‘gcr.io/knative-releases/github.com/knative/eventing-sources/cmd/github_receive_adapter@sha256:b5d6e12d16d16c6c42ae3d4325a1ef3a8a129dfc97740aa28000c0867edfc4ff’]<br>[‘gcr.io/knative-releases/github.com/knative/eventing-sources/cmd/manager@sha256:99cf1f559f74ae97f271632697ed6e78a3fdd88a155632a57341b0dd6eab6581’]</p><p>###py内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF | python -</span><br><span class="line"></span><br><span class="line">import os,re</span><br><span class="line"></span><br><span class="line">pwd=os.getcwd()</span><br><span class="line">with open(&#x27;eventing-sources.yaml&#x27;, &#x27;r&#x27;) as myfile:</span><br><span class="line">  reg=&#x27;gcr.io/.*&#x27;</span><br><span class="line">  imgre=re.compile(reg)</span><br><span class="line">  for yamlline in myfile:</span><br><span class="line">    myimage = re.findall(imgre, yamlline)</span><br><span class="line">    if myimage != []:</span><br><span class="line">      #print(myimage)</span><br><span class="line">      branch=myimage[0].split(&#x27;knative&#x27;)[-1].split(&#x27;@&#x27;)[0].replace(&#x27;/&#x27;,&#x27;-&#x27;).lstrip(&#x27;-&#x27;)</span><br><span class="line">      #print(branch)</span><br><span class="line">      if os.path.isdir(r&#x27;%s/eventing-sources/%s&#x27; % (pwd, branch)):</span><br><span class="line">        pass</span><br><span class="line">      else:</span><br><span class="line">        os.makedirs(r&#x27;%s/eventing-sources/%s&#x27; % (pwd, branch))</span><br><span class="line">      os.system(&#x27;echo %s &gt; %s/eventing-sources/%s/Dockerfile&#x27; % (&#x27;FROM&#x27;+&#x27; &#x27;+myimage[0], pwd, branch))</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>##monitoring.yaml</p><p>###其中包含对应镜像列表<br>[‘k8s.gcr.io/elasticsearch:v5.6.4’]<br>[‘k8s.gcr.io/fluentd-elasticsearch:v2.0.4’]<br>[‘k8s.gcr.io/addon-resizer:1.7’]</p><p>###py内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF | python -</span><br><span class="line"></span><br><span class="line">import os,re</span><br><span class="line"></span><br><span class="line">pwd=os.getcwd()</span><br><span class="line">with open(&#x27;monitoring.yaml&#x27;, &#x27;r&#x27;) as myfile:</span><br><span class="line">  reg=&#x27;k8s.gcr.io/.*&#x27;</span><br><span class="line">  imgre=re.compile(reg)</span><br><span class="line">  for yamlline in myfile:</span><br><span class="line">    myimage = re.findall(imgre, yamlline)</span><br><span class="line">    if myimage != []:</span><br><span class="line">      #print(myimage)</span><br><span class="line">      branch=myimage[0].replace(&#x27;/&#x27;,&#x27;-&#x27;).lstrip(&#x27;-&#x27;).replace(&#x27;:&#x27;,&#x27;-&#x27;)</span><br><span class="line">      #print(branch)</span><br><span class="line">      if os.path.isdir(r&#x27;%s/monitoring/%s&#x27; % (pwd, branch)):</span><br><span class="line">        pass</span><br><span class="line">      else:</span><br><span class="line">        os.makedirs(r&#x27;%s/monitoring/%s&#x27; % (pwd, branch))</span><br><span class="line">      os.system(&#x27;echo %s &gt; %s/monitoring/%s/Dockerfile&#x27; % (&#x27;FROM&#x27;+&#x27; &#x27;+myimage[0], pwd, branch))</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>##release.yaml</p><p>###其中包含对应镜像列表<br>[‘ gcr.io/knative-releases/github.com/knative/eventing/cmd/broker/ingress@sha256:a0acbe69420a67bef520e86aceaa237bf540c15882701c96245a6c4e06413bf6’]<br>[‘ gcr.io/knative-releases/github.com/knative/eventing/cmd/broker/filter@sha256:b4da7ce7b12aff2355066ed3237aadcf35df3b1c78db83cc538e6cffa564f208’]<br>[‘ gcr.io/knative-releases/github.com/knative/eventing/cmd/controller@sha256:85c010633944c06f4c16253108c2338dba271971b2b5f2d877b8247fa19ff5cb’]<br>[‘ gcr.io/knative-releases/github.com/knative/eventing/cmd/cronjob_receive_adapter@sha256:6bbb724d5a4dbaaead890ea51d5f84eb9514974a2d06e26c8753db59010987fb’]<br>[‘ gcr.io/knative-releases/github.com/knative/eventing/cmd/apiserver_receive_adapter@sha256:7349f83eebe85a3eed7cdc4d442a935deab1ba0c42f34294f219f4ef17b59fec’]<br>[‘ gcr.io/knative-releases/github.com/knative/eventing/cmd/sources_controller@sha256:aaa48f71a8db1b1dcf86c57d2dd72be1a65ed76d77f23a5abef4b2ad5c01c863’]<br>[‘ gcr.io/knative-releases/github.com/knative/eventing/cmd/webhook@sha256:34a7cac96f8c809a7ce8ea0a86445204bbc6ac897525b876f53babb325f50bdc’]<br>[‘ gcr.io/knative-releases/github.com/knative/eventing/cmd/in_memory/controller@sha256:496c19e81d9e7e40b3887c7c290304934f54f46c8a9186e800e314c014970c26’]<br>[‘ gcr.io/knative-releases/github.com/knative/eventing/cmd/in_memory/dispatcher@sha256:897f03ed16e0000944da9ee0fdc971c43c8a494ff771c4e64d0573caf357c013’]<br>[‘k8s.gcr.io/fluentd-elasticsearch:v2.0.4’]</p><p>####注意：因为这个yaml文件存在gcr.io和k8s.gcr.io的镜像，所以需要两次正则筛选</p><p>###py内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF | python -</span><br><span class="line"></span><br><span class="line">import os,re</span><br><span class="line"></span><br><span class="line">pwd=os.getcwd()</span><br><span class="line">reglist=[&#x27;\sgcr.io/.*&#x27;, &#x27;k8s.gcr.io/.*&#x27;]</span><br><span class="line">for reg in reglist:</span><br><span class="line">  imgre=re.compile(reg)</span><br><span class="line">  with open(&#x27;release.yaml&#x27;, &#x27;r&#x27;) as myfile:</span><br><span class="line">    for yamlline in myfile:</span><br><span class="line">      myimage = re.findall(imgre, yamlline)</span><br><span class="line">      if myimage != []:</span><br><span class="line">        #print(myimage)</span><br><span class="line">        branch=myimage[0].split(&#x27;knative&#x27;)[-1].split(&#x27;@&#x27;)[0].replace(&#x27;/&#x27;,&#x27;-&#x27;).lstrip(&#x27;-&#x27;).replace(&#x27;:&#x27;,&#x27;-&#x27;)</span><br><span class="line">        #print(branch)</span><br><span class="line">        if os.path.isdir(r&#x27;%s/release/%s&#x27; % (pwd, branch)):</span><br><span class="line">          pass</span><br><span class="line">        else:</span><br><span class="line">          os.makedirs(r&#x27;%s/release/%s&#x27; % (pwd, branch))</span><br><span class="line">        os.system(&#x27;echo %s &gt; %s/release/%s/Dockerfile&#x27; % (&#x27;FROM&#x27;+&#x27; &#x27;+myimage[0], pwd, branch))</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>##serving.yaml</p><p>###其中包含对应镜像列表<br>[‘ gcr.io/knative-releases/github.com/knative/serving/cmd/queue@sha256:1e40c99ff5977daa2d69873fff604c6d09651af1f9ff15aadf8849b3ee77ab45’]<br>[‘ gcr.io/knative-releases/github.com/knative/serving/cmd/activator@sha256:f553b6cb7599f2f71190ddc93024952e22f2f55e97a3f38519d4d622fc751651’]<br>[‘ gcr.io/knative-releases/github.com/knative/serving/cmd/autoscaler@sha256:3a466eaf05cd505338163322331ee8634c601204250fa639360ae3524756acc3’]<br>[‘ gcr.io/knative-releases/github.com/knative/serving/cmd/queue@sha256:1e40c99ff5977daa2d69873fff604c6d09651af1f9ff15aadf8849b3ee77ab45’]<br>[‘ gcr.io/knative-releases/github.com/knative/serving/cmd/controller@sha256:8f402eab0ada038d3de2ad753a40f9f441715d08058d890537146bb0aba11c8e’]<br>[‘ gcr.io/knative-releases/github.com/knative/serving/cmd/networking/certmanager@sha256:dc77db09a23103f64a554de4e01cfda7371cbb13bc0954c991bdc4141169257f’]<br>[‘ gcr.io/knative-releases/github.com/knative/serving/cmd/networking/istio@sha256:55fe9eeacfc20d97d3cd4f80bfc8a9b95cff7b5c50121bda87f754da8f05e57b’]<br>[‘ gcr.io/knative-releases/github.com/knative/serving/cmd/webhook@sha256:f0f98736bd4b55354f447f59183bf26b9be1ab01691b8b4aeee85caeb1166562’]<br>[‘k8s.gcr.io/fluentd-elasticsearch:v2.0.4’]<br>[‘gcr.io/knative-releases/github.com/knative/eventing-sources/cmd/github_receive_adapter@sha256:b5d6e12d16d16c6c42ae3d4325a1ef3a8a129dfc97740aa28000c0867edfc4ff’]<br>[‘gcr.io/knative-releases/github.com/knative/eventing-sources/cmd/manager@sha256:99cf1f559f74ae97f271632697ed6e78a3fdd88a155632a57341b0dd6eab6581’]<br>[‘k8s.gcr.io/elasticsearch:v5.6.4’]<br>[‘k8s.gcr.io/fluentd-elasticsearch:v2.0.4’]<br>[‘k8s.gcr.io/addon-resizer:1.7’]</p><p>###py内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF | python -</span><br><span class="line"></span><br><span class="line">import os,re</span><br><span class="line"></span><br><span class="line">pwd=os.getcwd()</span><br><span class="line">reglist=[&#x27;\sgcr.io/.*&#x27;, &#x27;k8s.gcr.io/.*&#x27;]</span><br><span class="line">for reg in reglist:</span><br><span class="line">  imgre=re.compile(reg)</span><br><span class="line">  with open(&#x27;serving.yaml&#x27;, &#x27;r&#x27;) as myfile:</span><br><span class="line">    for yamlline in myfile:</span><br><span class="line">      myimage = re.findall(imgre, yamlline)</span><br><span class="line">      if myimage != []:</span><br><span class="line">        #print(myimage)</span><br><span class="line">        branch=myimage[0].split(&#x27;knative&#x27;)[-1].split(&#x27;@&#x27;)[0].replace(&#x27;/&#x27;,&#x27;-&#x27;).lstrip(&#x27;-&#x27;).replace(&#x27;:&#x27;,&#x27;-&#x27;)</span><br><span class="line">        #print(branch)</span><br><span class="line">        if os.path.isdir(r&#x27;%s/serving/%s&#x27; % (pwd, branch)):</span><br><span class="line">          pass</span><br><span class="line">        else:</span><br><span class="line">          os.makedirs(r&#x27;%s/serving/%s&#x27; % (pwd, branch))</span><br><span class="line">        os.system(&#x27;echo %s &gt; %s/serving/%s/Dockerfile&#x27; % (&#x27;FROM&#x27;+&#x27; &#x27;+myimage[0], pwd, branch))</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>#设置git ssh登录并将各image的Dockerfile推送到不同的分支</p><p>##生成sshkey</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;huisebug@outlook.com&quot;</span><br></pre></td></tr></table></figure><p>##shell内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">cat &gt;gitpush.sh &lt;&lt; EOF</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">git config --global push.default current;</span><br><span class="line">git config --global user.name &quot;root&quot; </span><br><span class="line">git config --global user.email &quot;huisebug@outlook.com&quot;</span><br><span class="line">WORKDIR=/root/knative</span><br><span class="line">for yamldir in build eventing-sources monitoring release serving</span><br><span class="line">do</span><br><span class="line">cd $WORKDIR/$yamldir;</span><br><span class="line">for dir in $(ls -d */)</span><br><span class="line">do</span><br><span class="line"><span class="meta">#</span><span class="bash">去除<span class="string">&quot;/&quot;</span>符号</span></span><br><span class="line">branch=$(echo $dir | tr -d &#x27;/&#x27;)</span><br><span class="line">cd $WORKDIR/$yamldir/$dir;</span><br><span class="line">git init;</span><br><span class="line">git checkout -b $branch</span><br><span class="line">git add -A Dockerfile;</span><br><span class="line">git commit -m &quot;commit $branch&quot;;</span><br><span class="line">git remote add origin git@github.com:huisebug/knative-images.git;</span><br><span class="line">git push -f -u origin $branch;</span><br><span class="line">done</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>#所有分支信息，共27个镜像在gcr.io,其中的quay.io国内是可以拉取到的。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">build-cmd-controller</span><br><span class="line">build-cmd-creds-init</span><br><span class="line">build-cmd-git-init</span><br><span class="line">build-cmd-nop</span><br><span class="line">build-cmd-webhook</span><br><span class="line">gcr.io-cloud-builders-gcs-fetcher</span><br><span class="line">k8s.gcr.io-addon-resizer-1.7</span><br><span class="line">k8s.gcr.io-elasticsearch-v5.6.4</span><br><span class="line">k8s.gcr.io-fluentd-elasticsearch-v2.0.4</span><br><span class="line">eventing-sources-cmd-github_receive_adapter</span><br><span class="line">eventing-sources-cmd-manager</span><br><span class="line">eventing-cmd-apiserver_receive_adapter</span><br><span class="line">eventing-cmd-broker-filter</span><br><span class="line">eventing-cmd-broker-ingress</span><br><span class="line">eventing-cmd-controller</span><br><span class="line">eventing-cmd-cronjob_receive_adapter</span><br><span class="line">eventing-cmd-in_memory-controller</span><br><span class="line">eventing-cmd-in_memory-dispatcher</span><br><span class="line">eventing-cmd-sources_controller</span><br><span class="line">eventing-cmd-webhook</span><br><span class="line">serving-cmd-activator</span><br><span class="line">serving-cmd-autoscaler</span><br><span class="line">serving-cmd-controller</span><br><span class="line">serving-cmd-networking-certmanager</span><br><span class="line">serving-cmd-networking-istio</span><br><span class="line">serving-cmd-queue</span><br><span class="line">serving-cmd-webhook</span><br></pre></td></tr></table></figure><p>#分支对应在gcr.io的镜像，其中quay.io的镜像国内可以拉取，这也是image.txt</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; image.txt &lt;&lt; EOF</span><br><span class="line">build-cmd-git-init</span><br><span class="line">gcr.io/knative-releases/github.com/knative/build/cmd/git-init@sha256:ce2c17308e9cb81992be153861c359a0c9e5f69c501a490633c8fe54ec992d53</span><br><span class="line">build-cmd-creds-init</span><br><span class="line">gcr.io/knative-releases/github.com/knative/build/cmd/creds-init@sha256:101f537b53b895b28b84ac3c74ede7d250845e24c51c26516873d8ccb23168ce</span><br><span class="line">build-cmd-nop</span><br><span class="line">gcr.io/knative-releases/github.com/knative/build/cmd/nop@sha256:50e2be042298f24800b9840a9aef831a5fe4d89d9a8edea5e0559cdedf32369d</span><br><span class="line">build-cmd-controller</span><br><span class="line">gcr.io/knative-releases/github.com/knative/build/cmd/controller@sha256:6a762848a46786cb481f5870787133e0d5e15615f8d54a5ba50d86b8315a58eb</span><br><span class="line">build-cmd-webhook</span><br><span class="line">gcr.io/knative-releases/github.com/knative/build/cmd/webhook@sha256:8f0bbc50b63f368c9959acab87838c6986691c28d424847459f3526bf97f8a3e</span><br><span class="line">gcr.io-cloud-builders-gcs-fetcher</span><br><span class="line">gcr.io/cloud-builders/gcs-fetcher</span><br><span class="line">eventing-cmd-broker-ingress</span><br><span class="line">gcr.io/knative-releases/github.com/knative/eventing/cmd/broker/ingress@sha256:a0acbe69420a67bef520e86aceaa237bf540c15882701c96245a6c4e06413bf6</span><br><span class="line">eventing-cmd-broker-filter</span><br><span class="line">gcr.io/knative-releases/github.com/knative/eventing/cmd/broker/filter@sha256:b4da7ce7b12aff2355066ed3237aadcf35df3b1c78db83cc538e6cffa564f208</span><br><span class="line">eventing-cmd-controller</span><br><span class="line">gcr.io/knative-releases/github.com/knative/eventing/cmd/controller@sha256:85c010633944c06f4c16253108c2338dba271971b2b5f2d877b8247fa19ff5cb</span><br><span class="line">eventing-cmd-cronjob_receive_adapter</span><br><span class="line">gcr.io/knative-releases/github.com/knative/eventing/cmd/cronjob_receive_adapter@sha256:6bbb724d5a4dbaaead890ea51d5f84eb9514974a2d06e26c8753db59010987fb</span><br><span class="line">eventing-cmd-apiserver_receive_adapter</span><br><span class="line">gcr.io/knative-releases/github.com/knative/eventing/cmd/apiserver_receive_adapter@sha256:7349f83eebe85a3eed7cdc4d442a935deab1ba0c42f34294f219f4ef17b59fec</span><br><span class="line">eventing-cmd-sources_controller</span><br><span class="line">gcr.io/knative-releases/github.com/knative/eventing/cmd/sources_controller@sha256:aaa48f71a8db1b1dcf86c57d2dd72be1a65ed76d77f23a5abef4b2ad5c01c863</span><br><span class="line">eventing-cmd-webhook</span><br><span class="line">gcr.io/knative-releases/github.com/knative/eventing/cmd/webhook@sha256:34a7cac96f8c809a7ce8ea0a86445204bbc6ac897525b876f53babb325f50bdc</span><br><span class="line">eventing-cmd-in_memory-controller</span><br><span class="line">gcr.io/knative-releases/github.com/knative/eventing/cmd/in_memory/controller@sha256:496c19e81d9e7e40b3887c7c290304934f54f46c8a9186e800e314c014970c26</span><br><span class="line">eventing-cmd-in_memory-dispatcher</span><br><span class="line">gcr.io/knative-releases/github.com/knative/eventing/cmd/in_memory/dispatcher@sha256:897f03ed16e0000944da9ee0fdc971c43c8a494ff771c4e64d0573caf357c013</span><br><span class="line">serving-cmd-queue</span><br><span class="line">gcr.io/knative-releases/github.com/knative/serving/cmd/queue@sha256:1e40c99ff5977daa2d69873fff604c6d09651af1f9ff15aadf8849b3ee77ab45</span><br><span class="line">serving-cmd-activator</span><br><span class="line">gcr.io/knative-releases/github.com/knative/serving/cmd/activator@sha256:f553b6cb7599f2f71190ddc93024952e22f2f55e97a3f38519d4d622fc751651</span><br><span class="line">serving-cmd-autoscaler</span><br><span class="line">gcr.io/knative-releases/github.com/knative/serving/cmd/autoscaler@sha256:3a466eaf05cd505338163322331ee8634c601204250fa639360ae3524756acc3</span><br><span class="line">serving-cmd-controller</span><br><span class="line">gcr.io/knative-releases/github.com/knative/serving/cmd/controller@sha256:8f402eab0ada038d3de2ad753a40f9f441715d08058d890537146bb0aba11c8e</span><br><span class="line">serving-cmd-networking-certmanager</span><br><span class="line">gcr.io/knative-releases/github.com/knative/serving/cmd/networking/certmanager@sha256:dc77db09a23103f64a554de4e01cfda7371cbb13bc0954c991bdc4141169257f</span><br><span class="line">serving-cmd-networking-istio</span><br><span class="line">gcr.io/knative-releases/github.com/knative/serving/cmd/networking/istio@sha256:55fe9eeacfc20d97d3cd4f80bfc8a9b95cff7b5c50121bda87f754da8f05e57b</span><br><span class="line">serving-cmd-webhook</span><br><span class="line">gcr.io/knative-releases/github.com/knative/serving/cmd/webhook@sha256:f0f98736bd4b55354f447f59183bf26b9be1ab01691b8b4aeee85caeb1166562</span><br><span class="line">eventing-sources-cmd-github_receive_adapter</span><br><span class="line">gcr.io/knative-releases/github.com/knative/eventing-sources/cmd/github_receive_adapter@sha256:b5d6e12d16d16c6c42ae3d4325a1ef3a8a129dfc97740aa28000c0867edfc4ff</span><br><span class="line">eventing-sources-cmd-manager</span><br><span class="line">gcr.io/knative-releases/github.com/knative/eventing-sources/cmd/manager@sha256:99cf1f559f74ae97f271632697ed6e78a3fdd88a155632a57341b0dd6eab6581</span><br><span class="line">k8s.gcr.io-addon-resizer-1.7</span><br><span class="line">k8s.gcr.io/addon-resizer:1.7</span><br><span class="line">k8s.gcr.io-elasticsearch-v5.6.4</span><br><span class="line">k8s.gcr.io/elasticsearch:v5.6.4</span><br><span class="line">k8s.gcr.io-fluentd-elasticsearch-v2.0.4</span><br><span class="line">k8s.gcr.io/fluentd-elasticsearch:v2.0.4</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>中间建立github与dockerhub进行自动化构建</p><p>最后的操作即将5个yaml文件中的gcr.io镜像替换为我在dockerhub的镜像即可</p><p>##yaml文件替换shell脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cat &gt;sedyaml.sh &lt;&lt; EOF</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">WORKDIR=/root/knative</span><br><span class="line">cd $WORKDIR;</span><br><span class="line"></span><br><span class="line">for ((i=1;i&lt;54;i=i+2))</span><br><span class="line">do</span><br><span class="line">newimagetag=$(cat image.txt | sed -n &quot;$&#123;i&#125;p&quot;)</span><br><span class="line">a=$(($i+1))</span><br><span class="line">oldimage=$(cat image.txt  | sed -n &quot;$&#123;a&#125;p&quot;)</span><br><span class="line"></span><br><span class="line">for yaml in $(ls *.yaml)</span><br><span class="line">do</span><br><span class="line">  sed -i  &#x27;s#&#x27;$&#123;oldimage&#125;&#x27;#huisebug/knative:&#x27;$&#123;newimagetag&#125;&#x27;#g&#x27; $&#123;yaml&#125;</span><br><span class="line">done</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>最终效果图<br><img src="/2019/05/23/knative/media/knative%E9%9B%86%E7%BE%A4pod%E5%9B%BE.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文是在基于官方knative的基础上进行安装，因为官方yaml中涉及太多的gcr.io镜像和配置文件中也包含gcr.io，便于国内用户进行研究学习而进行替换为可拉取镜像的yaml文件。其中主要是替换镜像的操作。&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes" scheme="https://huisebug.github.io/categories/Kubernetes/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="istio" scheme="https://huisebug.github.io/tags/istio/"/>
    
    <category term="knative" scheme="https://huisebug.github.io/tags/knative/"/>
    
    <category term="python" scheme="https://huisebug.github.io/tags/python/"/>
    
    <category term="shell" scheme="https://huisebug.github.io/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>heketi安装结合EFK实践</title>
    <link href="https://huisebug.github.io/2019/05/08/gluster-heketi-efk/"/>
    <id>https://huisebug.github.io/2019/05/08/gluster-heketi-efk/</id>
    <published>2019-05-08T09:04:01.000Z</published>
    <updated>2021-07-07T08:50:04.054Z</updated>
    
    <content type="html"><![CDATA[<p>这是一个k8s集群，搭建gluster fs系统提供存储服务，搭建heketi进行管理gluster fs，结合k8s的StorageClass进行动态pv建立。提供给生产环境的日志收集系统EFK存储数据。</p><span id="more"></span><h1 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h1><ul><li>  操作系统：centos7</li><li>  硬盘：/dev/vdc 50Gi</li></ul><h1 id="GlusterFS"><a href="#GlusterFS" class="headerlink" title="GlusterFS"></a>GlusterFS</h1><p>搭建glusterfs来作为可持续存储k8s的CSI</p><h2 id="安装glusterfs"><a href="#安装glusterfs" class="headerlink" title="安装glusterfs"></a>安装glusterfs</h2><p><font color="red" size="3">三台服务器都要执行</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 先安装 gluster 源</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> yum install centos-release-gluster -y</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装 glusterfs 组件（这里包含了server和client）</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建 glusterfs服务运行目录</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir /opt/glusterd</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改 glusterd 目录，将/var/lib改成/opt</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sed -i <span class="string">&#x27;s/var/lib/opt/g&#x27;</span> /etc/glusterfs/glusterd.vol</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动 glusterfs（为挂载提供服务）</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl start glusterd.service</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置开机启动</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl <span class="built_in">enable</span> glusterd.service</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">查看状态</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl status glusterd.service</span></span><br></pre></td></tr></table></figure><h2 id="配置-glusterfs"><a href="#配置-glusterfs" class="headerlink" title="配置 glusterfs"></a>配置 glusterfs</h2><p><font color="red" size="3">三台服务器都要执行</font></p><p>配置本地解析文件hosts</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;&quot;&quot;</span><br><span class="line">192.168.1.49 api.328ym.com</span><br><span class="line">192.168.1.55 node1.328ym.com</span><br><span class="line">192.168.1.100 node2.328ym.com</span><br><span class="line">&quot;&quot;&quot; &gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><p>开放端口（24007是gluster服务运行所需的端口号）如果关闭了防火墙就省略此步操作。其他防火墙设置自己解决</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -I INPUT -p tcp --dport 24007 -j ACCEPT</span><br></pre></td></tr></table></figure><p>搭建完毕glusterFS</p><h1 id="heketi"><a href="#heketi" class="headerlink" title="heketi"></a>heketi</h1><p>本项目参考github地址：<br><a href="https://github.com/huisebug/heketi.git">https://github.com/huisebug/heketi.git</a></p><h2 id="安装客户端"><a href="#安装客户端" class="headerlink" title="安装客户端"></a>安装客户端</h2><p>下载地址：<br><a href="https://github.com/heketi/heketi/releases/">https://github.com/heketi/heketi/releases/</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar zxf heketi-client-v9.0.0.linux.amd64.tar.gz</span><br><span class="line">mv heketi-client/bin/heketi-cli /usr/local/bin/</span><br></pre></td></tr></table></figure><h2 id="创建ssh-key并分发"><a href="#创建ssh-key并分发" class="headerlink" title="创建ssh key并分发"></a>创建ssh key并分发</h2><p>在所有的glusterfs节点，创建hekeli的数据库存储目录、ssh免密码登录文件目录。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /data/heketi/&#123;db,.ssh&#125; &amp;&amp; chmod 700 /data/heketi/.ssh</span><br><span class="line">ssh-keygen -t rsa -b 2048 -f /data/heketi/.ssh/id_rsa</span><br></pre></td></tr></table></figure><p>如果执行操作的节点可以免密登录到其他节点就可以执行下面命令，不行就手动分发</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for NODE in node1 node2 node3; do scp -r /data/heketi/.ssh root@$&#123;NODE&#125;:/data/heketi; done</span><br><span class="line">for NODE in node1 node2 node3; do ssh-copy-id -i /data/heketi/.ssh/id_rsa.pub root@$&#123;NODE&#125; ; done</span><br></pre></td></tr></table></figure><p>手动分发</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /data/heketi/.ssh/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><h2 id="运行heketi"><a href="#运行heketi" class="headerlink" title="运行heketi"></a>运行heketi</h2><p>运行原理</p><p>Heketi服务使用建立的ssh key登录到gluster fs服务器的root账户，然后就可以执行gluster fs服务进行管理。</p><p>需要三个文件</p><ol><li> heketi-deployment.yaml：heketi服务运行yaml，其中需要声明登录到glusterfs的方式，<br><img src="/2019/05/08/gluster-heketi-efk/media/0cd50605f6fe88c9a2d54dcd601c5458.png"><br>需要指定ssh登录用户，此处我是root；ssh服务端口号，此处我未使用默认的22端口<br><img src="/2019/05/08/gluster-heketi-efk/media/a0c22912d94767400af35f22f8d5ccb3.png"><br>此处指定了node进行部署，所以记得修改你的节点特有的标签<br><img src="/2019/05/08/gluster-heketi-efk/media/e129a76f00b5b9ffb3261b8eff2b344f.png"><br>之前在所有运行gluster fs的服务器建立了/data/heketi/文件目录，这里指定其中节点运行就都可以使用hostPath方式volume，将生成的ssh key挂载到heketi服务中，让其可以远程发送命令操作gluster fs所在服务器，进行pv、vg管理（概念参考地址：<a href="https://www.cnblogs.com/zk47/p/4753987.html">https://www.cnblogs.com/zk47/p/4753987.html</a>）</li><li>heketi-secret.yaml：这是设置登录到heketi服务的密码</li><li>heketi-svc.yaml：因为我们需要使用客户端heketi-cli进行集群的创建，所以这里需要使用NodePort的方式进行访问</li></ol><p>执行安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f heketi-deployment.yaml -f heketi-secret.yaml</span><br></pre></td></tr></table></figure><p><img src="/2019/05/08/gluster-heketi-efk/media/e34c65fa09a2506ef24da09089309f92.png"></p><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>这里配置Heketi的service在NodeIP：30944上<br>通过命令检查heketi服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -s &lt;http://api.328ym.com:30944/hello&gt;</span><br></pre></td></tr></table></figure><p><img src="/2019/05/08/gluster-heketi-efk/media/eeac2c24a4fd82c65ec0c40451e706ca.png"></p><h2 id="导入glusterfs集群拓扑（topology）信息"><a href="#导入glusterfs集群拓扑（topology）信息" class="headerlink" title="导入glusterfs集群拓扑（topology）信息"></a>导入glusterfs集群拓扑（topology）信息</h2><h3 id="文件内容heketi-topology-vdc-json"><a href="#文件内容heketi-topology-vdc-json" class="headerlink" title="文件内容heketi-topology-vdc.json"></a>文件内容heketi-topology-vdc.json</h3><p>参考heketi-topology-vdc.json文件地址：<br><a href="https://raw.githubusercontent.com/huisebug/heketi/master/heketi-topology-vdc.json">https://raw.githubusercontent.com/huisebug/heketi/master/heketi-topology-vdc.json</a></p><ul><li>hostnames下的manage和storage配置为gluster的IP地址；</li><li>zone可以配置为非0值，这里配置为1,；</li><li>devices下制定gluster fs服务器下的文件驱动，例如：/dev/sdb，指定多个文件驱动！！！</li><li>destroydata:是否销毁文件驱动的数据，这里使用了true</li></ul><p>有关glusterfs集群的topology的配置参考<br><a href="https://github.com/heketi/heketi/blob/master/docs/admin/topology.md">https://github.com/heketi/heketi/blob/master/docs/admin/topology.md</a></p><h3 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">heketi-cli --user admin --secret password --server http://api.328ym.com:30944 topology load --json heketi-topology-vdc.json</span><br></pre></td></tr></table></figure><p>查看heketi服务日志（kubectl logs -f heketi-7749795dc7-k6qnb）如果提示如下错误</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[sshexec] ERROR 2019/04/28 07:41:01</span><br><span class="line">/src/github.com/heketi/heketi/pkg/utils/ssh/ssh.go:172: Failed to run command[/bin/bash -c &#x27;pvcreate --metadatasize=128M --dataalignment=256K &#x27;/dev/vdc&#x27;&#x27;] on192.168.1.55:36888: Err[Process exited with status 5]: Stdout []: Stderr[WARNING: dos signature detected on /dev/vdc at offset 510. Wipe it? [y/n]: [n]</span><br><span class="line"></span><br><span class="line">Aborted wiping of dos.</span><br><span class="line"></span><br><span class="line">1 existing signature left on the device.</span><br></pre></td></tr></table></figure><p>报错原因是无法手动向警告提示输入y或n,解决方法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">那么就手动到所有gluster fs服务的命令行手动建立pv</span></span><br><span class="line">pvcreate --metadatasize=128M --dataalignment=256K &#x27;/dev/vdc&#x27;</span><br></pre></td></tr></table></figure><p>执行后，每个节点出现ok就说明成功建立，可以使用vgscan命令进行查看。<br><img src="/2019/05/08/gluster-heketi-efk/media/56be8763bef7e833a452b30b66aa9d8f.png"></p><p>查看集群id</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">heketi-cli --user admin --secret password --server http://api.328ym.com:30944 cluster list</span><br></pre></td></tr></table></figure><p><img src="/2019/05/08/gluster-heketi-efk/media/8835df6c9d68d483a04655e492a2ad2b.png"></p><h1 id="建立volume"><a href="#建立volume" class="headerlink" title="建立volume"></a>建立volume</h1><h2 id="非k8s使用"><a href="#非k8s使用" class="headerlink" title="非k8s使用"></a>非k8s使用</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">heketi-cli --user admin --secret password --server http://10.142.21.21:30088 volume create --size=100 --replica=3 --clusters=38c2729af0f42338cf66eed6b80f116f</span><br></pre></td></tr></table></figure><h2 id="k8s使用"><a href="#k8s使用" class="headerlink" title="k8s使用"></a>k8s使用</h2><p>此处使用动态PV的方式，建立storageclass和pvc进行关联，然后动态建立pv</p><h3 id="建立StorageClass"><a href="#建立StorageClass" class="headerlink" title="建立StorageClass"></a>建立StorageClass</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: gluster-heketi</span><br><span class="line">  annotations:</span><br><span class="line"><span class="meta">#</span><span class="bash">这里是将次storageclass设置为默认的class</span></span><br><span class="line">    storageclass.kubernetes.io/is-default-class: &quot;true&quot;</span><br><span class="line">provisioner: kubernetes.io/glusterfs</span><br><span class="line">parameters:</span><br><span class="line"><span class="meta">#</span><span class="bash">注意这里下面的地址需要能解析的到，我之前写的是service名，是无法解析的</span></span><br><span class="line">  resturl: &quot;http://api.328ym.com:30944&quot;</span><br><span class="line"><span class="meta">  #</span><span class="bash">resturl: <span class="string">&quot;http://heketi.default.svc.cluster.local:8080&quot;</span></span></span><br><span class="line">  clusterid: &quot;38c2729af0f42338cf66eed6b80f116f&quot;</span><br><span class="line">  restuser: &quot;admin&quot;</span><br><span class="line">  secretNamespace: &quot;default&quot;</span><br><span class="line">  secretName: &quot;heketi-secret&quot;</span><br><span class="line">  volumetype: &quot;none&quot;</span><br></pre></td></tr></table></figure><p>执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f heketi-storageclass.yaml</span><br></pre></td></tr></table></figure><h3 id="建立pvc"><a href="#建立pvc" class="headerlink" title="建立pvc"></a>建立pvc</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: test-claim</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: gluster-heketi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br></pre></td></tr></table></figure><p>执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f heketi-test-pvc.yaml</span><br></pre></td></tr></table></figure><p>查看volume建立情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">heketi-cli --user admin --secret password --server http://api.328ym.com:30944 volume list</span><br></pre></td></tr></table></figure><p><img src="/2019/05/08/gluster-heketi-efk/media/900652bd5bcfa3fd2616833b624ba825.png"></p><p>查看是否建立pvc和自动建立pv<br><img src="/2019/05/08/gluster-heketi-efk/media/05fd955b3a7c9fa630b2d53827fd4196.png"></p><p>查看gluster fs 卷信息和挂载情况<br><img src="/2019/05/08/gluster-heketi-efk/media/629944482c78b0a4a1d17e63a355d443.png"></p><p>查看卷的使用情况和集群情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">heketi-cli --user admin --secret password --server http://api.328ym.com:30944 topology info</span><br></pre></td></tr></table></figure><p><img src="/2019/05/08/gluster-heketi-efk/media/43aed50dfec86966e2b3a6add7481d2f.png"></p><h4 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h4><p>###实践###</p><p>此处我们的gluster fs是使用了三块50Gi的硬盘组成的集群，验证当申请超过50Gi的时候会如何建立，能否建立成功？</p><ol><li> 我们复制之前的test-pvc 文件，将其修改为如下<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp -rf heketi-test-pvc.yaml heketi-test-pvc2.yaml</span><br></pre></td></tr></table></figure>heketi-test-pvc2.yaml<br><img src="/2019/05/08/gluster-heketi-efk/media/f300b6763104152518a88fdca4bf35e5.png"></li></ol><p>执行<br><img src="/2019/05/08/gluster-heketi-efk/media/dbd5f81c5583ceb2f6da81d02f9ede9b.png"></p><p>查看挂载情况<br><img src="/2019/05/08/gluster-heketi-efk/media/15e98081d9555131c50e70bb7c45946e.png"></p><p>难道说这里使用了30Gi，会平均从其他gluster节点划取空间？</p><p>查看gluster fs信息<br><img src="/2019/05/08/gluster-heketi-efk/media/f44ef1ae11314a578a70bfee826a5c25.png"></p><p>获取到另一个来源是192.168.1.55这台服务器，切换到这台服务器进行查看<br><img src="/2019/05/08/gluster-heketi-efk/media/00e976b33bb64b4e61c963bde7015fe6.png"></p><p>果然证实了我的猜测。</p><p>查看pvc和pv<br><img src="/2019/05/08/gluster-heketi-efk/media/a897acabf12b1880df044ee352d7d67a.png"></p><p>删除pvc配置<br><img src="/2019/05/08/gluster-heketi-efk/media/6cdb312b57897f5b6820d0d4fa207d7c.png"></p><p>pv也会自动删除<br><img src="/2019/05/08/gluster-heketi-efk/media/ec91e528428e072ba9b091ed2b7aa628.png"></p><h1 id="EFK"><a href="#EFK" class="headerlink" title="EFK"></a>EFK</h1><p>作为整个集群的日志系统，我们可以使用三台性能优越的服务器安装好kubelet、kube-proxy、docker后将其作为node加入到现有的集群中去，并将其打上efk的标签，不允许其他pod调度到服务器上面。</p><p>列如:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl label node efk1.huisebug.com efknode=efk</span><br><span class="line">kubectl label node efk2.huisebug.com efknode=efk</span><br><span class="line">kubectl label node efk3.huisebug.com efknode=efk</span><br></pre></td></tr></table></figure><p>我这里就不这样进行操作了，还是将efk服务安装到整个集群中</p><h2 id="yaml文件下载地址"><a href="#yaml文件下载地址" class="headerlink" title="yaml文件下载地址"></a>yaml文件下载地址</h2><p>参考我的git地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/huisebug/EFK.git</span><br></pre></td></tr></table></figure><h2 id="ElasticSearch"><a href="#ElasticSearch" class="headerlink" title="ElasticSearch"></a>ElasticSearch</h2><ul><li>ES的官网推荐，不太推荐使用分布式文件系统（NFS/GlusterFS等）来进行数据的存储，对ES的性能会造成很大的影响。</li><li>这里我就不使用之前搭建的GlusterFS系统了。使用 local-storage（1.9版本引入，将本地存储以PV形式提供给容器进行使用，实现存储空间的管理）作为storageClassName，PVC概念参考官方网址<br><a href="https://www.kubernetes.org.cn/pvpvcstorageclass">https://www.kubernetes.org.cn/pvpvcstorageclass</a></li></ul><h3 id="local-storage"><a href="#local-storage" class="headerlink" title="local-storage"></a>local-storage</h3><p><font color="red" size="3">三台服务器都要执行</font></p><h4 id="新加硬盘"><a href="#新加硬盘" class="headerlink" title="新加硬盘"></a>新加硬盘</h4><p>所以此处我们新加硬盘来建立local-storage</p><p>我这里是虚拟机，可以直接增加新硬盘SCSI接口硬盘进行热插拔，不重启系统，刷新出硬盘</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls /sys/class/scsi_host/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&quot;- - -&quot;</span> &gt; /sys/class/scsi_host/host0/scan</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&quot;- - -&quot;</span> &gt; /sys/class/scsi_host/host1/scan</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&quot;- - -&quot;</span> &gt; /sys/class/scsi_host/host2/scan</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> fdisk -l</span></span><br></pre></td></tr></table></figure><p><img src="/2019/05/08/gluster-heketi-efk/media/80781fb065626438799a77bd10bdc213.png"></p><p>上面的sdc硬盘20G就是我新加的硬盘</p><h4 id="分区格式化并挂载到各宿主机的-data目录"><a href="#分区格式化并挂载到各宿主机的-data目录" class="headerlink" title="分区格式化并挂载到各宿主机的/data目录"></a>分区格式化并挂载到各宿主机的/data目录</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> fdisk -l /dev/sdc</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 建立分区并格式化为xfs类型</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> fdisk /dev/sdc</span></span><br><span class="line">1.  n 建立新分区</span><br><span class="line">2.  p 建立主分区</span><br><span class="line">3.  1 分区号为1</span><br><span class="line">4.  一直回车默认，将整块磁盘建立为一个分区，使用全部空间</span><br><span class="line">5.  w 保存退出</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使分区生效</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> partprobe /dev/sdc</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 强制格式化为xfs</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkfs.xfs -f /dev/sdc1</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 手动将/dev/sdc1分区挂载到/data</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir /data &amp;&amp; mount /dev/sdc1 /data</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 开机自动挂载</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&quot;/dev/sdc1 /data xfs defaults 0 0&quot;</span> &gt;&gt; /etc/fstab</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看挂载状态</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> df -hT</span></span><br></pre></td></tr></table></figure><h4 id="建立相应的PV"><a href="#建立相应的PV" class="headerlink" title="建立相应的PV"></a>建立相应的PV</h4><p>参考地址：<br><a href="https://github.com/huisebug/EFK/blob/master/efkyaml/es-pv-api.yaml">https://github.com/huisebug/EFK/blob/master/efkyaml/es-pv-api.yaml</a><br><a href="https://github.com/huisebug/EFK/blob/master/efkyaml/es-pv-node1.yaml">https://github.com/huisebug/EFK/blob/master/efkyaml/es-pv-node1.yaml</a><br><a href="https://github.com/huisebug/EFK/blob/master/efkyaml/es-pv-node2.yaml">https://github.com/huisebug/EFK/blob/master/efkyaml/es-pv-node2.yaml</a></p><p>注意：存储大小，别超过实际大小；指定正确的存储类型；指定正确的本地路径；指定连接的服务器主机名</p><ul><li>accessModes:<pre><code>- ReadWriteOnce 注意这里定义的访问模式是单个节点读写。即一个pv只给一个pvc使用，像我们这里是3个pv，那么只能有3个pod调用pv，就算定义为ReadWriteMany也是如此。</code></pre></li><li>  回收策略persistentVolumeReclaimPolicy: Retain保留<br>该Retain回收政策允许资源的回收手册。当PersistentVolumeClaim删除时，PersistentVolume仍然存在，并且该卷被视为“已释放”。但它尚未提供另一项索赔，因为之前的索赔人的数据仍在数量上。管理员可以使用以下步骤手动回收卷。</li></ul><ol><li> 删除PersistentVolume。删除PV后，外部基础架构（例如AWS EBS，GCEPD，Azure磁盘或Cinder卷）中的关联存储资产仍然存在。</li><li> 相应地手动清理相关存储资产上的数据。</li><li> 手动删除关联的存储资产，或者如果要重用同一存储资产，请PersistentVolume使用存储资产定义创建新的存储资产。</li></ol><p>建立pv</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f es-pv-api.yaml -f es-pv-node1.yaml -f es-pv-node2.yaml</span><br></pre></td></tr></table></figure><p><img src="/2019/05/08/gluster-heketi-efk/media/3c40a922669b09615e630bad14aa0c1c.png"></p><h3 id="Elasticsearch安装"><a href="#Elasticsearch安装" class="headerlink" title="Elasticsearch安装"></a>Elasticsearch安装</h3><p>Elasticsearch最佳实践建议将节点分成三个角色：</p><ul><li>  Master 节点 - 仅用于集群管理，无数据，无HTTP API</li><li>  Data 节点 - 用于客户端使用和数据</li><li>  Ingest 节点 - 用于摄取期间的文档预处理<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Master节点</span><br><span class="line">es-master-discovery-svc.yaml  </span><br><span class="line">es-master-pdb.yaml  </span><br><span class="line">es-master-stateful.yaml  </span><br><span class="line">es-master-svc.yaml</span><br><span class="line">Data节点</span><br><span class="line">es-data-pdb.yaml  </span><br><span class="line">es-data-stateful.yaml</span><br><span class="line">es-svc.yaml</span><br><span class="line">Ingest 节点</span><br><span class="line">es-ingest-deployment.yaml  </span><br><span class="line">es-ingest-svc.yaml</span><br></pre></td></tr></table></figure></li></ul><p>（非常）重要说明</p><ul><li>  Elasticsearch pod需要init-container以特权模式运行，因此它可以设置一些VM选项。为此，kubelet应该使用args运行–allow-privileged，否则init-container将无法运行。</li><li>  默认情况下，ES_JAVA_OPTS设置为-Xms256m -Xmx256m。这是一个非常低的值，但许多用户，即minikube用户，由于主机内存不足而导致pod被杀的问题。可以在此存储库中可用的部署描述符中更改此设置。此处我的data就修改为了1024m,如果报错，请根据日志信息（kubectllogs es-data-0 -n efk）获取的日志信息来进行调整。</li><li>  目前，Kubernetes pod描述符emptyDir用于在每个数据节点容器中存储数据。这是为了简单起见，应根据一个人的存储需求进行调整。</li><li>  statefulset包含部署数据豆荚作为一个例子StatefulSet。这些使用一个volumeClaimTemplates为每个pod配置持久存储。此处已经说明了pv建立时模式是单节点的，所以可根据自己需求来进行调整两个statefulset的replicas数量（此处我是data1个 master 2个。）</li><li>  默认情况下，PROCESSORS设置为1。对于某些部署，这可能是不够的，尤其是在启动时。根据需要调整resources.limits.cpu和或livenessProbe相应。请注意，resources.limits.cpu必须是整数。</li><li>  elasticsearch的服务端和访问的客户端版本必须一致，不然会导致elasticsearch服务停掉。列如我这里对应版本为<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">   elasticsearch:6.3.2</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">   fluentd-elasticsearch:2.4.0</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">   fluent-bit:0.13.2</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">   kibana:6.3.2</span></span><br></pre></td></tr></table></figure><font color="red" size="6">常见问题</font></li></ul><p>为什么NUMBER_OF_MASTERS与master-replicas的数量不同？</p><ul><li>之前master-replicas的数量是3，因为pv的模式问题，我修改为了2;</li><li>此环境变量的默认值为2，表示群集至少需要2个主节点才能运行。如果一个集群有3个主服务器并且有一个管理器死亡，则集群仍可正常工通常是最小主节点n/2 +1，其中n是群集中主节点的数量。如果一个集群有5个主节点，则一个节点应该至少有3个节点，小于该节点并且集群停止。如果缩放主数量，请确保通过Elasticsearch API更新主节点的最小数量，因为设置环境变量仅适用于群集设置。</li></ul><h3 id="检查是否成功建立"><a href="#检查是否成功建立" class="headerlink" title="检查是否成功建立"></a>检查是否成功建立</h3><p>pv和pvc</p><p><img src="/2019/05/08/gluster-heketi-efk/media/1651a5b8c5412e6195d82bdc2388fa48.png"></p><p>pod</p><p><img src="/2019/05/08/gluster-heketi-efk/media/c962cc652b80f1d70e78588f05431745.png"></p><h3 id="Clean-up-with-Curator"><a href="#Clean-up-with-Curator" class="headerlink" title="Clean-up with Curator"></a>Clean-up with Curator</h3><p>主要用于清理elasticsearch超过天数的数据，更多高级用法参考官方网站<br><a href="https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html">https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html</a></p><h4 id="创建curator"><a href="#创建curator" class="headerlink" title="创建curator"></a>创建curator</h4><p>使用到的yaml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">es-curator-configmap.yaml </span><br><span class="line">es-curator-cronJob.yaml</span><br></pre></td></tr></table></figure><p>我们将其设置为一个定时任务，每天1点整进行清理超过3天的数据</p><h2 id="Kibana"><a href="#Kibana" class="headerlink" title="Kibana"></a>Kibana</h2><ul><li>  Kibana是一个开源的分析和可视化平台，设计用于和Elasticsearch一起工作。</li><li>  你用Kibana来搜索，查看，并和存储在Elasticsearch索引中的数据进行交互。</li><li>  你可以轻松地执行高级数据分析，并且以各种图标、表格和地图的形式可视化数据。</li><li>  Kibana使得理解大量数据变得很容易。它简单的、基于浏览器的界面使你能够快速创建和共享动态仪表板，实时显示Elasticsearch查询的变化。</li></ul><h3 id="使用到的yaml文件"><a href="#使用到的yaml文件" class="headerlink" title="使用到的yaml文件"></a>使用到的yaml文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kibana-configmap.yaml </span><br><span class="line">kibana-deployment.yaml </span><br><span class="line">kibana-ingress.yaml </span><br><span class="line">kibana-svc.yaml</span><br></pre></td></tr></table></figure><h3 id="kibana的汉化"><a href="#kibana的汉化" class="headerlink" title="kibana的汉化"></a>kibana的汉化</h3><p>kibana的汉化方式，已经放到了Docker目录下，也可以使用我已经汉化好的镜像</p><h3 id="验证效果"><a href="#验证效果" class="headerlink" title="验证效果"></a>验证效果</h3><p><img src="/2019/05/08/gluster-heketi-efk/media/bd690c08153c19412518a92c05a2b707.png"></p><p><img src="/2019/05/08/gluster-heketi-efk/media/fff34a31b356199cecb711511bed9074.png"></p><h2 id="Fluent"><a href="#Fluent" class="headerlink" title="Fluent"></a>Fluent</h2><table><thead><tr><th><strong>组件</strong></th><th><strong>用途</strong></th></tr></thead><tbody><tr><td>Fluent Bit</td><td>拉起在每台宿主机上采集宿主机上的容器日志。（Fluent Bit 比较新一些，但是资源消耗比较低，性能比Fluentd好一些，但稳定性有待于进一步提升）</td></tr><tr><td>Fluentd</td><td>两个用途：1 以日志收集中转中心角色拉起，Deployment部署模式；2 在部分Fluent Bit无法正常运行的主机上，以Daemon Set模式运行采集宿主机上的日志，并发送给日志收集中转中心</td></tr><tr><td>ElasticSearch</td><td>用来接收日志收集中转中心发送过来的日志，并通过Kibana分析展示出来，鉴于硬件资源有限，仅保留一周左右的数据。</td></tr><tr><td>Amazon S3（fluentd server）</td><td>用来接收日志收集中转中心发送过来的日志，对日志进行压缩归档，也可后续使用Spark进行进一步大数据分析。</td></tr></tbody></table><h3 id="fluentd直接传递es建立"><a href="#fluentd直接传递es建立" class="headerlink" title="fluentd直接传递es建立"></a>fluentd直接传递es建立</h3><p>此处我们演示fluentd直接向elasticsearch传递数据建立使用，使用到的yaml分别如下<br>fluentd</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fluentd-es-configmap.yaml </span><br><span class="line">fluentd-es-daemonset.yaml </span><br></pre></td></tr></table></figure><h4 id="给-Node-设置标签"><a href="#给-Node-设置标签" class="headerlink" title="给 Node 设置标签"></a>给 Node 设置标签</h4><p>定义fluentd-es-daemonset.yaml时设置了nodeSelector beta.kubernetes.io/fluentd-ds-ready=true,所以需要在期望运⾏fluentd 的 Node上设置该标签；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i in api.huisebug.com node1.huisebug.com node2.huisebug.com; do kubectl label nodes $i beta.kubernetes.io/fluentd-ds-ready=true; done</span><br></pre></td></tr></table></figure><p>我们这里是新建一个namespace:efk,所以建立fluentd-es-daemonset.yaml不能使用pod优先级priorityClassName:system-node-critical，需要注释掉。</p><p>查看是否成功建立<br><img src="/2019/05/08/gluster-heketi-efk/media/6e612c350a890843a2ce65d5669ac0be.png"></p><h4 id="验证整个EFK效果"><a href="#验证整个EFK效果" class="headerlink" title="验证整个EFK效果"></a>验证整个EFK效果</h4><p>添加kibana检测到fluent传递给es的日志信息，并建立索引，如果整个集群没有建立成功，是不会自动出现如下界面的。<br><img src="/2019/05/08/gluster-heketi-efk/media/1cf4f7d8a5825ddfa7f2d3dc077c2ca1.png"><br><img src="/2019/05/08/gluster-heketi-efk/media/3c50a0817eaa90a897007390f5db2c0a.png"><br><img src="/2019/05/08/gluster-heketi-efk/media/468f9ba86fc233a8b61b81f1c0ce8c6c.png"><br><img src="/2019/05/08/gluster-heketi-efk/media/f9de546baaaa48061282b11c46797b6e.png"><br><img src="/2019/05/08/gluster-heketi-efk/media/e5fa8cf07a03894e139bb2a17299a6d1.png"></p><h3 id="fluentd传递S3然后传递es"><a href="#fluentd传递S3然后传递es" class="headerlink" title="fluentd传递S3然后传递es"></a>fluentd传递S3然后传递es</h3><h4 id="Amazon-S3镜像建立"><a href="#Amazon-S3镜像建立" class="headerlink" title="Amazon S3镜像建立"></a>Amazon S3镜像建立</h4><p>基于官方镜像添加对S3的支持，可在Docker目录下查找到Dockerfile,参考地址：<br><a href="https://github.com/huisebug/EFK/blob/master/fluentd/Docker/Dockerfile">https://github.com/huisebug/EFK/blob/master/fluentd/Docker/Dockerfile</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM huisebug/sec_re:fluentd-elasticsearch-2.4.0</span><br><span class="line">RUN \</span><br><span class="line">    apt-get update -y &amp;&amp; apt-get install ruby-dev -y &amp;&amp; \</span><br><span class="line">    gem install fluent-plugin-s3 &amp;&amp; \</span><br><span class="line">    apt-get clean</span><br></pre></td></tr></table></figure><p>也可以直接使用我已经上传到dockerhub的镜像</p><p>使用到的yaml文件</p><p>fluentd</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fluentd-es-f-configmap.yaml  </span><br><span class="line">fluentd-es-f-daemonset.yaml</span><br></pre></td></tr></table></figure><p>S3</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fluentd-server-s3-configmap.yaml  </span><br><span class="line">fluentd-server-s3-deployment.yaml</span><br></pre></td></tr></table></figure><p>其中的fluentd-es-f-daemonset.yaml与之前的fluentd-es-daemonset.yaml内容相同，这里我为了便于区分。</p><p>fluentd-es-f-configmap.yaml 内容变更如下：</p><ul><li>  调整output.conf。</li><li>  移除ES片段。</li><li>  添加forward片段。</li></ul><p><font color="red" size="5">注意！！！</font></p><p>为了不影响测试效果，我们需要删除es建立的数据，并重新建立,切换到es的yaml文件所在目录<br>api.huisebug.com主机：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -f . &amp;&amp; rm -rf /data/*</span><br></pre></td></tr></table></figure><p>其余节点主机：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /data/*</span><br></pre></td></tr></table></figure><p>并且删除之前直接传递建立的fluentd</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -f fluentd-es-configmap.yaml -f fluentd-es-daemonset.yaml</span><br></pre></td></tr></table></figure><p>查看效果<br><img src="/2019/05/08/gluster-heketi-efk/media/b63dea69786594266a83dedb8dbb6fc8.png"></p><p>访问kibana验证<br><img src="/2019/05/08/gluster-heketi-efk/media/92d32089d7652845cf6a29131b802b55.png"></p><h3 id="fluentd-bit传递到S3然后传递es"><a href="#fluentd-bit传递到S3然后传递es" class="headerlink" title="fluentd bit传递到S3然后传递es"></a>fluentd bit传递到S3然后传递es</h3><p>使用到的yaml文件</p><p>fluentd-bit</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fluentd-bit-es-f-configmap.yaml  </span><br><span class="line">fluentd-bit-es-f-daemonset.yaml</span><br></pre></td></tr></table></figure><p>S3</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fluentd-bit-server-s3-configmap.yaml  </span><br><span class="line">fluentd-bit-server-s3-deployment.yaml</span><br></pre></td></tr></table></figure><p>其中的S3使用的yaml内容与之前的内容相同，这里我为了便于区分。</p><p>注意！！！</p><p>为了不影响测试效果，我们需要删除es建立的数据，并重新建立,切换到es的yaml文件所在目录</p><p>api.huisebug.com主机：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -f . &amp;&amp; rm -rf /data/*</span><br></pre></td></tr></table></figure><p>其余节点主机：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /data/*</span><br></pre></td></tr></table></figure><p>并且删除之前直接传递建立的fluentd-s3</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -f fluentd-es-f-configmap.yaml -f fluentd-es-f-daemonset.yaml</span><br></pre></td></tr></table></figure><p>查看效果<br><img src="/2019/05/08/gluster-heketi-efk/media/da78835eb4971c148ce076b23dd94ced.png"></p><h1 id="GlusterFS-Heketi-EFK"><a href="#GlusterFS-Heketi-EFK" class="headerlink" title="GlusterFS+Heketi+EFK"></a>GlusterFS+Heketi+EFK</h1><p>适用于生产环境</p><p>参考yaml文件</p><p><a href="https://github.com/huisebug/GlusterFS-Heketi-EFK.git">https://github.com/huisebug/GlusterFS-Heketi-EFK.git</a></p><p>三个文件夹，其他文件夹目录内容不变，这里主要需要修改<a href="https://github.com/huisebug/GlusterFS-Heketi-EFK/tree/master/ekheketi">ekheketi</a>文件中的内容</p><h2 id="ekheketi"><a href="#ekheketi" class="headerlink" title="ekheketi"></a>ekheketi</h2><hr><p>将之前的ElasticSearch服务的三个角色分类文件目录存放，修改statefulset使用到的pvc的为storageClassName:<br>gluster-heketi，如下<br><img src="/2019/05/08/gluster-heketi-efk/media/9fd8e80a42eb7024c7dbb992451022db.png"></p><p>即可完成！！！</p><p>此处整个efk集群使用到pvc的有es-master和es-data。</p><p>搭建完成后查看是否成功<br><img src="/2019/05/08/gluster-heketi-efk/media/20db3e1e73d82ec051463eddfe81345c.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这是一个k8s集群，搭建gluster fs系统提供存储服务，搭建heketi进行管理gluster fs，结合k8s的StorageClass进行动态pv建立。提供给生产环境的日志收集系统EFK存储数据。&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes" scheme="https://huisebug.github.io/categories/Kubernetes/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="heketi" scheme="https://huisebug.github.io/tags/heketi/"/>
    
    <category term="elasticsearch" scheme="https://huisebug.github.io/tags/elasticsearch/"/>
    
    <category term="gluster" scheme="https://huisebug.github.io/tags/gluster/"/>
    
    <category term="fluentd" scheme="https://huisebug.github.io/tags/fluentd/"/>
    
    <category term="fluent Bit" scheme="https://huisebug.github.io/tags/fluent-Bit/"/>
    
    <category term="Amazon S3" scheme="https://huisebug.github.io/tags/Amazon-S3/"/>
    
    <category term="kibana" scheme="https://huisebug.github.io/tags/kibana/"/>
    
  </entry>
  
  <entry>
    <title>华为云配置负载均衡器</title>
    <link href="https://huisebug.github.io/2019/04/19/huaweiyun-create-SLB/"/>
    <id>https://huisebug.github.io/2019/04/19/huaweiyun-create-SLB/</id>
    <published>2019-04-19T09:04:01.000Z</published>
    <updated>2021-07-07T08:50:11.642Z</updated>
    
    <content type="html"><![CDATA[<p>华为云配置负载均衡器SLB</p><span id="more"></span><h1 id="华为云配置负载均衡器"><a href="#华为云配置负载均衡器" class="headerlink" title="华为云配置负载均衡器"></a>华为云配置负载均衡器</h1><h1 id="http："><a href="#http：" class="headerlink" title="http："></a>http：</h1><p><img src="/2019/04/19/huaweiyun-create-SLB/media/7f7e73fb5873f14fda16c67775d18e1a.png"></p><p><img src="/2019/04/19/huaweiyun-create-SLB/media/8b02665e7c47c437894b60b9693e7aa2.png"></p><p><img src="/2019/04/19/huaweiyun-create-SLB/media/0291343dbb8359e05b59471944b99170.png"></p><p><img src="/2019/04/19/huaweiyun-create-SLB/media/6a285363b4c5a00f55d805e6ace6b2da.png"></p><p><img src="/2019/04/19/huaweiyun-create-SLB/media/833fa7a903e33f514e3fc3391e42ca9d.png"></p><p><img src="/2019/04/19/huaweiyun-create-SLB/media/1ea5f77d04f22cbf4571815c35ec2675.png"></p><h1 id="https"><a href="#https" class="headerlink" title="https"></a>https</h1><blockquote><p>  情况有两种：</p></blockquote><ol><li><p> 比如SLB后面的服务器自己开放了https（即443端口），这里就不能设置SLB的协议为HTTPS，只能让其代理443端口</p></li><li><p> 直接让SLB使用HTTPS，只能使用http跳转到后端服务器，适用于后端服务器是静态页面。</p></li></ol><h2 id="代理443"><a href="#代理443" class="headerlink" title="代理443"></a>代理443</h2><p><img src="/2019/04/19/huaweiyun-create-SLB/media/3c56abe6ffa49dc3ee0c8d6517907c1e.png"></p><p><img src="/2019/04/19/huaweiyun-create-SLB/media/678e566e65ebaaddd6469145c457eef4.png"></p><p><img src="/2019/04/19/huaweiyun-create-SLB/media/75801615f2881f431d62c9da5b101d4f.png"></p><p><img src="/2019/04/19/huaweiyun-create-SLB/media/df402afd63d74d7080d8ca79a8d768e0.png"></p><blockquote><p>  https方式</p></blockquote><p><img src="/2019/04/19/huaweiyun-create-SLB/media/fb74982ff716116676e25cf2977c9857.png"></p><blockquote><p>  后续操作基本相同。</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;华为云配置负载均衡器SLB&lt;/p&gt;</summary>
    
    
    
    <category term="小知识" scheme="https://huisebug.github.io/categories/%E5%B0%8F%E7%9F%A5%E8%AF%86/"/>
    
    
    <category term="华为云" scheme="https://huisebug.github.io/tags/%E5%8D%8E%E4%B8%BA%E4%BA%91/"/>
    
    <category term="SLB" scheme="https://huisebug.github.io/tags/SLB/"/>
    
    <category term="slb" scheme="https://huisebug.github.io/tags/slb/"/>
    
  </entry>
  
  <entry>
    <title>prometheus监控单一服务及告警</title>
    <link href="https://huisebug.github.io/2019/03/06/prometheus/"/>
    <id>https://huisebug.github.io/2019/03/06/prometheus/</id>
    <published>2019-03-06T05:53:02.000Z</published>
    <updated>2021-07-09T02:57:27.014Z</updated>
    
    <content type="html"><![CDATA[<p>这是一篇prometheus单独监控单服务的部署方法，其中涉及的插件基本都是使用docker进行安装。</p><span id="more"></span><h1 id="安装docker-ce"><a href="#安装docker-ce" class="headerlink" title="安装docker-ce"></a>安装docker-ce</h1><p># 安装docker</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">yum-config-manager \</span><br><span class="line">--add-repo \</span><br><span class="line">https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class="line">yum makecache fast</span><br><span class="line">yum install -y docker-ce</span><br></pre></td></tr></table></figure><p># 编辑systemctl的Docker启动文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &quot;13i ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT&quot; /usr/lib/systemd/system/docker.service</span><br></pre></td></tr></table></figure><p># 启动docker</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable docker</span><br><span class="line">systemctl start docker</span><br></pre></td></tr></table></figure><h1 id="Prometheus安装9090"><a href="#Prometheus安装9090" class="headerlink" title="Prometheus安装9090"></a>Prometheus安装9090</h1><h2 id="物理机安装"><a href="#物理机安装" class="headerlink" title="物理机安装"></a>物理机安装</h2><h3 id="这里是基于1-6版本进行安装"><a href="#这里是基于1-6版本进行安装" class="headerlink" title="这里是基于1.6版本进行安装"></a>这里是基于1.6版本进行安装</h3><p>下载地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/prometheus/prometheus/releases/download/v1.6.2/prometheus-1.6.2.linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><p>使用 tar 解压缩 prometheus-1.6.2.linux-amd64.tar.gz</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -xvzf ~/Download/prometheus-1.6.2.linux-amd64.tar.gz</span><br><span class="line">cd prometheus-1.6.2.linux-amd64</span><br></pre></td></tr></table></figure><p>当解压缩成功后，可以运行 version 检查运行环境是否正常</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./prometheus –version</span><br></pre></td></tr></table></figure><p>如果你看到类似输出，表示你已安装成功:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">prometheus, version 1.6.2 (branch: master, revision:</span><br><span class="line">b38e977fd8cc2a0d13f47e7f0e17b82d1a908a9a)</span><br><span class="line"></span><br><span class="line">build user: root\@c99d9d650cf4</span><br><span class="line"></span><br><span class="line">build date: 20170511-12:59:13</span><br><span class="line"></span><br><span class="line">go version: go1.8.1</span><br></pre></td></tr></table></figure><p>2．5版本下载地址<br><a href="https://github.com/prometheus/prometheus/releases/download/v2.5.0/prometheus-2.5.0.linux-amd64.tar.gz">https://github.com/prometheus/prometheus/releases/download/v2.5.0/prometheus-2.5.0.linux-amd64.tar.gz</a></p><h3 id="启动-Prometheus-Server"><a href="#启动-Prometheus-Server" class="headerlink" title="启动 Prometheus Server"></a>启动 Prometheus Server</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./prometheus</span><br></pre></td></tr></table></figure><p>如果 prometheus 正常启动，你将看到如下信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">INFO[0000] Starting prometheus (version=1.6.2, branch=master,</span><br><span class="line">revision=b38e977fd8cc2a0d13f47e7f0e17b82d1a908a9a) source=main.go:88</span><br><span class="line"></span><br><span class="line">INFO[0000] Build context (go=go1.8.1, user=root\@c99d9d650cf4,</span><br><span class="line">date=20170511-12:59:13) source=main.go:89</span><br><span class="line"></span><br><span class="line">INFO[0000] Loading configuration file prometheus.yml source=main.go:251</span><br><span class="line"></span><br><span class="line">INFO[0000] Loading series map and head chunks... source=storage.go:421</span><br><span class="line"></span><br><span class="line">INFO[0000] 0 series loaded. source=storage.go:432</span><br><span class="line"></span><br><span class="line">INFO[0000] Starting target manager... source=targetmanager.go:61</span><br><span class="line"></span><br><span class="line">INFO[0000] Listening on :9090 source=web.go:259</span><br></pre></td></tr></table></figure><p>从上述启动方式来看，我们可以用screen或者&amp;方式后台运行启动<br>通过启动日志，可以看到 Prometheus Server 默认端口是9090。<br>当 Prometheus 启动后，你可以通过浏览器来访问<a href="http://ip:9090/">http://IP:9090</a> ，将看到如下页面<br>记住关闭防火墙或者定义策略放掉9090端口<br><img src="/2019/03/06/prometheus/media/f1c4a95fb94909b05a9d610a2a58fb42.png"></p><p>在默认配置中，我们已经添加了 Prometheus Server 的监控，所以我们现在可以使用 PromQL （Prometheus Query Language）来查看，比如<br><img src="/2019/03/06/prometheus/media/3ab2d55b3b0d7d3b1b4b4244a6ce492a.png"><br><img src="/2019/03/06/prometheus/media/7229c0f7ad058484a45283c76854c961.png"></p><p>总结：<br>可以看出 Prometheus 二进制安装非常方便，没有依赖，自带查询 web 界面</p><h2 id="容器化安装"><a href="#容器化安装" class="headerlink" title="容器化安装"></a>容器化安装</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run -d \--restart=always \</span><br><span class="line">-p 9090:9090 \</span><br><span class="line">--name prometheus \</span><br><span class="line">-v /root/prometheus:/etc/prometheus \</span><br><span class="line">quay.io/prometheus/prometheus \</span><br><span class="line">--config.file=/etc/prometheus/prometheus.yml --web.enable-lifecycle</span><br></pre></td></tr></table></figure><p>这时候我们再次查询版本就是最新版<br><img src="/2019/03/06/prometheus/media/555791a79656b548173f8df28dccab34.png"></p><h2 id="Prometheus服务重载"><a href="#Prometheus服务重载" class="headerlink" title="Prometheus服务重载"></a>Prometheus服务重载</h2><p>Prometheus一般更新服务都是重启，这不适合生产环境，所以需要重载配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST &lt;http://IP/-/reload&gt;</span><br></pre></td></tr></table></figure><p>从 2.0 开始，hot reload 功能是默认关闭的，如需开启，需要在启动 Prometheus<br>的时候，添加 –web.enable-lifecycle 参数。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://192.168.100.80:9090/-/reload</span><br></pre></td></tr></table></figure><h2 id="Prometheus服务修改存储位置"><a href="#Prometheus服务修改存储位置" class="headerlink" title="Prometheus服务修改存储位置"></a>Prometheus服务修改存储位置</h2><ul><li>--storage.tsdb.path：这决定了Prometheus写入数据库的位置。默认为data/。镜像中定义的是/prometheus/data</li><li>--storage.tsdb.retention：这决定了何时删除旧数据。默认为15d。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --restart=always \</span><br><span class="line">-p 9090:9090 \</span><br><span class="line">--name prometheus \</span><br><span class="line">-v /root/prometheus:/etc/prometheus:rw \</span><br><span class="line">-v /root/prometheus/data:/etc/prometheus/data \</span><br><span class="line">quay.io/prometheus/prometheus \</span><br><span class="line">--config.file=/etc/prometheus/prometheus.yml --web.enable-lifecycle \</span><br><span class="line">--storage.tsdb.path=/etc/prometheus/data --storage.tsdb.retention=15d</span><br></pre></td></tr></table></figure><h1 id="Grafana安装3000"><a href="#Grafana安装3000" class="headerlink" title="Grafana安装3000"></a>Grafana安装3000</h1><h2 id="物理机安装-1"><a href="#物理机安装-1" class="headerlink" title="物理机安装"></a>物理机安装</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-5.2.4-1.x86_64.rpm</span><br><span class="line">sudo yum localinstall grafana-5.2.4-1.x86_64.rpm</span><br></pre></td></tr></table></figure><h2 id="容器化安装-1"><a href="#容器化安装-1" class="headerlink" title="容器化安装"></a>容器化安装</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --restart=always -dit -v /etc/localtime:/etc/localtime --name=grafana -p 3000:3000 grafana/grafana</span><br></pre></td></tr></table></figure><h2 id="连接prometheus"><a href="#连接prometheus" class="headerlink" title="连接prometheus"></a>连接prometheus</h2><p>没有开启prometheus的认证，这里采用代理方式，即浏览器方式，注意URL地址有http://，如果不能访问在切换到default的方式尝试下。<br><img src="/2019/03/06/prometheus/media/4328705550fa933777439196851f3f25.png"></p><p>添加dashboard模板<br><img src="/2019/03/06/prometheus/media/8e63447a3ebdaedea32650a4ddb96872.png"></p><h1 id="Node-Exporter安装9100"><a href="#Node-Exporter安装9100" class="headerlink" title="Node Exporter安装9100"></a>Node Exporter安装9100</h1><p>node_exporter 主要用于 UNIX 系统监控</p><h2 id="物理机安装-2"><a href="#物理机安装-2" class="headerlink" title="物理机安装"></a>物理机安装</h2><p>下载地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/prometheus/node_exporter/releases/download/v0.14.0/node_exporter-0.14.0.linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><p>解压</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar zxf node_exporter-0.14.0.linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><p>进入后运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./node_exporter</span><br></pre></td></tr></table></figure><p>访问<a href="http://192.168.100.80:9100/metrics%E5%8D%B3%E5%8F%AF%E7%9C%8B%E5%88%B0%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E3%80%82">http://192.168.100.80:9100/metrics即可看到时序数据。</a></p><h2 id="容器化安装-2"><a href="#容器化安装-2" class="headerlink" title="容器化安装"></a>容器化安装</h2><p>老命令是这样</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">docker run -d \</span><br><span class="line">-v /proc:/host/proc:ro \</span><br><span class="line">-v /sys:/host/sys:ro \</span><br><span class="line">-v /:/rootfs:ro \\</span><br><span class="line">--net=&quot;host&quot; \</span><br><span class="line">quay.io/prometheus/node-exporter \</span><br><span class="line">--collector.procfs /host/proc \</span><br><span class="line">--collector.sysfs /host/sys \</span><br><span class="line">--collector.filesystem.ignored-mount-points &quot;^/(sys\|proc\|dev\|host\|etc)($\|/)&quot;</span><br></pre></td></tr></table></figure><p>新命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name=node-exporter --restart=always \</span><br><span class="line">-v &quot;/proc:/host/proc&quot; \</span><br><span class="line">-v &quot;/sys:/host/sys&quot; \</span><br><span class="line">-v &quot;/:/rootfs&quot; \</span><br><span class="line">--net=&quot;host&quot; quay.io/prometheus/node-exporter \</span><br><span class="line">--path.procfs /host/proc \</span><br><span class="line">--path.sysfs /host/sys \</span><br><span class="line">--collector.filesystem.ignored-mount-points &quot;^/(sys|proc|dev|host|etc)($|/)&quot;</span><br></pre></td></tr></table></figure><p>读取node数据</p><p>在prometheus.yml文件中新加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- job_name: &quot; node-exporter&quot;</span><br><span class="line"> static_configs:</span><br><span class="line">   - targets: [&quot;127.0.0.1:9100&quot;]</span><br></pre></td></tr></table></figure><p>查询promQL</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;job=&quot;node-exporter&quot;&#125;</span><br></pre></td></tr></table></figure><h2 id="Grafana模板添加地址"><a href="#Grafana模板添加地址" class="headerlink" title="Grafana模板添加地址"></a>Grafana模板添加地址</h2><p><a href="https://grafana.com/dashboards/5573">https://grafana.com/dashboards/5573</a></p><p>上面地址中json下载后导入到grafana的dashboard中即可</p><h1 id="Mysql-exporter安装9104"><a href="#Mysql-exporter安装9104" class="headerlink" title="Mysql exporter安装9104"></a>Mysql exporter安装9104</h1><h2 id="容器化安装-3"><a href="#容器化安装-3" class="headerlink" title="容器化安装"></a>容器化安装</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --restart=always \</span><br><span class="line">-p 9104:9104 \</span><br><span class="line">--name mysqld-exporter \</span><br><span class="line">-v /root/prometheus/mysqld:/etc/mysqld \</span><br><span class="line">prom/mysqld-exporter --config.my-cnf=&quot;/etc/mysqld/my.cnf&quot;</span><br></pre></td></tr></table></figure><p>my.cnf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> [client]</span><br><span class="line">user=root</span><br><span class="line">password=数据库密码</span><br><span class="line">host=服务器所在地址</span><br><span class="line">port=3306</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>参考地址<br><a href="https://github.com/prometheus/mysqld_exporter/blob/master/mysqld_exporter_test.go">https://github.com/prometheus/mysqld_exporter/blob/master/mysqld_exporter_test.go</a></p><p>在prometheus.yml文件中新加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- job_name: &quot;mysql&quot;</span><br><span class="line"> static_configs:</span><br><span class="line">   - targets: [&quot;127.0.0.1:9104&quot;]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Grafana模板添加地址-1"><a href="#Grafana模板添加地址-1" class="headerlink" title="Grafana模板添加地址"></a>Grafana模板添加地址</h2><p><a href="https://grafana.com/dashboards/6239">https://grafana.com/dashboards/6239</a></p><p>上面地址中json下载后导入到grafana的dashboard中即可</p><h1 id="Nginx-exporter安装9913"><a href="#Nginx-exporter安装9913" class="headerlink" title="Nginx exporter安装9913"></a>Nginx exporter安装9913</h1><h2 id="容器化安装-4"><a href="#容器化安装-4" class="headerlink" title="容器化安装"></a>容器化安装</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --restart=always \</span><br><span class="line">--name nginx-exporter -p 9913:9913 \</span><br><span class="line">--env NGINX_STATUS=&quot;http://127.0.0.1/status/format/json&quot; \</span><br><span class="line">sophos/nginx-vts-exporter</span><br></pre></td></tr></table></figure><p>在prometheus.yml文件中新加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- job_name: &quot;nginx&quot;</span><br><span class="line"> static_configs:</span><br><span class="line">   - targets: [&quot;127.0.0.1:9913&quot;]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="修改nginx配置"><a href="#修改nginx配置" class="headerlink" title="修改nginx配置"></a>修改nginx配置</h2><p>一．nginx添加vts模块（重新编译后进行安装）</p><p>下载模块</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git://github.com/vozlt/nginx-module-vts.git</span><br></pre></td></tr></table></figure><p>在nginx编译时添加vts模块<br>--add-module=nginx-module-vts</p><p>编译并安装nginx安装包解压后路径下，此处$PWD是因为我的vts模块git到了nginx包路径下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./configure --prefix=/software/nginx --add-module=$PWD/nginx-module-vts &amp;&amp; make &amp;&amp; make install</span><br></pre></td></tr></table></figure><p>二．Nginx Conf配置</p><p>更改Nginx Conf的配置，添加监控接口/status/：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">vhost_traffic_status_zone;</span><br><span class="line">vhost_traffic_status_filter_by_host on;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">...</span><br><span class="line">location /status &#123;</span><br><span class="line">vhost_traffic_status_display;</span><br><span class="line">vhost_traffic_status_display_format html;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>配置建议：</p><p>打开vhost过滤：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vhost_traffic_status_filter_by_host on;</span><br></pre></td></tr></table></figure><p>开启此功能，在Nginx配置有多个server_name的情况下，会根据不同的server_name进行流量的统计，否则默认会把流量全部计算到第一个server_name上。</p><p>在不想统计流量的server区域禁用vhost_traffic_status，配置示例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">...</span><br><span class="line">vhost_traffic_status off;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>假如nginx没有规范配置server_name或者无需进行监控的server上，那么建议在此vhost上禁用统计监控功能。否则会出现“127.0.0.1”，hostname等的域名监控信息。</p><p>重启nginx，不是重载</p><h2 id="Grafana模板添加地址-2"><a href="#Grafana模板添加地址-2" class="headerlink" title="Grafana模板添加地址"></a>Grafana模板添加地址</h2><p><a href="https://grafana.com/dashboards/2949">https://grafana.com/dashboards/2949</a></p><p>上面地址中json下载后导入到grafana的dashboard中即可</p><h1 id="Redis-exporter安装9121"><a href="#Redis-exporter安装9121" class="headerlink" title="Redis exporter安装9121"></a>Redis exporter安装9121</h1><h2 id="容器化安装-5"><a href="#容器化安装-5" class="headerlink" title="容器化安装"></a>容器化安装</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name redis-exporter -p 9121:9121 --env REDIS_ADDR=&quot;192.168.3.144:6379&quot; oliver006/redis_exporter</span><br></pre></td></tr></table></figure><p>环境变量</p><table><thead><tr><th>名称</th><th>描述</th></tr></thead><tbody><tr><td>REDIS_ADDR</td><td>Redis节点的地址</td></tr><tr><td>REDIS_PASSWORD</td><td>验证Redis时使用的密码</td></tr><tr><td>REDIS_ALIAS</td><td>Redis节点的别名</td></tr><tr><td>REDIS_FILE</td><td>包含Redis节点的文件路径</td></tr></tbody></table><p>在prometheus.yml文件中新加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- job_name: &quot;redis&quot;</span><br><span class="line"> static_configs:</span><br><span class="line">   - targets: [&quot;127.0.0.1:9121&quot;]</span><br></pre></td></tr></table></figure><h2 id="Grafana模板添加地址-3"><a href="#Grafana模板添加地址-3" class="headerlink" title="Grafana模板添加地址"></a>Grafana模板添加地址</h2><p><a href="https://grafana.com/dashboards/763">https://grafana.com/dashboards/763</a></p><p>上面地址中json下载后导入到grafana的dashboard中即可</p><h1 id="SpringBoot2-x监控"><a href="#SpringBoot2-x监控" class="headerlink" title="SpringBoot2.x监控"></a>SpringBoot2.x监控</h1><p>监控java项目的jvm、缓冲区等</p><p>修改java代码，使其可以生成时序数据</p><p>参考链接地址</p><p><a href="https://blog.csdn.net/MyHerux/article/details/80667524?tdsourcetag=s_pcqq_aiomsg">https://blog.csdn.net/MyHerux/article/details/80667524?tdsourcetag=s_pcqq_aiomsg</a></p><p>在prometheus.yml文件中新加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- job_name: &quot;webhook&quot;</span><br><span class="line">  metrics_path: &#x27;可访问的时序数据路径&#x27;</span><br><span class="line">  static_configs:</span><br><span class="line">    - targets: [&quot;项目地址&quot;]</span><br><span class="line">      labels:</span><br><span class="line">        instance: 项目名</span><br></pre></td></tr></table></figure><h2 id="Grafana模板添加地址-4"><a href="#Grafana模板添加地址-4" class="headerlink" title="Grafana模板添加地址"></a>Grafana模板添加地址</h2><p><a href="https://grafana.com/dashboards/6756">https://grafana.com/dashboards/6756</a><br>上面地址中json下载后导入到grafana的dashboard中即可</p><h1 id="Pushgateway安装"><a href="#Pushgateway安装" class="headerlink" title="Pushgateway安装"></a><strong>Pushgateway安装</strong></h1><p>Pushgateway 是 Prometheus 生态中一个重要工具，使用它的原因主要是：</p><ul><li>  Prometheus 采用 pull 模式，可能由于不在一个子网或者防火墙原因，导致Prometheus 无法直接拉取各个 target 数据。</li><li>  在监控业务数据的时候，需要将不同数据汇总, 由 Prometheus统一收集。<br>由于以上原因，不得不使用 pushgateway，但在使用之前，有必要了解一下它的一些弊端：</li><li>  将多个节点数据汇总到 pushgateway, 如果 pushgateway 挂了，受影响比多个 target大。</li><li>  Prometheus 拉取状态 up 只针对 pushgateway, 无法做到对每个节点有效。</li><li>  Pushgateway 可以持久化推送给它的所有监控数据。<br>因此，即使你的监控已经下线，prometheus 还会拉取到旧的监控数据，需要手动清理pushgateway 不要的数据。</li></ul><h2 id="物理机安装-3"><a href="#物理机安装-3" class="headerlink" title="物理机安装"></a>物理机安装</h2><p>下载地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/prometheus/pushgateway/releases/download/v0.4.0/pushgateway-0.4.0.linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><p>和其他服务一样，也是解压后运行服务即可</p><h2 id="容器化安装-6"><a href="#容器化安装-6" class="headerlink" title="容器化安装"></a>容器化安装</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --restart=always --name=pushgateway -p 9091:9091 prom/pushgateway</span><br></pre></td></tr></table></figure><h1 id="Alertmanager安装9093"><a href="#Alertmanager安装9093" class="headerlink" title="Alertmanager安装9093"></a>Alertmanager安装9093</h1><h2 id="物理机安装-4"><a href="#物理机安装-4" class="headerlink" title="物理机安装"></a>物理机安装</h2><p>下载地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/prometheus/alertmanager/releases/download/v0.14.0/alertmanager-0.14.0.linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><p>解压后创建配置文件</p><p>alertmanager.yml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">  resolve_timeout: 2h</span><br><span class="line"></span><br><span class="line">route:</span><br><span class="line">  group_by: [&#x27;alertname&#x27;]</span><br><span class="line">  group_wait: 5s</span><br><span class="line">  group_interval: 10s</span><br><span class="line">  repeat_interval: 1h</span><br><span class="line">  receiver: &#x27;webhook&#x27;</span><br><span class="line"></span><br><span class="line">receivers:</span><br><span class="line">- name: &#x27;webhook&#x27;</span><br><span class="line">  webhook_configs:</span><br><span class="line">  - url: &#x27;http://example.com/xxxx&#x27;</span><br><span class="line">send_resolved: true</span><br></pre></td></tr></table></figure><p>说明： 这里我们使用 Alertmanager 的 webhook_configs选项来接收消息，当接收到新的告警信息，它会将消息转发到配置的 url地址。</p><h2 id="容器化安装-7"><a href="#容器化安装-7" class="headerlink" title="容器化安装"></a>容器化安装</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --restart=always -p 9093:9093 --name=alertmanager \</span><br><span class="line">-v /root/prometheus/alertmanager:/etc/alertmanager \</span><br><span class="line">quay.io/prometheus/alertmanager --config.file=/etc/alertmanager/alertmanager.yml</span><br></pre></td></tr></table></figure><h2 id="配置QQ邮件报警1-6版本"><a href="#配置QQ邮件报警1-6版本" class="headerlink" title="配置QQ邮件报警1.6版本"></a>配置QQ邮件报警1.6版本</h2><p>Prometheus 版本：prometheus-1.6<br>Alertmanager 版本：alertmanager-0.8<br>发送告警邮件的邮箱：qq email</p><h3 id="修改-alertmanager-yml-配置文件"><a href="#修改-alertmanager-yml-配置文件" class="headerlink" title="修改 alertmanager.yml 配置文件"></a>修改 alertmanager.yml 配置文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">  smtp_smarthost: &#x27;smtp.qq.com:587&#x27;</span><br><span class="line">  smtp_from: &#x27;发件人邮箱&#x27;     // xxx@qq.com</span><br><span class="line">  smtp_auth_username: &#x27;发件人邮箱&#x27;   //xxx@qq.com</span><br><span class="line">  smtp_auth_password: &#x27;QQ邮箱授权码&#x27;  //QQ邮箱设置中心生成</span><br><span class="line"></span><br><span class="line">route:</span><br><span class="line">  repeat_interval: 10s</span><br><span class="line">  receiver: &#x27;收件人信息&#x27;   // xxxX</span><br><span class="line"></span><br><span class="line">receivers:</span><br><span class="line">- name: &#x27;收件人信息&#x27;  // xxxX</span><br><span class="line">  email_configs:</span><br><span class="line">  - to: &#x27;收件人邮箱&#x27;   // xxxX@qq.com</span><br></pre></td></tr></table></figure><h3 id="在prometheus下添加-alert-rules-文件"><a href="#在prometheus下添加-alert-rules-文件" class="headerlink" title="在prometheus下添加 alert.rules 文件"></a>在prometheus下添加 alert.rules 文件</h3><p>alert.rules</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ALERT memory_high</span><br><span class="line">  IF prometheus_local_storage_memory_series &gt;= 0</span><br><span class="line">  FOR 15s</span><br><span class="line">  ANNOTATIONS &#123;</span><br><span class="line">    summary = &quot;Prometheus using more memory than it should &#123;&#123; $labels.instance &#125;&#125;&quot;,</span><br><span class="line">    description = &quot;&#123;&#123; $labels.instance &#125;&#125; has lots of memory man (current value: &#123;&#123; $value &#125;&#125;s)&quot;,</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="修改prometheus-yml新加配置"><a href="#修改prometheus-yml新加配置" class="headerlink" title="修改prometheus.yml新加配置"></a>修改prometheus.yml新加配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rule_files:</span><br><span class="line"><span class="meta">  #</span><span class="bash"> - <span class="string">&quot;first.rules&quot;</span></span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> - <span class="string">&quot;second.rules&quot;</span></span></span><br><span class="line">  - &quot;alert.rules&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2019/03/06/prometheus/media/392cdd9cd910c8861614dbbbf08c9cff.png"></p><p>重启两个服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./alertmanager -config.file=simple.yml</span><br><span class="line">./prometheus -alertmanager.url=http://localhost:9093</span><br></pre></td></tr></table></figure><h2 id="企业微信报警2-0版本"><a href="#企业微信报警2-0版本" class="headerlink" title="企业微信报警2.0版本"></a>企业微信报警2.0版本</h2><ul><li>step 1: 访问网站<a href="https://work.weixin.qq.com/">https://work.weixin.qq.com/</a> 注册企业微信账号（不需要企业认证）。</li><li>step 2: 访问apps 创建应用，点击 创建应用按钮 -&gt; 填写应用信息：</li></ul><p><img src="/2019/03/06/prometheus/media/2167460f7929e9f8d6b804602d053162.png"><br><img src="/2019/03/06/prometheus/media/b7f660d26c3574286fa0f476d8e181c8.png"></p><p>prometheus: 2.0<br>node_exporter: 0.15<br>alertmanager: 0.14</p><p>至此，所有服务插件使用容器化安装！！！</p><h3 id="修改prometheus-yml配置文件"><a href="#修改prometheus-yml配置文件" class="headerlink" title="修改prometheus.yml配置文件"></a>修改prometheus.yml配置文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.</span><br><span class="line">  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.</span><br><span class="line"></span><br><span class="line">scrape_configs:</span><br><span class="line">  - job_name: &#x27;prometheus&#x27;</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&#x27;localhost:9090&#x27;]</span><br><span class="line">  - job_name: &quot;node&quot;</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&quot;192.168.100.80:9100&quot;]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Alertmanager configuration</span></span><br><span class="line">alerting:</span><br><span class="line">  alertmanagers:</span><br><span class="line">  - static_configs:</span><br><span class="line">    - targets:</span><br><span class="line">      - 192.168.100.80:9093</span><br><span class="line"></span><br><span class="line">rule_files:</span><br><span class="line">  - &quot;/etc/prometheus/rules.yml&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="修改alertmanager-yml配置文件"><a href="#修改alertmanager-yml配置文件" class="headerlink" title="修改alertmanager.yml配置文件"></a>修改alertmanager.yml配置文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">global:</span><br><span class="line">  resolve_timeout: 1h</span><br><span class="line">route:</span><br><span class="line">  group_by: [&#x27;alertname&#x27;]</span><br><span class="line">  receiver: &#x27;wechat&#x27;</span><br><span class="line">  group_wait: 30s</span><br><span class="line">  group_interval: 30s</span><br><span class="line">  repeat_interval: 10m</span><br><span class="line"></span><br><span class="line">receivers:</span><br><span class="line">- name: &#x27;wechat&#x27;</span><br><span class="line">  wechat_configs:</span><br><span class="line">  - corp_id: &#x27;xxx&#x27;</span><br><span class="line">    to_party: &#x27;8&#x27;</span><br><span class="line">    agent_id: &#x27;1000009&#x27;</span><br><span class="line">    api_secret: &#x27;xxx&#x27;</span><br></pre></td></tr></table></figure><ul><li>  corp_id: 企业微信账号唯一 ID， 可以在”我的企业”中查看。</li><li>  to_party: 需要发送的组(部门ID)。即<br><img src="/2019/03/06/prometheus/media/a5971d9b5aec90857f23a49be18077d8.png"></li><li>  agent_id: 第三方企业应用的 ID，可以在自己创建的第三方企业应用详情页面查看。</li><li>  api_secret:第三方企业应用的密钥，可以在自己创建的第三方企业应用详情页面查看。</li></ul><h3 id="在prometheus文件夹下添加-rules-yml-文件"><a href="#在prometheus文件夹下添加-rules-yml-文件" class="headerlink" title="在prometheus文件夹下添加 rules.yml 文件"></a>在prometheus文件夹下添加 rules.yml 文件</h3><p>rules.yml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">groups:</span><br><span class="line">- name: node</span><br><span class="line">  rules:</span><br><span class="line">  - alert: Server_Status</span><br><span class="line">    expr: up&#123;job=&quot;node&quot;&#125; == 0</span><br><span class="line">    for: 15s</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;机器 &#123;&#123; $labels.instance &#125;&#125; 挂了&quot;</span><br></pre></td></tr></table></figure><p>然后重启prometheus和alertmanager容器，再关闭node-exporter容器，即可在企业微信接收到报警信息</p><p>也可不断刷新alertmanager的web页面<a href="http://192.168.100.80:9093//#/alerts%E8%BF%9B%E8%A1%8C%E6%9F%A5%E7%9C%8B">http://192.168.100.80:9093/\#/alerts进行查看</a><br><img src="/2019/03/06/prometheus/media/9e7e663e7c4e6597a842fd5a8a1fb2e2.png"></p><p>企业微信接收到的报警信息<br><img src="/2019/03/06/prometheus/media/b52ae0f08ae7696f09e7dec9cab59c52.png"></p><h2 id="企业微信-QQmail同时报警"><a href="#企业微信-QQmail同时报警" class="headerlink" title="企业微信/QQmail同时报警"></a>企业微信/QQmail同时报警</h2><p>配置文件如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">  resolve_timeout: 1h</span><br><span class="line">  smtp_smarthost: &#x27;smtp.qq.com:587&#x27;</span><br><span class="line">  smtp_from: &#x27;发件人&#x27;     </span><br><span class="line">  smtp_auth_username: &#x27;发件人&#x27;   </span><br><span class="line">  smtp_auth_password: &#x27;授权码&#x27;  </span><br><span class="line"></span><br><span class="line">route:</span><br><span class="line">  group_by: [&#x27;alertname&#x27;] </span><br><span class="line">  group_wait: 30s</span><br><span class="line">  group_interval: 30s</span><br><span class="line">  repeat_interval: 10m</span><br><span class="line">  receiver: &#x27;huisebug&#x27;</span><br><span class="line">  routes:</span><br><span class="line">  - receiver: &#x27;wechat&#x27;</span><br><span class="line">    continue: true</span><br><span class="line">  - receiver: &#x27;huisebug&#x27;</span><br><span class="line">    continue: true</span><br><span class="line"></span><br><span class="line">receivers:</span><br><span class="line">- name: &#x27;huisebug&#x27;  </span><br><span class="line">  email_configs:</span><br><span class="line">  - to: &#x27;收件人&#x27;</span><br><span class="line">    send_resolved: true</span><br><span class="line">- name: &#x27;wechat&#x27;</span><br><span class="line">  wechat_configs:</span><br><span class="line">  - corp_id: &#x27;&#x27;</span><br><span class="line">    to_party: &#x27;8&#x27;</span><br><span class="line">    agent_id: &#x27;1000009&#x27;</span><br><span class="line">    api_secret: &#x27;&#x27;</span><br><span class="line">    send_resolved: true</span><br><span class="line">    </span><br><span class="line">templates: </span><br><span class="line">- &#x27;/etc/alertmanager/configmaps/alertmanager-temp/temp.yaml&#x27;</span><br></pre></td></tr></table></figure><h2 id="钉钉webhook报警2-0版本8060"><a href="#钉钉webhook报警2-0版本8060" class="headerlink" title="钉钉webhook报警2.0版本8060"></a>钉钉webhook报警2.0版本8060</h2><h3 id="安装钉钉插件"><a href="#安装钉钉插件" class="headerlink" title="安装钉钉插件"></a>安装钉钉插件</h3><h4 id="物理机安装-5"><a href="#物理机安装-5" class="headerlink" title="物理机安装"></a>物理机安装</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/timonwong/prometheus-webhook-dingtalk/releases/download/v0.3.0/prometheus-webhook-dingtalk-0.3.0.linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><p>解压并启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nohup ./prometheus-webhook-dingtalk \</span><br><span class="line">--ding.profile=&quot;ops_dingding=钉钉机器人处复制webhook&quot; \</span><br><span class="line"><span class="meta">2&gt;</span><span class="bash">&amp;1 1&gt;dingding.log &amp;</span></span><br></pre></td></tr></table></figure><h4 id="容器化安装-8"><a href="#容器化安装-8" class="headerlink" title="容器化安装"></a>容器化安装</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --restart=always -p 8060:8060 --name=prometheus-webhook-dingtalk timonwong/prometheus-webhook-dingtalk \</span><br><span class="line">--ding.profile=&quot;ops_dingding=钉钉机器人处复制webhook&quot;</span><br></pre></td></tr></table></figure><p>修改alertmanager.yml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">  resolve_timeout: 5s</span><br><span class="line">route:</span><br><span class="line">  receiver: webhook</span><br><span class="line">  group_wait: 30s</span><br><span class="line">  group_interval: 5s</span><br><span class="line">  repeat_interval: 90s</span><br><span class="line">  group_by: [alertname]</span><br><span class="line">  routes:</span><br><span class="line">  - receiver: webhook</span><br><span class="line">    group_wait: 10s</span><br><span class="line">    match:</span><br><span class="line">      team: node</span><br><span class="line">receivers:</span><br><span class="line">- name: webhook</span><br><span class="line">  webhook_configs:</span><br><span class="line">  - url: http://192.168.3.144:8060/dingtalk/ops_dingding/send</span><br><span class="line">    send_resolved: true</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="Prometheus-server的读写分离"><a href="#Prometheus-server的读写分离" class="headerlink" title="Prometheus-server的读写分离"></a>Prometheus-server的读写分离</h1><p>建立三个文件夹prometheus1（读） prometheus2（写） prometheus3（写）</p><p>读 prometheus.yml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.</span><br><span class="line">  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.</span><br><span class="line"></span><br><span class="line">remote_read:</span><br><span class="line">  - url: &#x27;http://192.168.100.80:9191/api/v1/read&#x27;</span><br><span class="line">    remote_timeout: 8s</span><br><span class="line">  - url: &#x27;http://192.168.100.80:9192/api/v1/read&#x27;</span><br><span class="line">    remote_timeout: 8s</span><br></pre></td></tr></table></figure><p>写 prometheus.yml 两个相同</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.</span><br><span class="line">  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.</span><br><span class="line"></span><br><span class="line">scrape_configs:</span><br><span class="line">  - job_name: &#x27;prometheus&#x27;</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&#x27;localhost:9090&#x27;]</span><br><span class="line">  - job_name: &quot;node&quot;</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&quot;192.168.100.80:9100&quot;]</span><br></pre></td></tr></table></figure><p>容器化建立三个服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --restart=always \</span><br><span class="line">-p 9190:9090 \</span><br><span class="line">--name prometheus-read \</span><br><span class="line">-v /root/prometheus/prometheus1:/etc/prometheus \</span><br><span class="line">quay.io/prometheus/prometheus \</span><br><span class="line">--config.file=/etc/prometheus/prometheus.yml --web.enable-lifecycle</span><br><span class="line"></span><br><span class="line">docker run -d --restart=always \</span><br><span class="line">-p 9191:9090 \</span><br><span class="line">--name prometheus-write-1 \</span><br><span class="line">-v /root/prometheus/prometheus2:/etc/prometheus \</span><br><span class="line">quay.io/prometheus/prometheus \</span><br><span class="line">--config.file=/etc/prometheus/prometheus.yml --web.enable-lifecycle</span><br><span class="line"></span><br><span class="line">docker run -d --restart=always \\</span><br><span class="line">-p 9192:9090 \</span><br><span class="line">--name prometheus-write-2 \</span><br><span class="line">-v /root/prometheus/prometheus3:/etc/prometheus \</span><br><span class="line">quay.io/prometheus/prometheus </span><br><span class="line">--config.file=/etc/prometheus/prometheus.yml --web.enable-lifecycle</span><br></pre></td></tr></table></figure><h2 id="测试效果"><a href="#测试效果" class="headerlink" title="测试效果"></a>测试效果</h2><p>访问读所在web页面进行测试</p><p><img src="/2019/03/06/prometheus/media/b1bc1d4f39d509bd26b16674fd7802bd.png"></p><p>三个web页面将会相同</p><h2 id="新建一个node-exporter来验证效果"><a href="#新建一个node-exporter来验证效果" class="headerlink" title="新建一个node-exporter来验证效果"></a>新建一个node-exporter来验证效果</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name=node-exporter-test --restart=always \</span><br><span class="line">-v &quot;/proc:/host/proc&quot; \</span><br><span class="line">-v &quot;/sys:/host/sys&quot; \</span><br><span class="line">-v &quot;/:/rootfs&quot; \</span><br><span class="line">-p 9101:9100 quay.io/prometheus/node-exporter --path.procfs /host/proc --path.sysfs /host/sys --collector.filesystem.ignored-mount-points &quot;^/(sys|proc|dev|host|etc)($|/)&quot;</span><br></pre></td></tr></table></figure><p>然后修改其中一个写的配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">global:</span><br><span class="line">  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.</span><br><span class="line">  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.</span><br><span class="line"></span><br><span class="line">scrape_configs:</span><br><span class="line">  - job_name: &#x27;prometheus&#x27;</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&#x27;localhost:9090&#x27;]</span><br><span class="line">  - job_name: &quot;sample&quot;</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&quot;192.168.100.80:8080&quot;]</span><br><span class="line">  - job_name: &quot;node&quot;</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&quot;192.168.100.80:9100&quot;]</span><br><span class="line">      - targets: [&quot;192.168.100.80:9101&quot;]</span><br></pre></td></tr></table></figure><p>重载服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST &lt;http://192.168.100.80:9191/-/reload&gt;</span><br></pre></td></tr></table></figure><p>测试效果<br><img src="/2019/03/06/prometheus/media/e888d4197a1869abe5e872d5be69d903.png"><br><img src="/2019/03/06/prometheus/media/ca5593d47b6675e60231db0b34b6b89a.png"><br><img src="/2019/03/06/prometheus/media/c0569301248d27423508ff60fbd055a7.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这是一篇prometheus单独监控单服务的部署方法，其中涉及的插件基本都是使用docker进行安装。&lt;/p&gt;</summary>
    
    
    
    <category term="Prometheus" scheme="https://huisebug.github.io/categories/Prometheus/"/>
    
    
    <category term="prometheus" scheme="https://huisebug.github.io/tags/prometheus/"/>
    
    <category term="alertmanager" scheme="https://huisebug.github.io/tags/alertmanager/"/>
    
    <category term="grafana" scheme="https://huisebug.github.io/tags/grafana/"/>
    
    <category term="钉钉告警" scheme="https://huisebug.github.io/tags/%E9%92%89%E9%92%89%E5%91%8A%E8%AD%A6/"/>
    
    <category term="nginx" scheme="https://huisebug.github.io/tags/nginx/"/>
    
    <category term="springboot2" scheme="https://huisebug.github.io/tags/springboot2/"/>
    
    <category term="prometheus读写分离" scheme="https://huisebug.github.io/tags/prometheus%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/"/>
    
  </entry>
  
  <entry>
    <title>持续更新-20190306-k8s常见方法及错误解决</title>
    <link href="https://huisebug.github.io/2018/09/06/k8s%E5%B8%B8%E8%A7%81%E6%96%B9%E6%B3%95%E5%8F%8A%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3/"/>
    <id>https://huisebug.github.io/2018/09/06/k8s%E5%B8%B8%E8%A7%81%E6%96%B9%E6%B3%95%E5%8F%8A%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3/</id>
    <published>2018-09-06T08:48:27.000Z</published>
    <updated>2021-07-09T02:59:57.853Z</updated>
    
    <content type="html"><![CDATA[<p>k8s常见方法及错误解决方法</p><span id="more"></span><h1 id="如果kubectl-get-nodes状态为notready"><a href="#如果kubectl-get-nodes状态为notready" class="headerlink" title="如果kubectl get nodes状态为notready"></a>如果kubectl get nodes状态为notready</h1><p>可以查看kubelet服务是否开启</p><h1 id="容器之间使用flanneld通信，可以ping通docker0，容器之间无法ping通的时候"><a href="#容器之间使用flanneld通信，可以ping通docker0，容器之间无法ping通的时候" class="headerlink" title="容器之间使用flanneld通信，可以ping通docker0，容器之间无法ping通的时候"></a>容器之间使用flanneld通信，可以ping通docker0，容器之间无法ping通的时候</h1><p>iptables -F<br>iptables -X<br>iptables -Z<br>iptables -P INPUT ACCEPT<br>iptables -P OUTPUT ACCEPT<br>iptables -P FORWARD ACCEPT</p><p>运行iptables关闭</p><h1 id="system-anonymous”-cannot-proxy-services-in-the-namespace-“kube-system"><a href="#system-anonymous”-cannot-proxy-services-in-the-namespace-“kube-system" class="headerlink" title="system:anonymous” cannot proxy services in the namespace “kube-system"></a>system:anonymous” cannot proxy services in the namespace “kube-system</h1><p>浏览器访问 URL：</p><p><a href="https://192.168.100.80:6443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard">https://192.168.100.80:6443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard</a>（浏览器会提示证书验证，因为通过加密通道，以改⽅式访问的话，需要提前导⼊证书到你的计算机中）。不然会提示User<br>“system:anonymous” cannot proxy services in the namespace “kube-system”.</p><p>导⼊证书</p><p>将⽣成的admin.pem证书转换格式（/etc/kubernetes/ssl目录下）</p><p>openssl pkcs12 -export -in admin.pem -out admin.p12 -inkey admin-key.pem</p><p>将⽣成的 admin.p12<br>证书导⼊的你的电脑的浏览器，导出的时候记住你设置的密码，导⼊的时候还要⽤到</p><h1 id="拉取国外镜像方法"><a href="#拉取国外镜像方法" class="headerlink" title="拉取国外镜像方法"></a>拉取国外镜像方法</h1><p>首先你要有GitHub和dockerhub的账号，将两个进行关联，关联请百度</p><p>Docker 新建仓库关联github，这里是关联后的显示了我的github下有这四个项目</p><p><img src="/2018/09/06/k8s%E5%B8%B8%E8%A7%81%E6%96%B9%E6%B3%95%E5%8F%8A%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3/media/102f9d9e8f071e8e0005e0dde3170ca7.png"></p><p>然后选择一个进行关联，在进行创建</p><p><img src="/2018/09/06/k8s%E5%B8%B8%E8%A7%81%E6%96%B9%E6%B3%95%E5%8F%8A%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3/media/7f156f9f3b8921e76230cc6e2927d504.png"></p><p>然后在关联的github下建立一个Dockerfile</p><p><img src="/2018/09/06/k8s%E5%B8%B8%E8%A7%81%E6%96%B9%E6%B3%95%E5%8F%8A%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3/media/333495aea16857eaaf52ddf42866438b.png"></p><p>Dockerfile的内容就写一个你要拉取的国外镜像</p><p><img src="/2018/09/06/k8s%E5%B8%B8%E8%A7%81%E6%96%B9%E6%B3%95%E5%8F%8A%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3/media/b165ca3332616bcb3d6b36527c87ddbd.png"></p><p>这样就会关联过来一个，我这里只是举例，关联的并不是jenkin那个github项目</p><p><img src="/2018/09/06/k8s%E5%B8%B8%E8%A7%81%E6%96%B9%E6%B3%95%E5%8F%8A%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3/media/7fd49d0468f4c664d8c50fbc1115c0d4.png"></p><p>设置构建操作，点击那个trigger按键即可开始根据Dockerfile内容构建</p><p><img src="/2018/09/06/k8s%E5%B8%B8%E8%A7%81%E6%96%B9%E6%B3%95%E5%8F%8A%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3/media/2d9c0fc642bd39da68847d819ee9c3d1.png"></p><p>查看构建状态</p><p><img src="/2018/09/06/k8s%E5%B8%B8%E8%A7%81%E6%96%B9%E6%B3%95%E5%8F%8A%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3/media/89fb0fae28a9610ff15d215e0a9bd4cd.png"></p><p>然后即可看到成功构建这个镜像了，你只需要pull到你本地，然后tag改下名字就是了，原理就是国内没法pull镜像，我们就利用dockerhub可以访问国外镜像来帮我们拉取，我们再从dockerhub去拉取</p><p><img src="/2018/09/06/k8s%E5%B8%B8%E8%A7%81%E6%96%B9%E6%B3%95%E5%8F%8A%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3/media/4eeb4bbaae00073c3858d4f295ceb719.png"></p><h1 id="Error-‘dial-tcp-172-30-13-5-9090-getsockopt-no-route-to-host’"><a href="#Error-‘dial-tcp-172-30-13-5-9090-getsockopt-no-route-to-host’" class="headerlink" title="Error: ‘dial tcp 172.30.13.5:9090: getsockopt: no route to host’"></a>Error: ‘dial tcp 172.30.13.5:9090: getsockopt: no route to host’</h1><p>Trying to reach: ‘<a href="http://172.30.13.5:9090/&#39;">http://172.30.13.5:9090/&#39;</a></p><ul><li><p>  flanneld服务停止了；</p></li><li><p>  flanneld服务运行正常，在排查iptables防火墙</p></li></ul><p>iptables -F</p><p>iptables -X</p><p>iptables -Z</p><p>iptables -P INPUT ACCEPT</p><p>iptables -P OUTPUT ACCEPT</p><p>iptables -P FORWARD ACCEPT</p><p>iptables-save &gt; /etc/iptables.rules</p><ul><li><p>  或者是否是具体服务的pod ip变了；</p></li><li><p>  检查master和node是否都有安装flanneld服务</p></li></ul><h1 id="集群ip无法访问，节点可以访问"><a href="#集群ip无法访问，节点可以访问" class="headerlink" title="集群ip无法访问，节点可以访问"></a>集群ip无法访问，节点可以访问</h1><p>集群ip是提供给集群访问的，比如你的master节点只是安装了api，controller，scheduler，并未安装kubelet和kube-proxy他是不能使用集群ip进行测试访问服务的</p><h1 id="未解决-device-or-resource-busy"><a href="#未解决-device-or-resource-busy" class="headerlink" title="未解决*device or resource busy"></a>未解决*device or resource busy</h1><p>FailedMount MountVolume.SetUp failed for volume<br>“kubernetes.io/configmap/fc973baa-6d50-11e8-8999-000c296e73e7-influxdb-config”<br>(spec.Name: “influxdb-config”) pod “fc973baa-6d50-11e8-8999-000c296e73e7” (UID:<br>“fc973baa-6d50-11e8-8999-000c296e73e7”) with: remove<br>/var/lib/kubelet/pods/fc973baa-6d50-11e8-8999-000c296e73e7/volumes/kubernetes.io~configmap/influxdb-config/resolv.conf:<br>device or resource busy</p><h1 id="kibana报错Request-Timeout-after-30000ms"><a href="#kibana报错Request-Timeout-after-30000ms" class="headerlink" title="kibana报错Request Timeout after 30000ms"></a>kibana报错Request Timeout after 30000ms</h1><p>加大elasticsearch内存。Node节点内存不足</p><p>或者修改kibana的检测时间</p><p>sed -i ‘/# elasticsearch.requestTimeout:<br>30000/a\\elasticsearch.requestTimeout: 100000’ /kibana/config/kibana.yml</p><p>我的最无语的是：</p><p>配置的kube-dns因为iptables防火墙的原因被堵塞了。导致无法解析ip地址。</p><h1 id="Unable-to-connect-to-Elasticsearch-at-http-elasticsearch-logging-9200"><a href="#Unable-to-connect-to-Elasticsearch-at-http-elasticsearch-logging-9200" class="headerlink" title="Unable to connect to Elasticsearch at http://elasticsearch-logging:9200."></a>Unable to connect to Elasticsearch at <a href="http://elasticsearch-logging:9200/">http://elasticsearch-logging:9200</a>.</h1><p>因为我的 elasticsearch镜像建立错了，导致 elasticsearch实际没有运行</p><h1 id="pod处于pending状态"><a href="#pod处于pending状态" class="headerlink" title="pod处于pending状态"></a>pod处于pending状态</h1><p>如果master节点运⾏的是kuberentes1.1.或更⾼版本，⽽node节点的版本低于1.1版本，则API<br>server将也可以接受新的特权模式的pod，但是⽆法启动，pod将处于pending状态。</p><p>执⾏ kubectl describe pod FooPodName<br>，可以看到为什么pod处于pending状态。输出的event列表中将显示：</p><p>Error validating pod”FooPodName”.”FooPodNamespace” from api,<br>ignoring:spec.containers[0].securityContext.privileged: forbidden<br>‘&lt;*&gt;(0xc2089d3248)true’</p><p>如果master节点的版本低于1.1，⽆法创建特权模式的pod。如果你仍然试图去创建的话，你得到如下错误：</p><p>The Pod “FooPodName” is invalid.spec.containers[0].securityContext.privileged:<br>forbidden ‘&lt;*&gt;(0xc20b222db0)true’</p><h1 id="K8s共用仓库私有项目认证账户"><a href="#K8s共用仓库私有项目认证账户" class="headerlink" title="K8s共用仓库私有项目认证账户"></a>K8s共用仓库私有项目认证账户</h1><p>kubernetes.io/dockerconfigjson</p><p>可以直接用 kubectl 命令来创建用于docker registry认证的secret：</p><p>$ kubectl create secret docker-registry myregistrykey \</p><p>--docker-server=DOCKER_REGISTRY_SERVER \</p><p>--docker-username=DOCKER_USER –docker-password=DOCKER_PASSWORD \</p><p>--docker-email=DOCKER_EMAIL</p><p>secret “myregistrykey” created.</p><p>这种方式需要你已经定义上述的环境变量，如果没有定义，则无法获取变量值，会失败。</p><p>也可以直接读取 ~/.docker/config.json 的内容来创建：</p><p>可以查看到你的docker服务已经存储的仓库认证信息</p><p>$ cat ~/.docker/config.json</p><p>进行base64编码格式加密转换</p><p>$ cat ~/.docker/config.json | base64</p><p>也可将显示的密文使用解密进行查看</p><p>echo “你的密文” | base64 –decode</p><p>$ vim myregistrykey.yaml</p><p>apiVersion: v1</p><p>kind: Secret</p><p>metadata:</p><p>name: myregistrykey</p><p>data:</p><p>.dockerconfigjson: 密文是连续的，不能分开</p><p>type: kubernetes.io/dockerconfigjson</p><p>$ kubectl create -f myregistrykey.yaml</p><p>建立一个pod；来测试，我这里是测试拉取本地的仓库中私有项目的image，这里就需要用到认证</p><p>vim testreg.yaml</p><p>apiVersion: v1</p><p>kind: Pod</p><p>metadata:</p><p>name: testreg</p><p>spec:</p><p>containers:</p><p>- name: foo</p><p>image: 172.16.3.55:8000/kubernetes/kibana:v4.6.1-1</p><p>imagePullSecrets:</p><p>- name: myregistrykey</p><p>~</p><h1 id="Influxdb数据库web无法访问"><a href="#Influxdb数据库web无法访问" class="headerlink" title="Influxdb数据库web无法访问"></a>Influxdb数据库web无法访问</h1><p>0 error - Could not connect to <a href="http://172.30.32.2:8086/">http://172.30.32.2:8086</a></p><p>Hint: the InfluxDB API runs on port 8086 by default</p><p>当使用http方式访问的时间输入node ip：映射的端口即可访问，使用https方式就不行</p><h1 id="Secret-configmap更新后pod中以环境变量的值并没跟着更新"><a href="#Secret-configmap更新后pod中以环境变量的值并没跟着更新" class="headerlink" title="Secret/configmap更新后pod中以环境变量的值并没跟着更新 "></a>Secret/configmap更新后pod中以环境变量的值并没跟着更新 </h1><p>secret挂载到里面去使用的是挂载方式的支持自动更新的，使用环境变量方式的是不支持自动更新的</p><h1 id="K8s中同一个namespace中的所有pod使用私有docker-registry的认证信息"><a href="#K8s中同一个namespace中的所有pod使用私有docker-registry的认证信息" class="headerlink" title="K8s中同一个namespace中的所有pod使用私有docker registry的认证信息"></a>K8s中同一个namespace中的所有pod使用私有docker registry的认证信息</h1><p>$ cat ~/.docker/config.json</p><p>进行base64编码格式加密转换</p><p>$ cat ~/.docker/config.json | base64</p><p>也可将显示的密文使用解密进行查看</p><p>echo “你的密文” | base64 –decode</p><p>$ vim myregistrykey.yaml</p><p>apiVersion: v1</p><p>kind: Secret</p><p>metadata:</p><p>name: myregistrykey</p><p>data:</p><p>.dockerconfigjson: 密文是连续的，不能分开</p><p>type: kubernetes.io/dockerconfigjson</p><p>$ kubectl create -f myregistrykey.yaml</p><p>1.命令方式</p><p>建立好后，在ServiceAccount中新增imagePullSecrets<br>的Secret为myregistrykey（这里使用的是ServiceAccount名为default的。）</p><p>kubectl patch serviceaccount default -p ‘{“imagePullSecrets”: [{“name”:<br>“myregistrykey”}]}’</p><p>2.yaml文件方式</p><p>$ kubectl get serviceaccounts default -o yaml &gt; ./sa.yaml</p><p>$ cat sa.yaml</p><p>apiVersion: v1</p><p>kind: ServiceAccount</p><p>metadata:</p><p>creationTimestamp: 2015-08-07T22:02:39Z</p><p>name: default</p><p>namespace: default</p><p>resourceVersion: “243024”</p><p>selfLink: /api/v1/namespaces/default/serviceaccounts/default</p><p>uid: 052fb0f4-3d50-11e5-b066-42010af0d7b6</p><p>secrets:</p><p>- name: default-token-uudge</p><p>$ vi sa.yaml（这里是新增和删除的部分）</p><p>[editor session not shown]</p><p>[delete line with key “resourceVersion”]</p><p>[add lines with “imagePullSecret:”]</p><p>$ cat sa.yaml</p><p>apiVersion: v1</p><p>kind: ServiceAccount</p><p>metadata:</p><p>creationTimestamp: 2015-08-07T22:02:39Z</p><p>name: default</p><p>namespace: default</p><p>selfLink: /api/v1/namespaces/default/serviceaccounts/default</p><p>uid: 052fb0f4-3d50-11e5-b066-42010af0d7b6</p><p>secrets:</p><p>- name: default-token-uudge</p><p>imagePullSecrets:</p><p>- name: myregistrykey</p><p>$ kubectl replace serviceaccount default -f ./sa.yaml</p><p>新建的ServiceAccount会默认使用secret的token</p><p>每个namespace都有一个默认的serviceaccount（default）</p><h1 id="uid-unable-to-do-port-forwarding-socat-not-found"><a href="#uid-unable-to-do-port-forwarding-socat-not-found" class="headerlink" title="uid : unable to do port forwarding: socat not found"></a>uid : unable to do port forwarding: socat not found</h1><p>Kubectl port-forward实现使用localhost_ip访问并且可以进行pod调度</p><p>在节点上需要安装socat</p><p>yum –y install socat</p><h1 id="flannel不能随意重启，因为会导致每个节点从etcd获取的网段不同。"><a href="#flannel不能随意重启，因为会导致每个节点从etcd获取的网段不同。" class="headerlink" title="flannel不能随意重启，因为会导致每个节点从etcd获取的网段不同。"></a>flannel不能随意重启，因为会导致每个节点从etcd获取的网段不同。</h1><h1 id="Flanneld维护整个容器"><a href="#Flanneld维护整个容器" class="headerlink" title="Flanneld维护整个容器"></a>Flanneld维护整个容器</h1><p>Flanneld是容器的路由表，所以就算是你使用了nodeip加上映射后的端口访问也是要经过flannel的，当关闭flannel的时候，docker也关闭了，这是因为在flannel.service服务文件中定义的。</p><h1 id="Ingree的path加了后提示404找不到"><a href="#Ingree的path加了后提示404找不到" class="headerlink" title="Ingree的path加了后提示404找不到"></a>Ingree的path加了后提示404找不到</h1><p>-path的路径会带入后端服务中，比如:</p><p>Nginx容器的根目录为/usr/share/nginx/html/</p><p>而我的ingree定义是这样的</p><p>- path: /nginx</p><p>backend:</p><p>serviceName: my-nginx</p><p>servicePort: web</p><p>那么我需要在nginx容器的根目录下新建一个nginx文件夹</p><p>mkdir –p /usr/share/nginx/html/nginx 然后将网页放在这里，不然会提示找不到404</p><h1 id="treafik的web显示"><a href="#treafik的web显示" class="headerlink" title="treafik的web显示"></a>treafik的web显示</h1><p>是以一个kind：ingree的yaml文件中第一个出现的host作为前端名FRONTENDS显示，这样其实不够严谨，没法区分前后端。</p><h1 id="PodExceedsFreeMemory"><a href="#PodExceedsFreeMemory" class="headerlink" title="PodExceedsFreeMemory"></a>PodExceedsFreeMemory</h1><p>Node内存不足，无法调度</p><h1 id="Error-from-server-ServerTimeout"><a href="#Error-from-server-ServerTimeout" class="headerlink" title="Error from server (ServerTimeout)"></a>Error from server (ServerTimeout)</h1><p>重启apiserver服务</p><h1 id="解决heapster无法连接报错"><a href="#解决heapster无法连接报错" class="headerlink" title="解决heapster无法连接报错"></a>解决heapster无法连接报错</h1><p>E1101 08:56:05.014891 1 summary.go:97] error while getting metrics summary from<br>Kubelet kube-node2(192.168.100.82:10255): Get<br><a href="http://192.168.100.82:10255/stats/summary/">http://192.168.100.82:10255/stats/summary/</a>: dial tcp 192.168.100.82:10255:<br>getsockopt: connection refused</p><p>第一种方式</p><p>参考<a href="https://github.com/kubernetes/heapster/issues/1586">https://github.com/kubernetes/heapster/issues/1586</a></p><p>是不是覆盖网络的问题，优化calico网络组件</p><p>第二种方式，可以成功运行</p><p>因为默认10250端口号是kubelet的只读端口，版本高以后默认是关闭的，首先确认kubelet服务是否正常运行</p><p>[root@kube-node1 ~]# netstat -lntp | grep kubelet</p><p>tcp 0 0 127.0.0.1:10248 0.0.0.0:* LISTEN 74042/kubelet</p><p>tcp 0 0 127.0.0.1:43411 0.0.0.0:* LISTEN 74042/kubelet</p><p>tcp6 0 0 :::10250 :::* LISTEN 74042/kubelet</p><p>明显看到上面没有运行10255端口，这时我们修改kubelet的配置文件，从哪里获取配置文件位置呢？</p><p>使用命令systemctl status kubelet<br>-l可以获取kubelet启动加了哪些参数，列如我这里就获取到是</p><p><img src="/2018/09/06/k8s%E5%B8%B8%E8%A7%81%E6%96%B9%E6%B3%95%E5%8F%8A%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3/media/4b9059dffedd9cb794516242677fece8.png"></p><p>编辑配置文件，看其中是否有下面的配置，没有就新加，为了方面查看就加到port的下面，然后重启服务就可以查看到10255端口启用了。所有的node都需要修改</p><p><img src="/2018/09/06/k8s%E5%B8%B8%E8%A7%81%E6%96%B9%E6%B3%95%E5%8F%8A%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3/media/a5d326f388de176af47e36eb8825cdab.png"></p><p>readOnlyPort: 10255</p><p>第三种方式</p><p>修改heapster的启动参数 -<br>–source=kubernetes:<a href="https://kubernetes.default/?kubeletHttps=true&amp;kubeletPort=10250">https://kubernetes.default?kubeletHttps=true&amp;kubeletPort=10250</a></p><p>提示下面的错误</p><p>E1101 09:17:05.022691 1 kubelet.go:231] error while getting containers from<br>Kubelet: failed to get all container stats from Kubelet URL<br>“<a href="https://192.168.100.82:10250/stats/container/&quot;">https://192.168.100.82:10250/stats/container/&quot;</a>: Post<br><a href="https://192.168.100.82:10250/stats/container/">https://192.168.100.82:10250/stats/container/</a>: x509: cannot validate certificate<br>for 192.168.100.82 because it doesn’t contain any IP SANs</p><p>第四种方式</p><p>-<br>–source=kubernetes:<a href="https://kubernetes.default/?inClusterConfig=false&amp;useServiceAccount=true&amp;auth=">https://kubernetes.default?inClusterConfig=false&amp;useServiceAccount=true&amp;auth=</a></p><p>提示下面的错误</p><p>E1101 09:49:48.557497 1 reflector.go:190]<br>k8s.io/heapster/metrics/processors/namespace_based_enricher.go:84: Failed to<br>list *v1.Namespace: Get<br><a href="https://kubernetes.default/api/v1/namespaces?resourceVersion=0">https://kubernetes.default/api/v1/namespaces?resourceVersion=0</a>: x509:<br>certificate signed by unknown authority</p><p>E1101 09:49:48.558574 1 reflector.go:190]<br>k8s.io/heapster/metrics/heapster.go:322: Failed to list *v1.Pod: Get<br><a href="https://kubernetes.default/api/v1/pods?resourceVersion=0">https://kubernetes.default/api/v1/pods?resourceVersion=0</a>: x509: certificate<br>signed by unknown authority</p><p>E1101 09:49:48.592681 1 reflector.go:190]<br>k8s.io/heapster/metrics/util/util.go:51: Failed to list *v1.Node: Get<br><a href="https://kubernetes.default/api/v1/nodes?resourceVersion=0">https://kubernetes.default/api/v1/nodes?resourceVersion=0</a>: x509: certificate<br>signed by unknown authority</p><h1 id="Invalid-Kubernetes-API-v1-endpoint-https-10-254-0-1-443-api-SSL-connect-returned-1-errno-0-state-error-certificate-verify-failed"><a href="#Invalid-Kubernetes-API-v1-endpoint-https-10-254-0-1-443-api-SSL-connect-returned-1-errno-0-state-error-certificate-verify-failed" class="headerlink" title="Invalid Kubernetes API v1 endpoint https://10.254.0.1:443/api: SSL_connect returned=1 errno=0 state=error: certificate verify failed"></a>Invalid Kubernetes API v1 endpoint <a href="https://10.254.0.1/api">https://10.254.0.1:443/api</a>: SSL_connect returned=1 errno=0 state=error: certificate verify failed</h1><p>docker run –rm -it –entrypoint=”cat” huisebug/fluentd-elasticsearch:1.22<br>/etc/td-agent/td-agent.conf &gt; td-agent.conf</p><p>修改文件内容</p><p>在td-agent.conf的配置文件的中增加两条配置配置：（注意是新加两条，不是新建）</p><p>&lt;filter kubernetes.**&gt;</p><p>type kubernetes_metadata</p><p>kubernetes_url kubeapi地址:8080</p><p>verify_ssl false</p><p>&lt;/filter&gt;</p><p>创建ConfigMap</p><p>kubectl create configmap td-agent-config –from-file=./td-agent.conf -n<br>kube-system</p><p>在fluend.yaml中新加configmap</p><p>- name: td-agent-config</p><p>mountPath: /etc/td-agent</p><p>…</p><p>volumes:</p><p>- name: td-agent-config</p><p>configMap:</p><p>name: td-agent-config</p><h1 id="efk提示open-var-run-secrets-kubernetes-io-serviceaccount-token-no-such-file-or-directory"><a href="#efk提示open-var-run-secrets-kubernetes-io-serviceaccount-token-no-such-file-or-directory" class="headerlink" title="efk提示open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory"></a>efk提示open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory</h1><p>efk的建立中e的建立确保你的k8s支持serviceaccout是否开启，不然会导致token没建立，/elasticsearch_logging_discovery无法访问API<br>Server导致 /elasticsearch/config/elasticsearch.yml没有被正确生成造成的，</p><h1 id="Grafana在Pods仪表板不显示终止的pod"><a href="#Grafana在Pods仪表板不显示终止的pod" class="headerlink" title="Grafana在Pods仪表板不显示终止的pod"></a>Grafana在Pods仪表板不显示终止的pod</h1><p>默认的pod查询语句是这样的</p><p>我们需要在influxdb中新建一个采样数据，然后将其指向采样数据，这样就会过滤掉失效的pod</p><p>Influxdb建立时需要开启web UI默认访问端口是8083，</p><p>我这里使用获取的svc经过nodeport方式后进行访问</p><p>http://服务器ip:31899</p><p>连接数据库中可能会提示找不到数据库，因为系统会有一个默认配置，如果你发现配置错误，比如我这里是服务器ip：31004，连接成功后即可show<br>database。</p><p>然后在query栏执行以下两句influxdb sql</p><p>CREATE RETENTION POLICY “2hours” ON “k8s” DURATION 2h REPLICATION 1</p><p>CREATE CONTINUOUS QUERY current_pods_query ON k8s BEGIN SELECT max(value) AS<br>value INTO k8s.”2hours”.current_pods FROM k8s.”default”.uptime WHERE type =<br>‘pod’ GROUP BY time(5m), namespace_name, nodename, pod_name END</p><p>执行完毕后切换回grafana配置界面，修改query语句</p><p>SHOW TAG VALUES FROM k8s.”2hours”.current_pods WITH KEY = “pod_name” WHERE<br>“namespace_name” =~ /$namespace$/</p><p>保存修改，一段时间后即可看到失效pod就没有了</p><h1 id="pod-Spec-SecurityContext-RunAsUser-is-forbidden容器不调度"><a href="#pod-Spec-SecurityContext-RunAsUser-is-forbidden容器不调度" class="headerlink" title="pod.Spec.SecurityContext.RunAsUser is forbidden容器不调度"></a>pod.Spec.SecurityContext.RunAsUser is forbidden容器不调度</h1><p>是因为apiserver开启了SecurityConrextDeny，将其去除</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;k8s常见方法及错误解决方法&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes" scheme="https://huisebug.github.io/categories/Kubernetes/"/>
    
    
    <category term="docker" scheme="https://huisebug.github.io/tags/docker/"/>
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="prometheus" scheme="https://huisebug.github.io/tags/prometheus/"/>
    
  </entry>
  
  <entry>
    <title>Docker-machine+Docker swarm+Nginx+Nginx代理</title>
    <link href="https://huisebug.github.io/2017/12/19/docker-swarm/"/>
    <id>https://huisebug.github.io/2017/12/19/docker-swarm/</id>
    <published>2017-12-19T09:04:01.000Z</published>
    <updated>2021-07-09T02:59:43.102Z</updated>
    
    <content type="html"><![CDATA[<p>这是一个swarm集群，从零开始搭建服务器环境，Docker-machine+Docker swarm+Nginx+Nginx代理</p><span id="more"></span><h1 id="Docker-machine-Docker-swarm-Nginx-Nginx代理"><a href="#Docker-machine-Docker-swarm-Nginx-Nginx代理" class="headerlink" title="Docker-machine+Docker swarm+Nginx+Nginx代理"></a>Docker-machine+Docker swarm+Nginx+Nginx代理</h1><h2 id="准备环境："><a href="#准备环境：" class="headerlink" title="准备环境："></a>准备环境：</h2><ul><li> 6台centos7操作系统（需要关闭防火墙，关闭selinux）</li></ul><p><img src="/2017/12/19/docker-swarm/media/86bdee66a4904907a8cfb23411ec6a20.png"></p><ul><li> IP地址划分</li></ul><table><thead><tr><th>Docker-machine</th><th>Swarm node1</th><th>Swarm node1</th><th>Swarm node1</th><th>Nginx服务</th><th>Nginx代理</th></tr></thead><tbody><tr><td>192.168.100.70</td><td>192.168.100.71</td><td>192.168.100.72</td><td>192.168.100.73</td><td>192.168.100.74</td><td>192.168.100.75</td></tr></tbody></table><ul><li><p> docker版本</p></li><li><p> 初始环境要求：</p></li></ul><p>准备的是全新的操作系统，只需IP地址配置ok，全网可以互通。</p><h2 id="项目执行之前需要实现的要求"><a href="#项目执行之前需要实现的要求" class="headerlink" title="项目执行之前需要实现的要求"></a>项目执行之前需要实现的要求</h2><ul><li><p>  全部不使用root，关闭密码验证，使用ssh密钥进行验证。</p></li><li><p>  全部机器使用IP地址为70的这台作为堡垒机进行管理。</p></li><li><p>  要求nginx代理为外部的提供访问的地址，（如果允许，可以再搭建一台dns解析服务和使用一台win7来进行测试。）</p></li><li><p>  后续更多需求可根据实际进行添加。</p></li></ul><h2 id="修改不使用root，使用新用户admin操作"><a href="#修改不使用root，使用新用户admin操作" class="headerlink" title="修改不使用root，使用新用户admin操作"></a>修改不使用root，使用新用户admin操作</h2><h3 id="远程软件连接"><a href="#远程软件连接" class="headerlink" title="远程软件连接"></a>远程软件连接</h3><p>使用终端软件连接，暂时没有关闭密钥验证和不使用root登录，所以使用root先登录</p><p><img src="/2017/12/19/docker-swarm/media/7122132bd524c46f401e4ceeb021940f.png"></p><h3 id="上传expect脚本"><a href="#上传expect脚本" class="headerlink" title="上传expect脚本"></a>上传expect脚本</h3><p>将我当初写的expect脚本复制到作为堡垒机的100.70这台服务器</p><p><img src="/2017/12/19/docker-swarm/media/d0ba236e97aed08cf0d7189e3275e8f6.png"></p><h4 id="简介："><a href="#简介：" class="headerlink" title="简介："></a>简介：</h4><ul><li>login.exp：里面包含了登录、新建用户、授权sudo使用权限、设置用户密码、复制公钥到新建用户的ssh目录下并关闭sshd服务的端口号和禁止密码验证登录操作。</li><li>scp.exp：里面包含了将公钥上传到指定的服务器的指定目录下，以便提供给login.exp登录进行操作。</li><li>sshlogin.sh:这里定义的是exp脚本中将会使用位置变量的参数，在这里定义后，exp脚本可以引用shell脚本的变量。</li><li>wyf.pub：这是公钥文件，自己去生成一个</li><li>passwd.txt：这个是shell脚本sshlogin.sh使用for循环的时候会使用到的各个位置变量的参数值。</li><li>use.txt：中文操作手册<h4 id="文件内容："><a href="#文件内容：" class="headerlink" title="文件内容："></a>文件内容：</h4></li></ul><p>login.exp</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/usr/bin/expect -f</span></span><br><span class="line"></span><br><span class="line">set ipaddress [lindex $argv 0]</span><br><span class="line"></span><br><span class="line">set passwd [lindex $argv 1]</span><br><span class="line"></span><br><span class="line">set user [lindex $argv 2]</span><br><span class="line"></span><br><span class="line">set pubkey [lindex $argv 3]</span><br><span class="line"></span><br><span class="line">set sshport [lindex $argv 4]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">set</span> sudo <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$user</span> ALL=(ALL) ALL</span></span></span><br><span class="line"></span><br><span class="line">set timeout 30</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="string">登入操作</span></span></span><br><span class="line">spawn ssh root@$ipaddress</span><br><span class="line"></span><br><span class="line">expect &#123;</span><br><span class="line">&quot;yes/no&quot; &#123; send &quot;yes\r&quot;;exp_continue &#125;</span><br><span class="line">&quot;password:&quot; &#123; send &quot;$passwd\r&quot; &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">expect &#123;</span><br><span class="line">&quot;*from*&quot; &#123;</span><br><span class="line">send &quot;useradd $user -m -s /bin/bash\r&quot;</span><br><span class="line">send &quot;echo &#x27;$user ALL=(ALL) ALL&#x27; &gt;&gt; /etc/sudoers\r&quot;</span><br><span class="line">send &quot;chown $user: /tmp/$pubkey\r&quot;&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="string">修改用户密码</span></span></span><br><span class="line">send &quot;passwd $user\r&quot;</span><br><span class="line">expect &#123;</span><br><span class="line">&quot;UNIX&quot; &#123; send &quot;pwD@1234\r&quot;;exp_continue&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">expect &#123;</span><br><span class="line">&quot;#&quot; &#123; send &quot;su - $user\r&quot;</span><br><span class="line">send &quot;mkdir -p /home/$user/.ssh/\r&quot;</span><br><span class="line">send &quot;chmod 700 /home/$user/.ssh\r&quot;</span><br><span class="line">send &quot;cat /tmp/$pubkey &gt; /home/$user/.ssh/authorized_keys\r&quot;</span><br><span class="line">send &quot;chmod 600 /home/$user/.ssh/authorized_keys\r&quot;</span><br><span class="line">send &quot;exit\r&quot;</span><br><span class="line">send &quot;sed -i &#x27;s/#Port 22/Port $sshport/&#x27; /etc/ssh/sshd_config\r&quot;</span><br><span class="line">send &quot;sed -i &#x27;s/Port 22/Port $sshport/&#x27; /etc/ssh/sshd_config\r&quot;</span><br><span class="line">send &quot;sed -i &#x27;s/PasswordAuthentication yes/PasswordAuthentication no/&#x27; /etc/ssh/sshd_config\r&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="string">send &quot;</span>systemctl restart sshd\r<span class="string">&quot;</span></span></span><br><span class="line">send &quot;exit\r&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">interact</span><br></pre></td></tr></table></figure><p>scp.exp</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/usr/bin/expect -f</span></span><br><span class="line"></span><br><span class="line">set ipaddress [lindex $argv 0]</span><br><span class="line"></span><br><span class="line">set passwd [lindex $argv 1]</span><br><span class="line"></span><br><span class="line">set user [lindex $argv 2]</span><br><span class="line"></span><br><span class="line">set pubkey [lindex $argv 3]</span><br><span class="line"></span><br><span class="line">set timeout 30</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">上传密钥wyf</span></span><br><span class="line">spawn scp $pubkey root@$ipaddress:/tmp/</span><br><span class="line"></span><br><span class="line">expect &#123;</span><br><span class="line">&quot;yes/no&quot; &#123; send &quot;yes\r&quot;&#125;</span><br><span class="line">&quot;password:&quot; &#123; send &quot;$passwd\r&quot; &#125;</span><br><span class="line">&quot;#&quot; &#123; send &quot;exit\r&quot; &#125;&#125;</span><br><span class="line">interact</span><br><span class="line"><span class="meta">#</span><span class="bash">expect <span class="string">&quot;<span class="variable">$pubkey</span>&quot;</span></span></span><br></pre></td></tr></table></figure><p>sshlogin.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">for i in `awk &#x27;&#123;print $1&#125;&#x27; $PWD/passwd.txt`</span><br><span class="line"></span><br><span class="line">do</span><br><span class="line"></span><br><span class="line">j=`awk -v I=&quot;$i&quot; &#x27;&#123;if(I==$1)print $2&#125;&#x27; $PWD/passwd.txt`</span><br><span class="line">u=`awk -v I=&quot;$i&quot; &#x27;&#123;if(I==$1)print $3&#125;&#x27; $PWD/passwd.txt`</span><br><span class="line">p=`awk -v I=&quot;$i&quot; &#x27;&#123;if(I==$1)print $4&#125;&#x27; $PWD/passwd.txt`</span><br><span class="line">port=`awk -v I=&quot;$i&quot; &#x27;&#123;if(I==$1)print $5&#x27; $PWD/passwd.txt`</span><br><span class="line">expect $PWD/scp.exp $i $j $u $p $port</span><br><span class="line">expect $PWD/login.exp $i $j $u $p $port</span><br><span class="line"></span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>wyf.pub </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-dss AAAAB3NzaC1kc3MAAACBANVB2rrIvkUGCJA6b/8h+GyKd1GVpxZFAM1n8sP2bo7TSkWpkxV81Y4zkdaYomsFDXLGnaI2QbHqv/pA0PndJFQ1NfYqbBw6FaGI1+cG8mgJBCPwVPmQnDvf4TPzJkEpdV1CK/wYjRVjuFL2lYOd8hepkNN4c96Dx5STdMyBSH+rAAAAFQCtUm3dYyX3j9pKY+yev3OKmB/LtwAAAIEAytEQ4hgceD7T00soGq/o0aS42QJoeZLJd/jwdIAqkUdg5y8q4lqzmlO2OaxD3YVf9eOYiuokZQu2qP5A+2iTbebpoqDDP8NksCgGPY+7viyKpUy+wrcm/Gwa0YA9B2mzpHudxXKuw/2wjtBKTjh+gFx7zlXBFHYaMdV8zsP/QkAAAACBALJc+rMJXSsS7+3i874iJIlP/r2qxrfkO4+qjmZoTbGYnhOzEzYbwP6newO27MBkaOHSPlWFh8kSQtiputcvg9GawG6Y6WO/0bfgHfxrDg0NnmQJgvcjMStGeuLwYB4NNG2KsomciN4hUruWHg0/+VwV8qknTHYFMJmOJC1ketuS</span><br></pre></td></tr></table></figure><p>passwd.txt</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">192.168.100.71 pwd@123 wyf5 wyf.pub 234</span><br></pre></td></tr></table></figure><p>use.txt</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">复制当前目录下的所有文件到同一个文件夹中</span><br><span class="line"></span><br><span class="line">只需要定义passwd.txt中的值即可</span><br><span class="line">格式为:列如像这样</span><br><span class="line">IP地址 root密码 新建用户名 公钥文件 sshd端口号</span><br><span class="line">192.168.100.71 pwd@123 wyf5 wyf.pub 234</span><br><span class="line"></span><br><span class="line">定义后执行sh脚本文件即可</span><br><span class="line"></span><br><span class="line">前置动作，yum -y install expect</span><br><span class="line">执行脚本的系统最好是使用英文的，不然在提示栏会显示中文。这是基于centos系统的</span><br></pre></td></tr></table></figure><h3 id="expect脚本的配置文件修改"><a href="#expect脚本的配置文件修改" class="headerlink" title="expect脚本的配置文件修改"></a>expect脚本的配置文件修改</h3><h4 id="将其他5台服务器的信息写到passwd文件中"><a href="#将其他5台服务器的信息写到passwd文件中" class="headerlink" title="将其他5台服务器的信息写到passwd文件中"></a>将其他5台服务器的信息写到passwd文件中</h4><p><img src="/2017/12/19/docker-swarm/media/629185cf57397b00b7234c6afd0b5250.png"></p><h4 id="安装expect脚本执行命令"><a href="#安装expect脚本执行命令" class="headerlink" title="安装expect脚本执行命令"></a>安装expect脚本执行命令</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install expect（这里我已经安装）</span><br></pre></td></tr></table></figure><p><img src="/2017/12/19/docker-swarm/media/4d71ec33e30dadbe02111c64f6de4a97.png"></p><h4 id="检查各脚本是否有错误的地方"><a href="#检查各脚本是否有错误的地方" class="headerlink" title="检查各脚本是否有错误的地方"></a>检查各脚本是否有错误的地方</h4><p>在login.exp这个脚本中重启sshd服务的操作，我注释掉了，先不重启，我们待会分别进去查看是否修改了配置文件，也是确保修改正确后再进行重启操作。<br><img src="/2017/12/19/docker-swarm/media/cad50107107cca9b8c4afd6578db5ebc.png"></p><p>执行过程中，因为中文支持的原因提示的密码输入不一样，所以将提示修改为这样的（这里可以看出操作系统不同，所提示的也不同，需要对应操作系统来修改这里）<br><img src="/2017/12/19/docker-swarm/media/dc58bc2891da1ea61b73f511044a026c.png"></p><p>因为操作系统的原因，每台sshd_config的配置文件也不同，比如centos7的port前面会有一个#号，其他系统可能不同;<br>所以我们可以修改写两个sed<br><img src="/2017/12/19/docker-swarm/media/12500c4ff12464e3cf99e06284290868.png"></p><p>还有一个问题需要说明，我这里写成了222端口是问题的，比如#port 22,第一次修改后为port 222，第二次又会匹配下一条sed，既会变成port 2222。所以尽量不要用22开头的作为修改后的端口号<br>执行后在查看其中一台<br><img src="/2017/12/19/docker-swarm/media/ff3c8af84f358c2699e7521e2b0bcab1.png"><br><img src="/2017/12/19/docker-swarm/media/4871a018365baf8ad513d591767eb01c.png"></p><p>后续重启sshd可能不成功，因为selinux和防火墙的原因，需要关闭。<br>查看无问题了，既重新启动sshd服务。</p><h3 id="分别登录admin账户"><a href="#分别登录admin账户" class="headerlink" title="分别登录admin账户"></a>分别登录admin账户</h3><p><img src="/2017/12/19/docker-swarm/media/44dc6033f8d46355e662feee96ad4f9a.png"><br><img src="/2017/12/19/docker-swarm/media/6f87f9585c9a4cc3064e5f9d9a32d7d5.png"></p><p>我这里的</p><ul><li>root密码是pwd@123</li><li>admin密码是qwaszx@）！&amp;</li><li>wyf公钥的密码是1836</li></ul><p>上面这里因为关闭了密钥验证，所以这里要使用密钥的验证。</p><h2 id="安装docker的等一系列服务"><a href="#安装docker的等一系列服务" class="headerlink" title="安装docker的等一系列服务"></a>安装docker的等一系列服务</h2><h3 id="新增项目测试："><a href="#新增项目测试：" class="headerlink" title="新增项目测试："></a>新增项目测试：</h3><h4 id="这里的100-70机器是全新的一台机器，当我们只安装docker-machine的时候看是否一定需要docker服务"><a href="#这里的100-70机器是全新的一台机器，当我们只安装docker-machine的时候看是否一定需要docker服务" class="headerlink" title="这里的100.70机器是全新的一台机器，当我们只安装docker-machine的时候看是否一定需要docker服务"></a>这里的100.70机器是全新的一台机器，当我们只安装docker-machine的时候看是否一定需要docker服务</h4><h5 id="查看三台节点服务器情况："><a href="#查看三台节点服务器情况：" class="headerlink" title="查看三台节点服务器情况："></a>查看三台节点服务器情况：</h5><p>验证并没有安装docker服务<br><img src="/2017/12/19/docker-swarm/media/944d126be5553a428ddc4dd77a209cc6.png"></p><p>上传docker-machine到服务器并移动到全局变量下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">ls</span></span><br><span class="line">docker-machine</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash">sudo mv docker-machine /usr/bin/</span></span><br><span class="line"><span class="meta">$</span><span class="bash">sudo chmod +x /usr/bin/docker-machine</span></span><br><span class="line"><span class="meta">$</span><span class="bash">docker-machine ls</span></span><br><span class="line">NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash">docker-machine version</span></span><br><span class="line">docker-machine version 0.12.2, build 9371605</span><br></pre></td></tr></table></figure><p>先查看其他作为swarm集群的几台机器的情况</p><p><img src="/2017/12/19/docker-swarm/media/89e181f2d2aeb684fd0cc7c04115221e.png"><br><img src="/2017/12/19/docker-swarm/media/b2c1603f7279218e2064a1f0424a915f.png"><br><img src="/2017/12/19/docker-swarm/media/31482fe11be6a50646ffba08fa26145a.png"></p><p>可以看到上面的三个节点已经安装好了docker，</p><p>这里为什么需要sudo呢？</p><p>因为docker安装的时候是使用的root用户，admin用户是没有权限操作docker的。所以我们这里可以将admin用户加入到docker组（这个组是安装docker的时候自动创建的，这样admin就可以操作docker服务了）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gpasswd -a $&#123;USER&#125; docker</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -a -G docker admin</span><br></pre></td></tr></table></figure><p>重启docker</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure><p>这里有个小问题</p><p>查看组，<br><img src="/2017/12/19/docker-swarm/media/bb3cccc8b4e729af591fa0d7cd1916cc.png"></p><p>可以看到docker组中有admin这个用户，但是还是无法使用docker命令，从权限可以看出docker组是用读取写入权限的。但是admin还是无法使用docker<br><img src="/2017/12/19/docker-swarm/media/52032f76dc3f8d0dd40f2d624dfdfe73.png"></p><p>解决方法就是关闭这个终端，重新连接<br><img src="/2017/12/19/docker-swarm/media/fddf84d3c25c3d2ac18dc4016368f2b5.png"></p><h5 id="使用docker-machine-的驱动进行连接并管理其他服务器"><a href="#使用docker-machine-的驱动进行连接并管理其他服务器" class="headerlink" title="使用docker-machine 的驱动进行连接并管理其他服务器"></a>使用docker-machine 的驱动进行连接并管理其他服务器</h5><p>100.70作为docker-machine管理器，并使用admin用户操作。</p><p>在admin用户下生成密钥对<br><img src="/2017/12/19/docker-swarm/media/c01e6768e538c2263bfe8ab0a258565c.png"></p><p>将生成的公钥放到远程主机的admin（远程主机的admin账户是可以操作docker的）账户去，会提示输入远程主机的admin用户密码<br><img src="/2017/12/19/docker-swarm/media/7f14289ff65ae009238aa4d2b3b416b9.png"></p><p>这里出现的问题就是，我们已经关闭了每台主机的密码验证，这种是无法上传的，所以只有分别复制过去<br><img src="/2017/12/19/docker-swarm/media/b28e51e491fdf71f66c33e8b492fbe40.png"></p><p>第一个ssh是本地远程的xshell的公钥，第二个是100.70下的admin用户生成的公钥</p><p>测试能否远程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -p 222 admin@192.168.100.71</span><br></pre></td></tr></table></figure><p><img src="/2017/12/19/docker-swarm/media/f686a811841ae359acd5f602f833ee12.png"></p><p>开始关联</p><p>注意，create 命令本是要创建虚拟主机并安装Docker，因为本例中的目标主机已经存在docker。所以已经安装会跳过的。-d 是 –driver的简写形式，主要用来指定使用什么驱动程序来创建目标主机。Docker Machine支持在云服务器上创建主机，就是靠使用不同的驱动来实现了。本例中使用 generic就可以了。接下来以 –generic开头的三个参数主要是指定操作的目标主机和使用的账户。最后一个参数 node1是虚拟机的名称，Docker Machine 会用它来设置目标主机的名称。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-machine create -d generic --generic-ip-address=192.168.100.71 --generic-ssh-port=222 --generic-ssh-user=admin --generic-ssh-key ~/.ssh/id_rsa node1</span><br></pre></td></tr></table></figure><p><img src="/2017/12/19/docker-swarm/media/1d974a46bd49d743f09a8616061d3bfe.png"></p><p>这里会出错，原因是admin用户无法执行在创建过程中需要执行的命令。所以我们修改，将ssh密钥导入到root用户</p><p><img src="/2017/12/19/docker-swarm/media/8872f604b0912d38fe46ae3534000777.png"></p><p>删除node1，再次进行创建</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-machine create -d generic --generic-ip-address=192.168.100.71 --generic-ssh-port=222 --generic-ssh-user=root --generic-ssh-key ~/.ssh/id_rsa node1</span><br></pre></td></tr></table></figure><p>等待过10分钟，基本就可以了<br><img src="/2017/12/19/docker-swarm/media/b66840b33576ddefbb3cfccaa8dfbd79.png"></p><p>将另外的两个节点也加入进来</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i in 2 3 ; do docker-machine create -d generic --generic-ip-address=192.168.100.7$i --generic-ssh-port=222 --generic-ssh-user=root --generic-ssh-key ~/.ssh/id_rsa node$i; done</span><br></pre></td></tr></table></figure><p><img src="/2017/12/19/docker-swarm/media/389e936bf5bf2de9495c1d08a9da489b.png"></p><h4 id="测试效果"><a href="#测试效果" class="headerlink" title="测试效果"></a>测试效果</h4><p>现在测试100.70在不安装docker服务的情况下，使用了docker 节点的变量能否使用docker<br><img src="/2017/12/19/docker-swarm/media/5f407df039a332b7bd8e3f9d3e7c5250.png"></p><p>从上面可以看出，并不能，原因是因为没有安装docker客户端。这里我们就不研究如何只安装客户端了，还是安装上docker服务吧，使用admin用户来安装</p><p>安装教程<br><a href="https://docs.docker.com/engine/installation/linux/docker-ce/centos/#install-using-the-repository">https://docs.docker.com/engine/installation/linux/docker-ce/centos/#install-using-the-repository</a></p><p>这里当安装好了以后<br><img src="/2017/12/19/docker-swarm/media/ac73e7c39d1f36e9dde595a62f497bca.png"></p><p>上面可以看出安装的是docker-ce17.09版本，因为之前的变量导入还在，所以看到客户端版本为17.09，服务端是17.06。<br>移除变量后，再次查看，提示服务端并没运行，这样不就实现了我们的想法，只安装了客户端。<br><img src="/2017/12/19/docker-swarm/media/bc979658500c683434f18603cafbf57a.png"></p><p>如果想运行服务，需要加入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl enable docker</span><br><span class="line">sudo systemctl start docker</span><br></pre></td></tr></table></figure><p>这里我不加入开机启动，和启动服务端。</p><h3 id="建立swarm集群"><a href="#建立swarm集群" class="headerlink" title="建立swarm集群"></a>建立swarm集群</h3><p><img src="/2017/12/19/docker-swarm/media/b13d7b05fb2a91b9d98d6b4ddf904f18.png"></p><h4 id="将node1作为swarm集群的创建者"><a href="#将node1作为swarm集群的创建者" class="headerlink" title="将node1作为swarm集群的创建者"></a>将node1作为swarm集群的创建者</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker swarm init --advertise-addr $(docker-machine ip node1)</span><br></pre></td></tr></table></figure><p><img src="/2017/12/19/docker-swarm/media/00798f4616ec2ba44044b9748766f891.png"></p><p>退出node1</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">eval $(docker-machine env -u)</span><br></pre></td></tr></table></figure><h4 id="分别将node2和node3，将作为节点服务器加入到集群中"><a href="#分别将node2和node3，将作为节点服务器加入到集群中" class="headerlink" title="分别将node2和node3，将作为节点服务器加入到集群中"></a>分别将node2和node3，将作为节点服务器加入到集群中</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i in 2 3; do docker-machine ssh node$i docker swarm join --token SWMTKN-1-2ov80t4wgnrmh4t1d48a0n3uxrrmaojkv9raf1733irvfiw4w2-d0vrx62myyg5wetxkd7r3jl7a $(docker-machine ip node1):2377; done</span><br></pre></td></tr></table></figure><p><img src="/2017/12/19/docker-swarm/media/bf82c07ec4f9cc2ee03b1608a0dd05c0.png"></p><h4 id="查看集群的节点信息"><a href="#查看集群的节点信息" class="headerlink" title="查看集群的节点信息"></a>查看集群的节点信息</h4><p><img src="/2017/12/19/docker-swarm/media/6619e10f92224cbd772b272113d83dbf.png"></p><h4 id="批量修改加速器（这样无法实现），必须要ssh进去后操作后才可以实现"><a href="#批量修改加速器（这样无法实现），必须要ssh进去后操作后才可以实现" class="headerlink" title="批量修改加速器（这样无法实现），必须要ssh进去后操作后才可以实现"></a>批量修改加速器（这样无法实现），必须要ssh进去后操作后才可以实现</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i in 1 2 3; do docker-machine ssh node$i curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://ac1e54d0.m.daocloud.io &amp;&amp; sudo systemctl restart docker; done</span><br></pre></td></tr></table></figure><h4 id="建立一个nginx测试容器"><a href="#建立一个nginx测试容器" class="headerlink" title="建立一个nginx测试容器"></a>建立一个nginx测试容器</h4><p>这里是三台机器组成集群，副本因子为3，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-machine ssh node1 docker service create -p 80:80 --name swarm-nginx --replicas 3 fsoppelsa/swarm-nginx</span><br></pre></td></tr></table></figure><p><img src="/2017/12/19/docker-swarm/media/b12a36820f6209c79b87f728508f6f68.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-machine ssh node1 docker service ps swarm-nginx</span><br></pre></td></tr></table></figure><p><img src="/2017/12/19/docker-swarm/media/d6602ef5a5cf3b1abde1ac435fada734.png"></p><p>从上面可以看出node1作为集群建立者，管理者，也是会作为work节点建立容器的<br>这里我们可以猜想是不是作为节点管理者，在副本因子定义为2的时候，会不会优先选择非管理者建立节点</p><p>测试：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-machine ssh node1 docker service create -p 8080:80 --name swarm-nginx8080 --replicas 2 fsoppelsa/swarm-nginx</span><br></pre></td></tr></table></figure><p><img src="/2017/12/19/docker-swarm/media/67d0f5b714ee1908a4d9731871485d59.png"></p><p>结果证明：没有定义策略的情况下，还是建立到了管理者node1上面</p><p>使用调度策略（约束）来建立一个不在管理者上建立容器的service</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-machine ssh node1 docker service create --constraint &#x27;node.role!=manager&#x27; -p 88:80 --name swarm-nginx88 --replicas 3 fsoppelsa/swarm-nginx</span><br></pre></td></tr></table></figure><p><img src="/2017/12/19/docker-swarm/media/bfa6e545402b3fe94ff9db0fba3d07cc.png"></p><p><strong>Swarm集群已经建立，清空所有的service。</strong></p><h2 id="搭建jenkins服务器（192-168-100-70）"><a href="#搭建jenkins服务器（192-168-100-70）" class="headerlink" title="搭建jenkins服务器（192.168.100.70）"></a>搭建jenkins服务器（192.168.100.70）</h2><h3 id="搭建java和mvn环境"><a href="#搭建java和mvn环境" class="headerlink" title="搭建java和mvn环境"></a>搭建java和mvn环境</h3><p>解压</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf jdk-8u111-linux-x64.tar.gz</span><br><span class="line">sudo mv jdk1.8.0_111/ /usr/local/java</span><br><span class="line">tar zxf apache-maven-3.3.9-bin.tar.gz</span><br><span class="line">sudo mv apache-maven-3.3.9 /usr/local/maven</span><br></pre></td></tr></table></figure><p>设置环境变量<br>/etc/profile</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/java</span><br><span class="line">export PATH=\$PATH:/usr/local/java/bin</span><br><span class="line">export MAVEN_HOME=/usr/local/maven</span><br><span class="line">export PATH=\$PATH:/usr/local/maven/bin</span><br></pre></td></tr></table></figure><p>生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br><span class="line">mvn -v</span><br></pre></td></tr></table></figure><p><img src="/2017/12/19/docker-swarm/media/eece6544cac4ec77f7b4307c6eb07b8d.png"></p><h3 id="安装jenkins服务"><a href="#安装jenkins服务" class="headerlink" title="安装jenkins服务"></a>安装jenkins服务</h3><p>建立一个jenkins目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir Jenkins</span><br><span class="line">mv jenkins.war ./jenkins</span><br></pre></td></tr></table></figure><p>安装分屏命令screen</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum -y install screen</span><br></pre></td></tr></table></figure><p>建立一个名称为jenkins的screen分屏</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -S jenkins</span><br></pre></td></tr></table></figure><p>Ctrl+a+d返回终端</p><p>进入jenkins分屏</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">screen -r jenkins</span><br><span class="line">sudo /usr/local/java/bin/java -jar ./jenkins.war --httpPort=404</span><br></pre></td></tr></table></figure><p><img src="/2017/12/19/docker-swarm/media/def860b661431e21a4c049be661d9925.png"><br>Ctrl+a+d返回终端<br><img src="/2017/12/19/docker-swarm/media/e012caad6f4e986e620b08290fc16338.png"></p><p>Web浏览器打开登录<a href="http://192.168.100.70:404/">http://192.168.100.70:404</a><br><img src="/2017/12/19/docker-swarm/media/ef842e4773eb0c9bcd20eb61180cf7c0.png"></p><p>切换为终端查看，并复制到web页面去<br><img src="/2017/12/19/docker-swarm/media/94d531c8b16d6556d21e4f407a6fb1ec.png"></p><p>选择安装<br><img src="/2017/12/19/docker-swarm/media/b702399de69ecbb62a8910bdb60296d2.png"></p><p><strong>安装好了后建立一个jenkins账户，密码为jenkins的管理员账户</strong><br><img src="/2017/12/19/docker-swarm/media/c13a15c3ba6c382f5879ab15f4be37fa.png"></p><h3 id="修改jenkins的配置"><a href="#修改jenkins的配置" class="headerlink" title="修改jenkins的配置"></a>修改jenkins的配置</h3><p><strong>Maven的配置</strong><br><img src="/2017/12/19/docker-swarm/media/007b745635f9b3b0f33b0a77cb00efcb.png"></p><p><strong>使用本地的maven</strong><br><img src="/2017/12/19/docker-swarm/media/e5e28663bc2621463cf50da5109805b5.png"></p><p><strong>Java和git使用本地的</strong><br><img src="/2017/12/19/docker-swarm/media/7815874989f7be3c6ccfda91da33da05.png"></p><h3 id="新建一个jenkins-swarm项目测试"><a href="#新建一个jenkins-swarm项目测试" class="headerlink" title="新建一个jenkins-swarm项目测试"></a>新建一个jenkins-swarm项目测试</h3><p><img src="/2017/12/19/docker-swarm/media/001903bfe51edd60e98869a90c24d5ea.png"></p><p><strong>自定义工作目录</strong><br><img src="/2017/12/19/docker-swarm/media/a00a2e2f41d7ad4f6a40e6ce58ec3bf6.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">runuser -l admin -c &#x27;docker-machine ssh node1 docker service create -p 80:80 --name swarm-nginx --replicas 3 fsoppelsa/swarm-nginx&#x27;</span><br></pre></td></tr></table></figure><p><img src="/2017/12/19/docker-swarm/media/bf469da815cb0f35252c77692eb736f6.png"></p><p><strong>这里因为docker-machine管理是使用的admin用户，而运行jenkins的root用户。所以root用户也无法使用docker-machine管理，这里我们使用runuser这个命令来执行，（是不是起到了安全的作用，我们可以建立一个用户来专门作为管理docker-machine的，）</strong></p><p><strong>保存后，点击立即构建</strong><br><img src="/2017/12/19/docker-swarm/media/d88cda87517d1303f4e8d1977e456ea7.png"></p><p><strong>登录docker-machine的机器验证</strong><br><img src="/2017/12/19/docker-swarm/media/52408e5603f2cd706dd99e4886662142.png"></p><p><strong>访问验证</strong><br><img src="/2017/12/19/docker-swarm/media/8693b63e5cedd93086a0cdc91b72ea8a.png"><br><img src="/2017/12/19/docker-swarm/media/345ad087a16c1e6614eabac8e37fac51.png"><br><img src="/2017/12/19/docker-swarm/media/6c33911618787cca90f6d4ae6be1901a.png"></p><p><strong>这里有一个猜想：</strong><br><strong>这里的docker-machine因为是使用的admin用户，所以docker-machine管理的主机的配置文件都放在/home/admin/.docker下，权限为700，这里我将权限设置为770后，并将root组设置为宿组，还是无法使用docker-machine，最后提示权限太高，导致无法运行。修改为700后即可。</strong><br><img src="/2017/12/19/docker-swarm/media/321f0f30dc407288b219a68613e838e9.png"></p><h3 id="构建一个maven项目"><a href="#构建一个maven项目" class="headerlink" title="构建一个maven项目"></a>构建一个maven项目</h3><p><strong>安装jenkins插件，</strong><br><img src="/2017/12/19/docker-swarm/media/2f323f77530f4a84fa6c8535f0f9971c.png"></p><p><strong>建立一个名为test-oss的maven项目</strong><br><img src="/2017/12/19/docker-swarm/media/77fd0ea2187e8d461459f080a04bb455.png"></p><p><strong>Git代码来源自己去配置配置好。存储位置：/tmp/jenkins-buildenv/${JOB_NAME}/workspace</strong><br><strong>build配置，指定pom.xml位置，第一次操作使用mvn install ，后续都使用mvn clean package -Dmaven.test.skip=true</strong><br><img src="/2017/12/19/docker-swarm/media/1a78b18457d64537808f7f88b5d0edfd.png"></p><p><strong>Build操作使用shell脚本完成</strong></p><p><strong>因为运行jenkins的账户是root，所以使用runuser命令来操作，首先将jenkins的build后的代码复制到docker swarm的manager上，然后再由manager创建集群时候指定挂载</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">BUILD_ID=DONTKILLME</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">项目定义变量</span></span><br><span class="line">IMAGE=huisebug/jetty:1</span><br><span class="line">WARPAG=17find-op.war</span><br><span class="line">PORT=8100</span><br><span class="line">PROJECTPATH=17find/17find-op</span><br><span class="line">WEBAPPS=/home/admin/$&#123;JOB_NAME&#125;/</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">启动</span></span><br><span class="line"><span class="meta">#</span><span class="bash">runuser -l admin -c <span class="string">&quot;mkdir <span class="variable">$WEBAPPS</span>&quot;</span></span></span><br><span class="line">runuser -l admin -c &quot;cd $WEBAPPS &amp;&amp; rm -rf * &amp;&amp; /usr/local/java/bin/jar xf $WORKSPACE/$PROJECTPATH/target/$WARPAG&quot;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">runuser -l admin -c <span class="string">&quot;docker-machine ssh node1 mkdir /root/swarmsource/&quot;</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">runuser -l admin -c <span class="string">&quot;docker-machine ssh node1 rm -rf /root/swarmsource/<span class="variable">$&#123;JOB_NAME&#125;</span>/&quot;</span></span></span><br><span class="line">runuser -l admin -c &quot;docker-machine scp -r $WEBAPPS node1:/root/swarmsource/ &quot;</span><br><span class="line">runuser -l admin -c &quot; \</span><br><span class="line">docker-machine ssh node1 docker service create \</span><br><span class="line">--mount &#x27;type=bind,src=/root/swarmsource/$&#123;JOB_NAME&#125;/,dst=/usr/local/jetty/webapps/ROOT/&#x27; \</span><br><span class="line">--name $&#123;JOB_NAME&#125; -p $PORT:8080 \</span><br><span class="line">--constraint &#x27;node.role!=manager&#x27; \</span><br><span class="line">--replicas 2 $IMAGE \</span><br><span class="line">&quot;</span><br></pre></td></tr></table></figure><p><strong>查看效果，稍微等下</strong><br><img src="/2017/12/19/docker-swarm/media/2dc47afee3026be1c4bce685952c2041.png"></p><h1 id="做到这里的时候总结出一些更好的想法："><a href="#做到这里的时候总结出一些更好的想法：" class="headerlink" title="做到这里的时候总结出一些更好的想法："></a>做到这里的时候总结出一些更好的想法：</h1><ol><li><strong>因为jenkins如果是使用的war的形式运行的，那么他会需要创建文件夹和配置文件，这样就需要管理权限，这样将会使用到root的权限，这样是有点不安全的，所以如果在真实环境中，推荐jenkins还是使用docker容器这样的方式来运行。</strong></li><li><strong>还有一种解决方式就是node1作为集群的manager，我们可以使用约束来不让他参与dockerservice的创建work节点容器，只作为jenkins这类的代码拉取工作。就可以省去我们这里的操作，需要在docker-machine控制台将文件夹复制到node1里面去，再运行dockerservice create的创建。</strong></li><li><strong>docker swarm mode是使用service的方式建立集群并负载均衡。这样可以将项目整合在一起加强做集群</strong></li><li><strong>这里我的操作都没用到docker-machine env的操作，因为为了使用shell，只得使用ssh 节点 命令的方式来进行操作。</strong></li><li><strong>Nginx服务器和nginx代理其实可以整合，但是我们为了优化配置，代理分开</strong></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;这是一个swarm集群，从零开始搭建服务器环境，Docker-machine+Docker swarm+Nginx+Nginx代理&lt;/p&gt;</summary>
    
    
    
    <category term="Docker" scheme="https://huisebug.github.io/categories/Docker/"/>
    
    
    <category term="docker" scheme="https://huisebug.github.io/tags/docker/"/>
    
    <category term="jenkins" scheme="https://huisebug.github.io/tags/jenkins/"/>
    
    <category term="expect脚本" scheme="https://huisebug.github.io/tags/expect%E8%84%9A%E6%9C%AC/"/>
    
    <category term="docker machine" scheme="https://huisebug.github.io/tags/docker-machine/"/>
    
    <category term="docker swarm" scheme="https://huisebug.github.io/tags/docker-swarm/"/>
    
  </entry>
  
  <entry>
    <title>Docker基础概念</title>
    <link href="https://huisebug.github.io/2017/12/19/docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"/>
    <id>https://huisebug.github.io/2017/12/19/docker%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/</id>
    <published>2017-12-19T06:04:01.000Z</published>
    <updated>2021-07-09T02:04:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>工作中可能常用的docker基础命令，可能有些命令已经淘汰，此处介绍常用的一些docker使用，主要是基础概念</p><span id="more"></span><h1 id="镜像"><a href="#镜像" class="headerlink" title="镜像"></a>镜像</h1><h2 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h2><p>1.CMD：命令/参数；</p><p>2.ENTRYPOINT:命令；docker –entrypoint=命令 镜像名 /进行覆盖操作</p><p>3.WORKDIR：工作目录；docker -w=绝对路径 镜像名 、进行覆盖</p><p>4.ENV：环境变量/变量；永久的保存到建立的容器中，docker -e 变量=值 镜像名 可进行容器增加全局变量</p><p>5.USER: 指定用户去运行容器，不指定就是root，用户名：组名；格式</p><p>6.VOLUME:卷</p><p>7.ADD：复制和解压，构建缓存失效，源文件必须在Dockerfile同一级目录下</p><p>8.COPY：复制文件</p><p>9.ONBUILD:触发器</p><p>Dockerfile中没法使用source来添加环境变量，必须使用ENV</p><h2 id="镜像制作命令"><a href="#镜像制作命令" class="headerlink" title="镜像制作命令"></a>镜像制作命令</h2><p>镜像名命名格式为：用户/服务：标签,如果你想上传到docker hub或者自己的私库，就需要将用户名设置正确的，如果不正确也可以使用docker tag进行复制镜像并重命名，也可将运行的容器docker commit为新镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t &quot;镜像名&quot; . </span><br></pre></td></tr></table></figure><p>–no-cache是所有建立镜像的过程都重新运行，不使用之前的缓存，最后面的.是构建上下文的意思，可自行百度理解，在这里你可以逻辑上理解为当前目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build --no-cache -t &quot;镜像名&quot; . </span><br></pre></td></tr></table></figure><h2 id="镜像管理命令"><a href="#镜像管理命令" class="headerlink" title="镜像管理命令"></a>镜像管理命令</h2><p>镜像列表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker images</span><br></pre></td></tr></table></figure><p>列出错误构建Dockerfile的时候生成的镜像，清除掉节约空间，其实就是错误的缓存，正确构建Dockerfile后，缓存是会整合到镜像中去的。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker images -f &quot;dangling=true&quot;</span><br></pre></td></tr></table></figure><p>查看镜像的ID</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker images --format &quot;&#123;&#123;.Reposittory&#125;&#125;&quot;</span><br></pre></td></tr></table></figure><p>将一个镜像导出为文件。 docker save -o 文件名 镜像名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker save 镜像名</span><br></pre></td></tr></table></figure><p>将docker save导出的文件导入为一个镜像，会保存该镜像的的所有历史记录。比docker export命令导出的文件大，很好理解，因为会保存镜像的所有历史记录。 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker load -i 文件名</span><br></pre></td></tr></table></figure><p>将一个容器导出为文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker export 容器ID</span><br></pre></td></tr></table></figure><p>将容器中export导出的文件导入成为一个新的镜像，但是相比docker save命令，容器文件会丢失所有元数据和历史记录，仅保存容器当时的状态，相当于虚拟机快照</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker import 文件名</span><br></pre></td></tr></table></figure><h1 id="容器操作"><a href="#容器操作" class="headerlink" title="容器操作"></a>容器操作</h1><p>这个很少用了，一般是之前用于docker -it 镜像名 这种方式建立的时候，这个的缺点是exit会关闭容器，除非你建立容器的时候加了-d</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker attach 容器名</span><br></pre></td></tr></table></figure><p>默认从docker hub拉取，拉取私库需要docker login dockerhub用户名（不是邮箱账户，就是用户名）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull huisebug/jetty:1</span><br></pre></td></tr></table></figure><p>dockerhub镜像搜索命令，最后面跟上你要搜索的镜像，进行过滤匹配</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker search nginx</span><br></pre></td></tr></table></figure><p>容器管理，（启动，重启，停止，强制停止））</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker start/restart/stop/kill -s 容器名</span><br></pre></td></tr></table></figure><p>容器日志查看</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker logs 容器名 ：</span><br></pre></td></tr></table></figure><p>相当于tail -f 的功能</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker logs -f 容器名 ：</span><br></pre></td></tr></table></figure><p>直接进入容器，这是需要安装一个docker-enter的脚本，脚本是调用的docker早期的nsenter这个进入容器的命令，现在已经被docker exec取代，想使用docker-enter这个命令，需要在建立容器时写成docker run -dit 镜像名, -dit是一个组合，-d后台运行，-i输入，-t终端。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-enter 容器名</span><br></pre></td></tr></table></figure><p>目录挂载，ro：只读 rw：可读写</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -v 宿主机路径:容器路径:ro -v 宿主机路径:容器路径:rw 镜像名</span><br></pre></td></tr></table></figure><p>单独的dns服务，一般容器会复制宿主机的/etc/resolv.conf文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --dns=61.139.2.69 镜像名</span><br></pre></td></tr></table></figure><p>特权模式：可以修改宿主机的模式，相当于获取了docker组的权限，docker组的权限和root组基本相同</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --privileged 镜像名</span><br></pre></td></tr></table></figure><p>将建立后的容器ID输出到一个文件中，可用于后面的启动全部容器for i in $(cat /root/cidfile); do docker stop $i ;done; cidfile是容器ID后加上一串其他的ID</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --cidfile=文件路径 镜像名</span><br></pre></td></tr></table></figure><p>用于调用之前的容器建立时候挂载的卷和在Dockerfile中声明的VOLUME的调用，常见的场景就是日志的查看，或者前一个容器是一个编译器，将编译好的文件放在挂载的目录下，然后另一个容器运行的服务需要编译好的包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --volumes-from 容器名 镜像名</span><br></pre></td></tr></table></figure><p>这里为什么不是镜像名呢，这个命令常用与构建镜像的时候Dockerfile出错了，docker build 每一层都是有镜像ID和容器ID的。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it 镜像ID bash</span><br></pre></td></tr></table></figure><p>这个常用于日志查看，这里镜像名一般都是采用Ubuntu或者centos这种只有操作系统的镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm --volumes-from 容器名 -i 镜像名 命令</span><br></pre></td></tr></table></figure><p>指定主机名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -h 镜像名</span><br></pre></td></tr></table></figure><p>这是让容器跟随docker服务重启时候自动重启</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --restart=always -d 镜像名</span><br></pre></td></tr></table></figure><p>列出容器中的文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker diff 容器名</span><br></pre></td></tr></table></figure><p>随机使用端口号49000~49900映射镜像中在编写Dockerfile中定义的EXPOSE端口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -P</span><br></pre></td></tr></table></figure><p>添加hosts本地解析</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --add-host 镜像名</span><br></pre></td></tr></table></figure><p>容器详细的参数，里面会列出当前容器的所有信息，挂载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker inspect 容器名</span><br></pre></td></tr></table></figure><p>清除未使用的对象</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker container容器/image镜像/volume存储卷/network防火墙规则/system所有对象 prune</span><br></pre></td></tr></table></figure><p>在要输出信息的命令后面加上–no-trunc输出不截断的完整信息</p><p>正在运行的容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps</span><br></pre></td></tr></table></figure><p>所有的容器，包含停止的容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps -a</span><br></pre></td></tr></table></figure><p>所有容器的ID</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps -a -q</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker rm 容器名 ：删除容器</span><br><span class="line">docker rmi 镜像名：删除镜像</span><br><span class="line">docker stop 容器名 ：停止容器</span><br><span class="line">docker stats 容器名：监控容器的CPU、内存、I/O</span><br></pre></td></tr></table></figure><p>向容器中传送bash命令，以实现进入容器的效果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it 容器名 /bin/bash</span><br></pre></td></tr></table></figure><p>指定容器用户</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it -u root 容器名 bash -c</span><br></pre></td></tr></table></figure><p>指定容器网络</p><p>创建容器的网卡，默认的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=&quot;bridge&quot; 镜像名</span><br></pre></td></tr></table></figure><p>无网络，可以使用link通信</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=&quot;none&quot; 镜像名</span><br></pre></td></tr></table></figure><p>可以看到host上所有的设备，不安全</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=&quot;host&quot; 镜像名</span><br></pre></td></tr></table></figure><p>建立与一个容器相同的网络</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run --net=&quot;容器名或者容器ID&quot; 镜像名</span><br><span class="line">docker network create bridge模式网络名</span><br><span class="line">docker run --network bridge模式网络名 容器1</span><br><span class="line">docker run --network bridge模式网络名 容器2</span><br></pre></td></tr></table></figure><p>容器1和容器2即可实现之前的–link功能</p><p>容器中使用的是美国的时区，所以总是会相差8个小时，将宿主机的时间挂载过去就好了，实在还是不行就手动修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -v /etc/localtime:/etc/localtime 镜像名</span><br></pre></td></tr></table></figure><p>docker同步时间</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">上传本地的Shanghai文件到容器</span><br><span class="line">docker cp /usr/share/zoneinfo/Asia/Shanghai redis:/etc/</span><br><span class="line">进入容器</span><br><span class="line">mkdir -p /usr/share/zoneinfo/Etc/</span><br><span class="line">cp -rf /etc/Shanghai /usr/share/zoneinfo/Etc/UTC</span><br><span class="line">cp -rf /etc/Shanghai /etc/localtime</span><br><span class="line">date</span><br></pre></td></tr></table></figure><p>容器和宿主机之间拷贝文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker cp 本地文件路径 容器名：容器中路径</span><br><span class="line"></span><br><span class="line">docker cp 容器名：容器文件路径 本地路径</span><br></pre></td></tr></table></figure><h1 id="docker早期知识"><a href="#docker早期知识" class="headerlink" title="docker早期知识"></a>docker早期知识</h1><p>docker-machine config 节点名 可查看证书连接，这是使用驱动为virtualbox的查看</p><p>docker swarm standalone 独立</p><p>docker stack deploy，这个需要在dammon.json文件定义开启部署功能。才可以部署，将docker-compose.yml文件打包为dab文件，然后进行swarm部署。使用service进行查看</p><p>fig早期的服务运用栈，现在使用docker-compose</p><p>consul进行服务发现，现在的docker swarm是默认建立了集群模式，使用的是etcd服务发现</p><p>centos7的docker守护进程文件位/usr/lib/systemd/system/docker.service进行私库指定，集群标签，加密通信等操作。</p><p>docker ENTRYPOINT 的脚本文件记得声明#!/bin/bash</p><p>dig一个DNS查询命令，需要安装</p><p>docker容器中的172.17.0.0网段IP地址会变化，不是固定的。这时候需要使用link机制，这个只有在互相配置的时候，如果是后续添加就无法实现，最终还是用自带的dns解析比较好。</p><p>FROM alpine 最简洁的linux系统docker镜像，可使用apk add 软件名可进行安装软件</p><h1 id="镜像模板"><a href="#镜像模板" class="headerlink" title="镜像模板"></a>镜像模板</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine</span><br><span class="line">MAINTAINER wyf Turnbull &quot;huisebug@outlook.com&quot;</span><br><span class="line">ENV REFRESHED_AT 2017-09-14</span><br><span class="line">RUN alpineverion=$(cat /etc/issue | grep &quot;Alpine&quot; | awk &#x27;&#123;print $NF&#125;&#x27;) \</span><br><span class="line">&amp;&amp; echo &quot;https://mirrors.aliyun.com/alpine/v$&#123;alpineverion&#125;/main/&quot; &gt; /etc/apk/repositories &amp;&amp; echo &quot;https://mirrors.aliyun.com/alpine/v$&#123;alpineverion&#125;/community/&quot; &gt;&gt; /etc/apk/repositories &amp;&amp; apk update \</span><br><span class="line"><span class="meta">#</span><span class="bash">替换时区</span></span><br><span class="line">&amp;&amp; apk add tzdata &amp;&amp; ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">安装常用工具busybox-extras就是telnet</span></span><br><span class="line">&amp;&amp; apk add vim net-tools less procps wget curl busybox-extras</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>轻量级容器镜像</p><p>debian:slim：自带包管理系统，兼容性最好</p><p>#替换dibian源为阿里云源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RUN sed -i s@/deb.debian.org/@/mirrors.aliyun.com/@g /etc/apt/sources.list \</span><br><span class="line">&amp;&amp; sed -i s@/security.debian.org/@/mirrors.aliyun.com/@g /etc/apt/sources.list \</span><br></pre></td></tr></table></figure><p>#修改时区</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&amp;&amp; apt-get install -y tzdata &amp;&amp; ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime </span><br></pre></td></tr></table></figure><p>#替换Ubuntu源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i s@/archive.ubuntu.com/@/mirrors.aliyun.com/@g /etc/apt/sources.list</span><br></pre></td></tr></table></figure><h1 id="调试命令"><a href="#调试命令" class="headerlink" title="调试命令"></a>调试命令</h1><h2 id="测试cpu过高"><a href="#测试cpu过高" class="headerlink" title="测试cpu过高"></a>测试cpu过高</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it busybox sh -c &quot;while true; do :; done&quot;</span><br></pre></td></tr></table></figure><h2 id="debian系列系统安装netstat命令"><a href="#debian系列系统安装netstat命令" class="headerlink" title="debian系列系统安装netstat命令"></a>debian系列系统安装netstat命令</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sed -i s\@/archive.ubuntu.com/\@/mirrors.aliyun.com/\@g /etc/apt/sources.list</span><br><span class="line">apt-get -y update</span><br><span class="line">apt-get -y install net-tools</span><br></pre></td></tr></table></figure><h2 id="容器中常用工具"><a href="#容器中常用工具" class="headerlink" title="容器中常用工具"></a>容器中常用工具</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim net-tools less procps telnet wget curl</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;工作中可能常用的docker基础命令，可能有些命令已经淘汰，此处介绍常用的一些docker使用，主要是基础概念&lt;/p&gt;</summary>
    
    
    
    <category term="Docker" scheme="https://huisebug.github.io/categories/Docker/"/>
    
    
    <category term="docker" scheme="https://huisebug.github.io/tags/docker/"/>
    
  </entry>
  
</feed>
