<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>huisebug</title>
  
  
  <link href="https://huisebug.github.io/atom.xml" rel="self"/>
  
  <link href="https://huisebug.github.io/"/>
  <updated>2022-12-09T03:04:02.000Z</updated>
  <id>https://huisebug.github.io/</id>
  
  <author>
    <name>huisebug</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>logfile-operator:服务日志系统收集多方案</title>
    <link href="https://huisebug.github.io/2024/06/28/logfile-operator/"/>
    <id>https://huisebug.github.io/2024/06/28/logfile-operator/</id>
    <published>2024-06-28T03:04:01.000Z</published>
    <updated>2022-12-09T03:04:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>为kubernetes集群中部署服务提供多日志收集服务，快速接入多种部署方案的日志系统</p><span id="more"></span><h1 id="需求场景"><a href="#需求场景" class="headerlink" title="需求场景"></a>需求场景</h1><ul><li><p>   当pod中有多个容器时，需要对pod中的容器进行日志收集</p></li><li><p>   研发人员需要快速对服务进行日志收集后在kibana中集中查看</p></li></ul><h1 id="功能描述"><a href="#功能描述" class="headerlink" title="功能描述"></a>功能描述</h1><ul><li>  利用kubernetes pod注入对运行的服务进行注入容器，注入的容器和原容器在pod annotations配置的日志文件所在目录进行卷挂载，注解中配置的日志文件进行卷挂载，例如：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      annotations:</span><br><span class="line">        logfile.huisebug.org/log1: /var/log/nginx/*.log</span><br></pre></td></tr></table></figure></li><li>  提供六种日志收集部署方案</li></ul><ol><li><p> filebeat+elasticsearch+kibana</p></li><li><p> filebeat+elasticsearch-cluster+kibana</p></li><li><p> filebeat+logstash+elasticsearch+kibana</p></li><li><p> filebeat+logstash+elasticsearch-cluster+kibana</p></li><li><p> filebeat+kafka+logstash+elasticsearch+kibana</p></li><li><p> filebeat+kafka-cluster+logstash+elasticsearch-cluster+kibana</p></li></ol><p>版本信息：<br>filebeat：8.5.0<br>logstash：8.5.0<br>kafka：3.3<br>zookeeper：3.8<br>elasticsearch：8.5.0<br>kibana：8.5.0</p><p>注意：elasticsearch集群这里采用的https集群</p><ul><li>  git项目首页提供的num的6个yaml文件对应了六种方案</li></ul><h1 id="部署命令（按顺序执行）"><a href="#部署命令（按顺序执行）" class="headerlink" title="部署命令（按顺序执行）"></a>部署命令（按顺序执行）</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f cert-manager.yaml</span><br><span class="line">kubectl apply -f deploy.yaml</span><br><span class="line">kubectl apply -f num数字.yaml</span><br></pre></td></tr></table></figure><h1 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h1><p><a href="https://github.com/huisebug/logfile-operator.git">https://github.com/huisebug/logfile-operator.git</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;为kubernetes集群中部署服务提供多日志收集服务，快速接入多种部署方案的日志系统&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes-Dev" scheme="https://huisebug.github.io/categories/Kubernetes-Dev/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="elasticsearch" scheme="https://huisebug.github.io/tags/elasticsearch/"/>
    
    <category term="kubebuilder" scheme="https://huisebug.github.io/tags/kubebuilder/"/>
    
    <category term="kubernetes" scheme="https://huisebug.github.io/tags/kubernetes/"/>
    
    <category term="logstash" scheme="https://huisebug.github.io/tags/logstash/"/>
    
    <category term="filebeat" scheme="https://huisebug.github.io/tags/filebeat/"/>
    
    <category term="kafka-cluster" scheme="https://huisebug.github.io/tags/kafka-cluster/"/>
    
    <category term="kafka" scheme="https://huisebug.github.io/tags/kafka/"/>
    
    <category term="zookeeper-cluster" scheme="https://huisebug.github.io/tags/zookeeper-cluster/"/>
    
    <category term="elasticsearch-cluster" scheme="https://huisebug.github.io/tags/elasticsearch-cluster/"/>
    
  </entry>
  
  <entry>
    <title>binlog恢复数据</title>
    <link href="https://huisebug.github.io/2024/06/27/mysql-binlog%E6%81%A2%E5%A4%8D/"/>
    <id>https://huisebug.github.io/2024/06/27/mysql-binlog%E6%81%A2%E5%A4%8D/</id>
    <published>2024-06-27T03:04:01.000Z</published>
    <updated>2024-06-28T02:10:51.536Z</updated>
    
    <content type="html"><![CDATA[<p>本文档介绍如何使用binlog进行数据恢复</p><span id="more"></span><h1 id="场景描述"><a href="#场景描述" class="headerlink" title="场景描述"></a>场景描述</h1><p>需要恢复的数据日期<br>开始时间为2024-05-28 01:33:02<br>截止时间为2024-06-06 09:40:00<br>先进入mysql的binlog目录所在，一般是/var/lib/mysql<br>对最新的binlog文件执行命令，比如最新的binlog文件为binlog.000050 查看最后10行，确保binlog内记录时间是对的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqlbinlog --no-defaults --base64-output=DECODE-ROWS --start-datetime=&quot;2024-05-28 01:33:02&quot; --stop-datetime=&quot;2024-06-06 09:40:00&quot;  -v ./binlog.000050 |  tail -10</span><br></pre></td></tr></table></figure><p>这条命令用于读取并解析 MySQL 二进制日志文件 (<code>binlog.000091</code>) 的内容，并且只显示日志文件中的最后 10 行。以下是各个参数的详细介绍：</p><ol><li><p><code>mysqlbinlog</code>: 这是用于处理 MySQL 二进制日志文件的工具。</p></li><li><p><code>--no-defaults</code>: 这个参数告诉 <code>mysqlbinlog</code> 工具不要读取默认的选项文件（例如 <code>/etc/my.cnf</code>）。</p></li><li><p><code>--base64-output=DECODE-ROWS</code>: 这个参数指示 <code>mysqlbinlog</code> 输出时将包含基于 BASE64 编码的行事件解码为人类可读的形式。<code>DECODE-ROWS</code> 是指将行格式事件解码为 SQL 语句。</p></li><li><p><code>--start-datetime=&quot;2024-05-28 01:33:02&quot;</code>: 指定读取二进制日志的开始时间，只处理这个时间点之后的事件。</p></li><li><p><code>--stop-datetime=&quot;2024-06-06 09:40:00&quot;</code>: 指定读取二进制日志的结束时间，只处理这个时间点之前的事件。</p></li><li><p><code>-v</code>: 这个参数告诉 <code>mysqlbinlog</code> 使用详细模式输出，显示更多的细节信息。</p></li><li><p><code>./binlog.000050</code>: 指定要读取的二进制日志文件的路径和文件名。</p></li><li><p><code>| tail -10</code>: 管道操作符 (<code>|</code>) 将 <code>mysqlbinlog</code> 的输出传递给 <code>tail</code> 命令，<code>tail -10</code> 表示只显示输出的最后 10 行。</p></li></ol><p>当在binlog.000050文件中并没有记录2024-06-06 09:40:00时间段时，就需要去查看binlog.000049、binlog.000048、binlog.000047 …文件了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqlbinlog --no-defaults --base64-output=DECODE-ROWS --start-datetime=&quot;2024-05-28 01:33:02&quot; --stop-datetime=&quot;2024-06-06 09:40:00&quot;  -v ./binlog.000049 |  tail -10</span><br></pre></td></tr></table></figure><p>假如最终查看到47文件才包含了截止时间</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqlbinlog --no-defaults --base64-output=DECODE-ROWS --start-datetime=&quot;2024-05-28 01:33:02&quot; --stop-datetime=&quot;2024-06-06 09:40:00&quot;  -v ./binlog.000047 |  tail -10</span><br></pre></td></tr></table></figure><p> 将其记录下来</p><p>确定开始时间在哪个binlog文件<br>一般binlog会定期清理,假如只剩下binlog.000030—binlog.000050的文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqlbinlog --no-defaults --base64-output=DECODE-ROWS --start-datetime=&quot;2024-05-28 01:33:02&quot; --stop-datetime=&quot;2024-06-06 09:40:00&quot;  -v ./binlog.000030 |  tail -10</span><br></pre></td></tr></table></figure><p>假如最终查看到35文件才包含了开始时间</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqlbinlog --no-defaults --base64-output=DECODE-ROWS --start-datetime=&quot;2024-05-28 01:33:02&quot; --stop-datetime=&quot;2024-06-06 09:40:00&quot;  -v ./binlog.000035 |  tail -10</span><br></pre></td></tr></table></figure><p> 将其记录下来</p><h1 id="将binlog转化为sql文件"><a href="#将binlog转化为sql文件" class="headerlink" title="将binlog转化为sql文件"></a>将binlog转化为sql文件</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir sqldir</span><br><span class="line">for i in &#123;35..47&#125;</span><br><span class="line">do</span><br><span class="line">mysqlbinlog --no-defaults --base64-output=DECODE-ROWS --start-datetime=&quot;2024-05-28 01:33:02&quot; --stop-datetime=&quot;2024-06-06 09:40:00&quot;  -v /var/lib/mysql/binlog.0000$i &gt; sqldir/out$i.sql</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h1 id="将sql文件导入数据库"><a href="#将sql文件导入数据库" class="headerlink" title="将sql文件导入数据库"></a>将sql文件导入数据库</h1><p>然后再按照开始时间为2024-05-28 01:33:02依次执行sql，一定要按照时间顺序执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for i in &#123;35..47&#125;</span><br><span class="line">do</span><br><span class="line"> mysql -uroot -p&#x27;密码&#x27; 要恢复的库名 &lt; sqldir/out$i.sql</span><br><span class="line">done</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文档介绍如何使用binlog进行数据恢复&lt;/p&gt;</summary>
    
    
    
    <category term="Mysql" scheme="https://huisebug.github.io/categories/Mysql/"/>
    
    
    <category term="mysql" scheme="https://huisebug.github.io/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>minio/cos/oss数据迁移到azure blob</title>
    <link href="https://huisebug.github.io/2024/06/06/minio-to-azure_blob/"/>
    <id>https://huisebug.github.io/2024/06/06/minio-to-azure_blob/</id>
    <published>2024-06-06T03:04:01.000Z</published>
    <updated>2024-06-28T02:04:01.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="将minio-cos-oss的数据迁移到azure-blob-storage"><a href="#将minio-cos-oss的数据迁移到azure-blob-storage" class="headerlink" title="将minio/cos/oss的数据迁移到azure blob storage"></a>将minio/cos/oss的数据迁移到azure blob storage</h1><span id="more"></span><h1 id="安装-MinIO-Client-mc-和-az-和-rclone"><a href="#安装-MinIO-Client-mc-和-az-和-rclone" class="headerlink" title="安装 MinIO Client (mc) 和 az 和 rclone"></a>安装 MinIO Client (mc) 和 az 和 rclone</h1><ul><li>请确保 <code>mc</code> <code>rclone</code> 和 <code>az</code> 已安装在你的系统中。如果没有安装，可以使用以下命令进行安装：</li><li>一般linux系统有一个mc命令，类似于windows的mmc</li><li>  这里的mc是minioclient的客户端</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 安装 MinIO Client</span></span><br><span class="line">wget https://dl.min.io/client/mc/release/linux-amd64/mc</span><br><span class="line">chmod +x mc</span><br><span class="line">sudo mv mc /usr/local/bin/</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装rclone</span></span><br><span class="line">sudo -v ; curl https://rclone.org/install.sh | sudo bash</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装 az  参照下面文档地址</span></span><br><span class="line">https://learn.microsoft.com/zh-cn/cli/azure/install-azure-cli</span><br></pre></td></tr></table></figure><h1 id="配置-MinIO-Client-mc"><a href="#配置-MinIO-Client-mc" class="headerlink" title="配置 MinIO Client mc"></a>配置 MinIO Client mc</h1><p>使用 mc 是基于用户的 ，不同用户需要都执行一次 mc alias set 命令为你的 MinIO 集群配置一个别名<br>   myminio是别名     </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mc alias set myminio http://minio-server:9000 YOUR_ACCESS_KEY YOUR_SECRET_KEY</span><br><span class="line">例如：</span><br><span class="line">mc alias set myminio http://192.168.137.100:9000 root root123</span><br></pre></td></tr></table></figure><h1 id="配置rclone登录minio-和-Azure-Blob-Storage"><a href="#配置rclone登录minio-和-Azure-Blob-Storage" class="headerlink" title="配置rclone登录minio 和  Azure Blob Storage"></a>配置rclone登录minio 和  Azure Blob Storage</h1><p>使用 rclone是基于用户的，所以在你准备执行的用户的用户目录下<br>例如azureuser用户<br>/home/azureuser/.config/rclone/rclone.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[az]</span><br><span class="line">type = azureblob</span><br><span class="line">account = admin</span><br><span class="line">key = blob key</span><br><span class="line">endpoint = https://admin.blob.core.windows.net/</span><br><span class="line">access_tier = Hot</span><br><span class="line">[minio]</span><br><span class="line">type = s3</span><br><span class="line">provider = Minio</span><br><span class="line">access_key_id = root</span><br><span class="line">secret_access_key = root123</span><br><span class="line">endpoint = http://192.168.137.100:9000</span><br><span class="line">[tencentcos]</span><br><span class="line">type = s3</span><br><span class="line">provider = TencentCOS</span><br><span class="line">env_auth = false</span><br><span class="line">access_key_id = key_id</span><br><span class="line">secret_access_key = access_key</span><br><span class="line">endpoint = cos.ap-tokyo.myqcloud.com     </span><br><span class="line">[aliyunoss]</span><br><span class="line">type = s3</span><br><span class="line">provider = Aliyun</span><br><span class="line">env_auth = false</span><br><span class="line">access_key_id = key_id</span><br><span class="line">secret_access_key = access_key</span><br><span class="line">endpoint = oss-cn-beijing.aliyuncs.com</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>上述中azure blob下的admin是存储账户名称,admin.blob.core.windows.net是定位到这个存储账户,admin根据不同账户名称不同<br>tencentcos下endpoint地址要注意，不同区域的地址不同，此处是东京<br>aliyunoss下endpoint地址要注意，不同区域的地址不同，此处是北京</p><h1 id="编写minio迁移脚本"><a href="#编写minio迁移脚本" class="headerlink" title="编写minio迁移脚本"></a>编写minio迁移脚本</h1><p>以下是一个完整的 Bash 脚本，用于将 MinIO 集群中的所有 bucket 迁移到 Azure Blob Storage：<br>migrate_minio_to_azure.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置 MinIO 和 Azure Blob Storage</span></span><br><span class="line">MINIO_ALIAS=&quot;myminio&quot;</span><br><span class="line">AZURE_STORAGE_ACCOUNT=&quot;admin&quot;</span><br><span class="line">AZURE_STORAGE_KEY=&quot;blob key&quot;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取所有 MinIO buckets</span></span><br><span class="line">buckets=$(mc ls myminio | awk &#x27;&#123;print $5&#125;&#x27; | tr -d &#x27;\/&#x27; )</span><br><span class="line"><span class="meta">#</span><span class="bash"> 遍历每个 bucket 并迁移到 Azure Blob Storage</span></span><br><span class="line">for bucket in $buckets</span><br><span class="line">do</span><br><span class="line">    echo &quot;正在迁移 bucket: $bucket&quot;</span><br><span class="line">    # 创建 Azure Blob Storage 容器</span><br><span class="line">    az storage container create --name $bucket --account-name $AZURE_STORAGE_ACCOUNT --account-key $AZURE_STORAGE_KEY</span><br><span class="line">    #使用rclone迁移数据,第一次使用copy，后续使用sync</span><br><span class="line">    rclone copy --log-level INFO minio:$bucket az:$bucket</span><br><span class="line">    echo &quot;迁移完成: $bucket&quot;</span><br><span class="line">done</span><br><span class="line">echo &quot;所有 bucket 迁移完成！&quot;</span><br></pre></td></tr></table></figure><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>确保 <code>MINIO_ALIAS</code>、<code>AZURE_STORAGE_ACCOUNT</code> 和 <code>AZURE_STORAGE_KEY</code> 替换为你的实际值。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x migrate_minio_to_azure.sh</span><br><span class="line">./migrate_minio_to_azure.sh</span><br></pre></td></tr></table></figure><p>这样，你就可以完成 MinIO 数据到 Azure Blob Storage 的迁移了。</p><h1 id="编写cos迁移脚本"><a href="#编写cos迁移脚本" class="headerlink" title="编写cos迁移脚本"></a>编写cos迁移脚本</h1><p>migrate_cos_to_azure.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">AZURE_STORAGE_ACCOUNT=&quot;admin&quot;</span><br><span class="line">AZURE_STORAGE_KEY=&quot;blob key&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取所有 MinIO buckets</span></span><br><span class="line">buckets=&#123;bucket1name,bucket2name,bucket3name,bucket4name&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 遍历每个 bucket 并迁移到 Azure Blob Storage</span></span><br><span class="line">for bucket in $buckets</span><br><span class="line">do</span><br><span class="line">    echo &quot;正在迁移 bucket: $bucket&quot;</span><br><span class="line">    # 创建 Azure Blob Storage 容器</span><br><span class="line">    az storage container create --name $bucket --account-name $AZURE_STORAGE_ACCOUNT --account-key $AZURE_STORAGE_KEY</span><br><span class="line">    #使用rclone迁移数据</span><br><span class="line">    rclone copy --log-level INFO tencentcos:$bucket az:$bucket</span><br><span class="line">    echo &quot;迁移完成: $bucket&quot;</span><br><span class="line">done</span><br><span class="line">echo &quot;所有 bucket 迁移完成！&quot;</span><br></pre></td></tr></table></figure><h3 id="注意事项-1"><a href="#注意事项-1" class="headerlink" title="注意事项"></a>注意事项</h3><p>确保 <code>AZURE_STORAGE_ACCOUNT</code> 和 <code>AZURE_STORAGE_KEY</code> 替换为你的实际值。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x migrate_cos_to_azure.sh</span><br><span class="line">./migrate_cos_to_azure.sh</span><br></pre></td></tr></table></figure><p>这样，你就可以完成 腾讯cos 数据到 Azure Blob Storage 的迁移了。</p><h1 id="编写oss迁移脚本"><a href="#编写oss迁移脚本" class="headerlink" title="编写oss迁移脚本"></a>编写oss迁移脚本</h1><p>migrate_oss_to_azure.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">AZURE_STORAGE_ACCOUNT=&quot;admin&quot;</span><br><span class="line">AZURE_STORAGE_KEY=&quot;blob key&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取所有 MinIO buckets</span></span><br><span class="line">buckets=&#123;bucket1name,bucket2name,bucket3name,bucket4name&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 遍历每个 bucket 并迁移到 Azure Blob Storage</span></span><br><span class="line">for bucket in $buckets</span><br><span class="line">do</span><br><span class="line">    echo &quot;正在迁移 bucket: $bucket&quot;</span><br><span class="line">    # 创建 Azure Blob Storage 容器</span><br><span class="line">    az storage container create --name $bucket --account-name $AZURE_STORAGE_ACCOUNT --account-key $AZURE_STORAGE_KEY</span><br><span class="line">    #使用rclone迁移数据</span><br><span class="line">    rclone copy --log-level INFO aliyunoss:$bucket az:$bucket</span><br><span class="line">    echo &quot;迁移完成: $bucket&quot;</span><br><span class="line">done</span><br><span class="line">echo &quot;所有 bucket 迁移完成！&quot;</span><br></pre></td></tr></table></figure><h3 id="注意事项-2"><a href="#注意事项-2" class="headerlink" title="注意事项"></a>注意事项</h3><p>确保 <code>AZURE_STORAGE_ACCOUNT</code> 和 <code>AZURE_STORAGE_KEY</code> 替换为你的实际值。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x migrate_oss_to_azure.sh</span><br><span class="line">./migrate_cos_to_azure.sh</span><br></pre></td></tr></table></figure><p>这样，你就可以完成 阿里oss 数据到 Azure Blob Storage 的迁移了。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;将minio-cos-oss的数据迁移到azure-blob-storage&quot;&gt;&lt;a href=&quot;#将minio-cos-oss的数据迁移到azure-blob-storage&quot; class=&quot;headerlink&quot; title=&quot;将minio/cos/oss的数据迁移到azure blob storage&quot;&gt;&lt;/a&gt;将minio/cos/oss的数据迁移到azure blob storage&lt;/h1&gt;</summary>
    
    
    
    <category term="小知识" scheme="https://huisebug.github.io/categories/%E5%B0%8F%E7%9F%A5%E8%AF%86/"/>
    
    
    <category term="minio" scheme="https://huisebug.github.io/tags/minio/"/>
    
    <category term="azure" scheme="https://huisebug.github.io/tags/azure/"/>
    
    <category term="cos" scheme="https://huisebug.github.io/tags/cos/"/>
    
    <category term="oss" scheme="https://huisebug.github.io/tags/oss/"/>
    
  </entry>
  
  <entry>
    <title>vector+clickhouse:新型kubernetes集群日志收集方案</title>
    <link href="https://huisebug.github.io/2024/05/21/vector+clickhouse/"/>
    <id>https://huisebug.github.io/2024/05/21/vector+clickhouse/</id>
    <published>2024-05-21T09:04:01.000Z</published>
    <updated>2024-05-21T09:04:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>Kubernetes集群中将 Vector 和 ClickHouse 结合使用可以构建一个强大的日志收集和分析系统。</p><span id="more"></span><p>方案下载地址： <a href="/download/zip/vector+clickhouse.zip">vector+clickhouse</a></p><p>以下是使用 Vector 和 ClickHouse 进行集群日志收集的方案说明：</p><h1 id="日志采集"><a href="#日志采集" class="headerlink" title="日志采集"></a>日志采集</h1><p>使用 Vector 作为日志采集器，它可以部署在每个节点上，负责收集和处理日志数据。<br>Vector 支持从多种来源采集日志，包括文件、标准输出、网络等。</p><h1 id="日志传输"><a href="#日志传输" class="headerlink" title="日志传输"></a>日志传输</h1><p>Vector 可以将采集到的日志数据进行处理和转换，然后传输到 ClickHouse。<br>传输过程中，Vector 支持各种数据格式和协议，确保日志数据可以高效、准确地发送到 ClickHouse。</p><h1 id="日志存储"><a href="#日志存储" class="headerlink" title="日志存储"></a>日志存储</h1><p>ClickHouse 作为日志数据的存储系统，提供高性能的读写能力。<br>ClickHouse 的表结构需要根据日志数据的特点进行设计，以优化查询性能。</p><h1 id="日志处理"><a href="#日志处理" class="headerlink" title="日志处理"></a>日志处理</h1><p>在 Vector 中，可以使用 Vector Remap Language (VRL) 对日志数据进行清洗和转换。<br>可以定义复杂的处理逻辑，将非结构化的日志数据转换为结构化的格式，以便于存储和查询。</p><h1 id="日志查询"><a href="#日志查询" class="headerlink" title="日志查询"></a>日志查询</h1><p>ClickHouse 提供了强大的 SQL 查询能力，可以对存储的日志数据执行快速查询。<br>可以利用 ClickHouse 的分布式查询功能，跨多个节点并行执行查询，提高查询效率。</p><h1 id="日志可视化"><a href="#日志可视化" class="headerlink" title="日志可视化"></a>日志可视化</h1><p>虽然 ClickHouse 本身不提供可视化界面，但可以将查询结果导出到 Grafana 或其他可视化工具中，以图形化的方式展示日志数据。</p><h1 id="日志生命周期管理"><a href="#日志生命周期管理" class="headerlink" title="日志生命周期管理"></a>日志生命周期管理</h1><p>ClickHouse 支持设置表的 TTL（Time To Live），自动清理过期的日志数据。<br>可以配置定时任务，对超时的日志执行删除操作，管理日志数据的生命周期。</p><h1 id="系统监控与告警"><a href="#系统监控与告警" class="headerlink" title="系统监控与告警"></a>系统监控与告警</h1><p>结合监控系统，可以对日志收集和查询的性能进行监控，并设置告警。<br>监控系统可以帮助及时发现和解决潜在的问题，保证日志系统的稳定运行。</p><h1 id="安全性"><a href="#安全性" class="headerlink" title="安全性"></a>安全性</h1><p>需要确保日志数据的安全，包括数据的加密传输和存储，以及访问控制。</p><h1 id="高可用性"><a href="#高可用性" class="headerlink" title="高可用性"></a>高可用性</h1><p>为了提高系统的可用性，可以在多个节点上部署 Vector 实例，并在 ClickHouse 中设置数据副本。</p><h1 id="扩展性"><a href="#扩展性" class="headerlink" title="扩展性"></a>扩展性</h1><p>随着日志数据量的增长，可以水平扩展 Vector 和 ClickHouse 集群，以应对更大的数据处理需求。<br>使用 Vector 和 ClickHouse 的日志收集方案可以提供高效、可扩展且成本效益高的日志管理能力。通过合理的配置和优化，可以满足大规模日志数据的收集、存储和分析需求。</p><p>ClickHouse 集群方案参考 <a href="/download/">download</a> 下的clickhous-cluster</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Kubernetes集群中将 Vector 和 ClickHouse 结合使用可以构建一个强大的日志收集和分析系统。&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes" scheme="https://huisebug.github.io/categories/Kubernetes/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="kubernetes" scheme="https://huisebug.github.io/tags/kubernetes/"/>
    
    <category term="vector" scheme="https://huisebug.github.io/tags/vector/"/>
    
    <category term="vector-aggregator" scheme="https://huisebug.github.io/tags/vector-aggregator/"/>
    
    <category term="clickhouse" scheme="https://huisebug.github.io/tags/clickhouse/"/>
    
    <category term="clickhouse-cluster" scheme="https://huisebug.github.io/tags/clickhouse-cluster/"/>
    
  </entry>
  
  <entry>
    <title>terraform部署kubernetes1.26.12集群</title>
    <link href="https://huisebug.github.io/2024/02/04/terraform-kubernetes/"/>
    <id>https://huisebug.github.io/2024/02/04/terraform-kubernetes/</id>
    <published>2024-02-04T08:04:01.000Z</published>
    <updated>2024-02-04T08:44:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>使用terraform部署一个kubernetes1.26.12集群</p><span id="more"></span><h1 id="Terraform"><a href="#Terraform" class="headerlink" title="Terraform"></a>Terraform</h1><p>用途： Terraform是一种基础设施即代码（IaC）工具，用于自动化云和本地基础设施的创建、更新和删除。它支持多云平台，如AWS、Azure、Google Cloud等，以及本地虚拟化平台。<br>工作原理： 用户通过编写HCL（HashiCorp Configuration Language）代码描述所需的基础设施资源，例如虚拟机、存储桶、网络配置等。然后，Terraform根据这些代码执行计划，将实际基础设施配置为代码中所描述的状态。</p><h1 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h1><p>用途： Kubernetes是一个开源的容器编排平台，用于自动化、部署、扩展和管理容器化应用程序。它提供了一种容器编排的解决方案，使得应用程序可以在一个集群中灵活运行，并自动处理容器的调度、伸缩和故障恢复。<br>工作原理： 用户将应用程序容器化，并使用Kubernetes定义应用程序的部署、服务、配置等方面的规范。Kubernetes集群负责管理这些容器，确保它们在可用节点上运行，并提供高度可扩展的、自我修复的应用程序部署。</p><h1 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h1><p><a href="https://github.com/huisebug/terraform-kubernetes.git">https://github.com/huisebug/terraform-kubernetes.git</a></p><h1 id="项目介绍"><a href="#项目介绍" class="headerlink" title="项目介绍"></a>项目介绍</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">├── down.sh                            #使用containerd下载kubernetes的客户端，如kubectl、kubelet</span><br><span class="line">├── main.tf                            #terraform 入口tf                            </span><br><span class="line">├── modules                            #terraform modules</span><br><span class="line">│   ├── container                      #container，主要是安装containerd服务</span><br><span class="line">│   │   ├── dnf.tf</span><br><span class="line">│   │   ├── main.tf</span><br><span class="line">│   │   ├── templates</span><br><span class="line">│   │   │   └── config.toml</span><br><span class="line">│   │   └── variables.tf</span><br><span class="line">│   ├── containerimage-pull            #使用把后续所需的容器镜像提前pull</span><br><span class="line">│   │   ├── main.tf</span><br><span class="line">│   │   └── variables.tf</span><br><span class="line">│   ├── coredns                        </span><br><span class="line">│   │   ├── main.tf</span><br><span class="line">│   │   ├── templates</span><br><span class="line">│   │   │   ├── coredns-configmap.yaml</span><br><span class="line">│   │   │   └── coredns.yaml</span><br><span class="line">│   │   └── variables.tf</span><br><span class="line">│   ├── etcd</span><br><span class="line">│   │   ├── main.tf</span><br><span class="line">│   │   ├── templates</span><br><span class="line">│   │   │   ├── etcd.config.yml</span><br><span class="line">│   │   │   └── etcd.yaml</span><br><span class="line">│   │   └── variables.tf</span><br><span class="line">│   ├── init                           #对centos 8 系统进行系统初始化配置</span><br><span class="line">│   │   ├── dnf.tf</span><br><span class="line">│   │   ├── files</span><br><span class="line">│   │   │   ├── containerd.conf</span><br><span class="line">│   │   │   ├── ipvs.conf</span><br><span class="line">│   │   │   ├── istio.conf</span><br><span class="line">│   │   │   ├── k8s-sysctl.conf</span><br><span class="line">│   │   │   └── limit.conf</span><br><span class="line">│   │   ├── main.tf</span><br><span class="line">│   │   ├── templates</span><br><span class="line">│   │   │   └── chrony.conf</span><br><span class="line">│   │   └── variables.tf</span><br><span class="line">│   ├── k8s-cni                        #使用kubernetes cni服务，支持calico和flannel</span><br><span class="line">│   │   ├── calico.tf</span><br><span class="line">│   │   ├── kube-flannel.tf</span><br><span class="line">│   │   ├── main.tf</span><br><span class="line">│   │   ├── templates</span><br><span class="line">│   │   │   ├── calico.yaml</span><br><span class="line">│   │   │   └── kube-flannel.yaml</span><br><span class="line">│   │   └── variables.tf</span><br><span class="line">│   ├── k8s-master</span><br><span class="line">│   │   ├── main.tf</span><br><span class="line">│   │   ├── templates</span><br><span class="line">│   │   │   ├── kube-apiserver.yaml</span><br><span class="line">│   │   │   ├── kube-controller-manager.yaml</span><br><span class="line">│   │   │   └── kube-scheduler.yaml</span><br><span class="line">│   │   └── variables.tf</span><br><span class="line">│   ├── kubeconfig</span><br><span class="line">│   │   ├── main.tf</span><br><span class="line">│   │   └── variables.tf</span><br><span class="line">│   ├── kubelet</span><br><span class="line">│   │   ├── main.tf</span><br><span class="line">│   │   ├── templates</span><br><span class="line">│   │   │   ├── kubelet-conf.yml</span><br><span class="line">│   │   │   └── kubelet.service</span><br><span class="line">│   │   └── variables.tf</span><br><span class="line">│   ├── kubelet-bootstrap</span><br><span class="line">│   │   ├── main.tf</span><br><span class="line">│   │   └── variables.tf</span><br><span class="line">│   ├── kubelet-bootstrap-csr</span><br><span class="line">│   │   ├── files</span><br><span class="line">│   │   │   └── kubelet-bootstrap-csr.yaml</span><br><span class="line">│   │   └── main.tf</span><br><span class="line">│   ├── kube-proxy</span><br><span class="line">│   │   ├── main.tf</span><br><span class="line">│   │   ├── templates</span><br><span class="line">│   │   │   └── kube-proxy.yaml</span><br><span class="line">│   │   └── variables.tf</span><br><span class="line">│   ├── kube-vip                        #kube-apiserver的负载均衡</span><br><span class="line">│   │   ├── main.tf</span><br><span class="line">│   │   ├── templates</span><br><span class="line">│   │   │   └── kube-vip.yaml</span><br><span class="line">│   │   └── variables.tf</span><br><span class="line">│   ├── ping                            #测试机器连通性</span><br><span class="line">│   │   ├── main.tf</span><br><span class="line">│   │   └── variables.tf</span><br><span class="line">│   └── tls</span><br><span class="line">│       ├── main.tf</span><br><span class="line">│       ├── templates</span><br><span class="line">│       │   └── openssl.cnf</span><br><span class="line">│       └── variables.tf</span><br><span class="line">├── terraform.tfvars                    #tf配置，需要修改相关信息</span><br><span class="line">├── tf.sh                               #执行shell脚本，无需执行其他命令，在这里进行交互执行即可</span><br><span class="line">└── variables.tf                        #tf变量</span><br></pre></td></tr></table></figure><h1 id="配置修改介绍terraform-tfvars"><a href="#配置修改介绍terraform-tfvars" class="headerlink" title="配置修改介绍terraform.tfvars "></a>配置修改介绍terraform.tfvars </h1><p>主要修改下面的地方</p><ul><li>  如果你有3台服务器,希望3台都是master节点(默认会安装node节点),那就按照如下格式进行填写，node变量留空</li><li>  当master超过2台时,说明集群需要负载均衡,这时候就需要在clusterha处填写预留的vip信息</li><li>  console是为了方便对机器系统初始化后进行重启生效,terraform不允许机器重启,后面会特别说明<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 作为terraform管理的节点</span></span><br><span class="line">console = &quot;192.168.137.100&quot;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">作为master的节点</span></span><br><span class="line">master = [</span><br><span class="line">  &#123;</span><br><span class="line">  ip    = &quot;192.168.137.100&quot;</span><br><span class="line">  hostname     = &quot;k8s-m1&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">  ip    = &quot;192.168.137.101&quot;</span><br><span class="line">  hostname     = &quot;k8s-m2&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">作为node的节点</span></span><br><span class="line">node = [</span><br><span class="line">  &#123;</span><br><span class="line">  ip    = &quot;192.168.137.102&quot;</span><br><span class="line">  hostname     = &quot;k8s-n1&quot;</span><br><span class="line">  &#125;,    </span><br><span class="line">]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">作为etcd的节点</span></span><br><span class="line">etcd = [</span><br><span class="line">  &#123;</span><br><span class="line">  ip    = &quot;192.168.137.100&quot;</span><br><span class="line">  clusterName     = &quot;k8s-m1&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">  ip    = &quot;192.168.137.101&quot;</span><br><span class="line">  clusterName     = &quot;k8s-m2&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">  ip    = &quot;192.168.137.102&quot;</span><br><span class="line">  clusterName     = &quot;k8s-m3&quot;</span><br><span class="line">  &#125;,    </span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">clusterha = &#123;</span><br><span class="line"><span class="meta">  #</span><span class="bash">k8s集群HA vip IP地址</span></span><br><span class="line">  vip = &quot;192.168.137.99&quot;</span><br><span class="line"><span class="meta">  #</span><span class="bash">k8s集群HA vip IP地址子网掩码，一般是32</span></span><br><span class="line">  vipSubnet = &quot;24&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h1 id="执行安装"><a href="#执行安装" class="headerlink" title="执行安装"></a>执行安装</h1><p>1)执行脚本，脚本会安装terraform，并且进行terraform init</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash tf.sh</span><br></pre></td></tr></table></figure><p><img src="/2024/02/04/terraform-kubernetes/media/terraform-install.png"></p><p>2)查看tf.sh效果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash tf.sh</span><br></pre></td></tr></table></figure><p><img src="/2024/02/04/terraform-kubernetes/media/tfsh.png"></p><p>依照上述的module会对应序列号，只需要按照序列号传递即可<br>例如 1 ,然后回车<br><img src="/2024/02/04/terraform-kubernetes/media/tfsh1.png"></p><p>3)初始化所有机器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash tf.sh    #输入2</span><br></pre></td></tr></table></figure><p><img src="/2024/02/04/terraform-kubernetes/media/tfsh2.png"><br>最终会提示机器ssh断开,这是正常的,terraform不允许机器重启,此时已经重启了除开terraform console的所有机器<br><img src="/2024/02/04/terraform-kubernetes/media/tfsh3.png"></p><p>4)重启当前机器<br>这时候terraform console机器(也就是本机)需要手动重启reboot 或 init 6</p><p>5)部署containerd服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash tf.sh    #输入3</span><br></pre></td></tr></table></figure><p><img src="/2024/02/04/terraform-kubernetes/media/tfsh4.png"><br>完成后会执行down.sh下载相关的kubernetes二进制包<br><img src="/2024/02/04/terraform-kubernetes/media/tfsh5.png"></p><p>6)直接执行安装剩余的过程：install  </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash tf.sh      #输入install  </span><br></pre></td></tr></table></figure><p><img src="/2024/02/04/terraform-kubernetes/media/tfshinstall.png"></p><p>7)查看节点状态和pod信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node  </span><br><span class="line">kubectl get pod -A </span><br></pre></td></tr></table></figure><p><img src="/2024/02/04/terraform-kubernetes/media/k8sok.png"><br>注意：我这里是2master 1node的集群</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;使用terraform部署一个kubernetes1.26.12集群&lt;/p&gt;</summary>
    
    
    
    <category term="Terraform" scheme="https://huisebug.github.io/categories/Terraform/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="shell" scheme="https://huisebug.github.io/tags/shell/"/>
    
    <category term="kubernetes" scheme="https://huisebug.github.io/tags/kubernetes/"/>
    
    <category term="terraform" scheme="https://huisebug.github.io/tags/terraform/"/>
    
  </entry>
  
  <entry>
    <title>golang terraform kubernetes结构体</title>
    <link href="https://huisebug.github.io/2024/02/04/terraform-k8s-golangstruct/"/>
    <id>https://huisebug.github.io/2024/02/04/terraform-k8s-golangstruct/</id>
    <published>2024-02-04T03:04:01.000Z</published>
    <updated>2024-05-21T08:00:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>terraform提供了kubernetes provider(相当于kubectl),但是却不兼容k8s api库的json输出名,对整体输出名都进行了重构;<br>此处例举了Deployment Service Job 等Kind的golang struct,希望可以帮到golang开发人员;</p><span id="more"></span><p>terraform-k8s-type.go</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br></pre></td><td class="code"><pre><span class="line">package terraform</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">appsv1 &quot;k8s.io/api/apps/v1&quot;</span><br><span class="line">corev1 &quot;k8s.io/api/core/v1&quot;</span><br><span class="line">&quot;k8s.io/apimachinery/pkg/api/resource&quot;</span><br><span class="line">&quot;k8s.io/apimachinery/pkg/util/intstr&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">type TfFile struct &#123;</span><br><span class="line">Provider Kubernetes     `json:&quot;provider,omitempty&quot;`</span><br><span class="line">Resource ResTypeAndName `json:&quot;resource,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type Kubernetes struct &#123;</span><br><span class="line">Kubernetes KubernetesConfig `json:&quot;kubernetes,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type KubernetesConfig struct &#123;</span><br><span class="line">ConfigPath        string   `json:&quot;config_path&quot;`</span><br><span class="line">IgnoreAnnotations []string `json:&quot;ignore_annotations&quot;`</span><br><span class="line">Insecure          bool     `json:&quot;insecure&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type ResTypeAndName struct &#123;</span><br><span class="line">KubernetesDeployment  map[string]DeploymentTF  `json:&quot;kubernetes_deployment,omitempty&quot;`</span><br><span class="line">KubernetesStatefulSet map[string]StatefulSetTF `json:&quot;kubernetes_stateful_set,omitempty&quot;`</span><br><span class="line">KubernetesJob         map[string]JobTF         `json:&quot;kubernetes_job,omitempty&quot;`</span><br><span class="line">KubernetesService     map[string]ServiceTF     `json:&quot;kubernetes_service,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line">type DeploymentTF struct &#123;</span><br><span class="line">Deployment</span><br><span class="line">WaitForRollout bool `json:&quot;wait_for_rollout&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type StatefulSetTF struct &#123;</span><br><span class="line">StatefulSet</span><br><span class="line">WaitForRollout bool `json:&quot;wait_for_rollout&quot;`</span><br><span class="line">&#125;</span><br><span class="line">type Deployment struct &#123;</span><br><span class="line">ObjectMeta ObjectMeta     `json:&quot;metadata,omitempty&quot;`</span><br><span class="line">Spec       DeploymentSpec `json:&quot;spec,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type StatefulSet struct &#123;</span><br><span class="line">ObjectMeta ObjectMeta      `json:&quot;metadata,omitempty&quot;`</span><br><span class="line">Spec       StatefulSetSpec `json:&quot;spec,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type StatefulSetSpec struct &#123;</span><br><span class="line">Replicas                             *int32                                           `json:&quot;replicas,omitempty&quot;`</span><br><span class="line">Selector                             *LabelSelector                                   `json:&quot;selector&quot;`</span><br><span class="line">Template                             PodTemplateSpec                                  `json:&quot;template&quot;`</span><br><span class="line">VolumeClaimTemplates                 []PersistentVolumeClaim                          `json:&quot;volume_claim_template,omitempty&quot;`</span><br><span class="line">ServiceName                          string                                           `json:&quot;service_name&quot;`</span><br><span class="line">PodManagementPolicy                  appsv1.PodManagementPolicyType                   `json:&quot;pod_management_policy,omitempty&quot;`</span><br><span class="line">UpdateStrategy                       StatefulSetUpdateStrategy                        `json:&quot;update_strategy,omitempty&quot;`</span><br><span class="line">RevisionHistoryLimit                 *int32                                           `json:&quot;revisionHistoryLimit,omitempty&quot;`</span><br><span class="line">PersistentVolumeClaimRetentionPolicy *StatefulSetPersistentVolumeClaimRetentionPolicy `json:&quot;persistent_volume_claim_retention_policy,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type PersistentVolumeClaim struct &#123;</span><br><span class="line">ObjectMeta ObjectMeta                `json:&quot;metadata,omitempty&quot;`</span><br><span class="line">Spec       PersistentVolumeClaimSpec `json:&quot;spec,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type PersistentVolumeClaimSpec struct &#123;</span><br><span class="line">AccessModes      []corev1.PersistentVolumeAccessMode `json:&quot;access_modes,omitempty&quot;`</span><br><span class="line">Resources        VolumeResourceRequirements          `json:&quot;resources,omitempty&quot;`</span><br><span class="line">StorageClassName *string                             `json:&quot;storage_class_name,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type VolumeResourceRequirements struct &#123;</span><br><span class="line">Limits   corev1.ResourceList `json:&quot;limits,omitempty&quot;`</span><br><span class="line">Requests corev1.ResourceList `json:&quot;requests,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type StatefulSetPersistentVolumeClaimRetentionPolicy struct &#123;</span><br><span class="line">WhenDeleted appsv1.PersistentVolumeClaimRetentionPolicyType `json:&quot;when_deleted,omitempty&quot;`</span><br><span class="line">WhenScaled  appsv1.PersistentVolumeClaimRetentionPolicyType `json:&quot;when_scaled,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type StatefulSetUpdateStrategy struct &#123;</span><br><span class="line">Type          appsv1.StatefulSetUpdateStrategyType `json:&quot;type,omitempty&quot;`</span><br><span class="line">RollingUpdate *RollingUpdateStatefulSetStrategy    `json:&quot;rolling_update,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type RollingUpdateStatefulSetStrategy struct &#123;</span><br><span class="line">Partition      *int32              `json:&quot;partition,omitempty&quot;`</span><br><span class="line">MaxUnavailable *intstr.IntOrString `json:&quot;max_unavailable,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type DeploymentSpec struct &#123;</span><br><span class="line">Replicas *int32             `json:&quot;replicas,omitempty&quot;`</span><br><span class="line">Selector *LabelSelector     `json:&quot;selector&quot;`</span><br><span class="line">Template PodTemplateSpec    `json:&quot;template&quot;`</span><br><span class="line">Strategy DeploymentStrategy `json:&quot;strategy,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type LabelSelector struct &#123;</span><br><span class="line">MatchLabels map[string]string `json:&quot;match_labels,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type DeploymentStrategy struct &#123;</span><br><span class="line">Type          appsv1.DeploymentStrategyType `json:&quot;type,omitempty&quot;`</span><br><span class="line">RollingUpdate *RollingUpdateDeployment      `json:&quot;rolling_update,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type RollingUpdateDeployment struct &#123;</span><br><span class="line">MaxUnavailable *intstr.IntOrString `json:&quot;max_unavailable,omitempty&quot;`</span><br><span class="line">MaxSurge       *intstr.IntOrString `json:&quot;max_surge,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type JobTF struct &#123;</span><br><span class="line">Job</span><br><span class="line">WaitForCompletion bool              `json:&quot;wait_for_completion&quot;`</span><br><span class="line">Timeouts          map[string]string `json:&quot;timeouts,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line">type Job struct &#123;</span><br><span class="line">ObjectMeta ObjectMeta `json:&quot;metadata,omitempty&quot;`</span><br><span class="line">Spec       JobSpec    `json:&quot;spec,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type ObjectMeta struct &#123;</span><br><span class="line">Name        string            `json:&quot;name,omitempty&quot;`</span><br><span class="line">Namespace   string            `json:&quot;namespace,omitempty&quot;`</span><br><span class="line">Labels      map[string]string `json:&quot;labels,omitempty&quot;`</span><br><span class="line">Annotations map[string]string `json:&quot;annotations,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type JobSpec struct &#123;</span><br><span class="line">Completions *int32            `json:&quot;completions,omitempty&quot;`</span><br><span class="line">Selector    map[string]string `json:&quot;selector,omitempty&quot;`</span><br><span class="line">Template    PodTemplateSpec   `json:&quot;template&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type ServiceTF struct &#123;</span><br><span class="line">Service</span><br><span class="line">WaitForLoadBalancer bool `json:&quot;wait_for_load_balancer&quot;`</span><br><span class="line">&#125;</span><br><span class="line">type Service struct &#123;</span><br><span class="line">ObjectMeta ObjectMeta  `json:&quot;metadata,omitempty&quot;`</span><br><span class="line">Spec       ServiceSpec `json:&quot;spec,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line">type ServiceSpec struct &#123;</span><br><span class="line">ClusterIP string             `json:&quot;cluster_ip,omitempty&quot;`</span><br><span class="line">Ports     []ServicePort      `json:&quot;port,omitempty&quot;`</span><br><span class="line">Selector  map[string]string  `json:&quot;selector,omitempty&quot;`</span><br><span class="line">Type      corev1.ServiceType `json:&quot;type,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type ServicePort struct &#123;</span><br><span class="line">Name string `json:&quot;name,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">Protocol corev1.Protocol `json:&quot;protocol,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">AppProtocol *string `json:&quot;app_protocol,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">Port int32 `json:&quot;port&quot; `</span><br><span class="line"></span><br><span class="line">TargetPort intstr.IntOrString `json:&quot;target_port,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">NodePort int32 `json:&quot;node_port,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type PodTemplateSpec struct &#123;</span><br><span class="line">ObjectMeta ObjectMeta `json:&quot;metadata,omitempty&quot;`</span><br><span class="line">Spec       PodSpec    `json:&quot;spec,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type PodSpec struct &#123;</span><br><span class="line">Volumes []Volume `json:&quot;volume,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">InitContainers []Container `json:&quot;init_container,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">Containers []Container `json:&quot;container&quot;`</span><br><span class="line"></span><br><span class="line">RestartPolicy corev1.RestartPolicy `json:&quot;restart_policy,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">NodeSelector map[string]string `json:&quot;node_selector,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">ServiceAccountName string `json:&quot;serviceAccountName,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">NodeName string `json:&quot;node_name,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">ImagePullSecrets []corev1.LocalObjectReference `json:&quot;image_pull_secrets,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">Affinity *Affinity `json:&quot;affinity,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">RuntimeClassName *string `json:&quot;runtime_class_name,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">EnableServiceLinks *bool `json:&quot;enable_service_links,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type Affinity struct &#123;</span><br><span class="line">NodeAffinity *NodeAffinity `json:&quot;node_affinity,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">PodAffinity *PodAffinity `json:&quot;pod_affinity,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">PodAntiAffinity *PodAntiAffinity `json:&quot;pod_anti_affinity,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type PodAffinity struct &#123;</span><br><span class="line">RequiredDuringSchedulingIgnoredDuringExecution []PodAffinityTerm `json:&quot;required_during_scheduling_ignored_during_execution,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">PreferredDuringSchedulingIgnoredDuringExecution []WeightedPodAffinityTerm `json:&quot;preferred_during_scheduling_ignored_during_execution,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type NodeAffinity struct &#123;</span><br><span class="line">RequiredDuringSchedulingIgnoredDuringExecution *NodeSelector `json:&quot;required_during_scheduling_ignored_during_execution,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">PreferredDuringSchedulingIgnoredDuringExecution []PreferredSchedulingTerm `json:&quot;preferred_during_scheduling_ignored_during_execution,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type PodAntiAffinity struct &#123;</span><br><span class="line">RequiredDuringSchedulingIgnoredDuringExecution []PodAffinityTerm `json:&quot;required_during_scheduling_ignored_during_execution,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">PreferredDuringSchedulingIgnoredDuringExecution []WeightedPodAffinityTerm `json:&quot;preferred_during_scheduling_ignored_during_execution,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type PreferredSchedulingTerm struct &#123;</span><br><span class="line">Weight     int32            `json:&quot;weight&quot;`</span><br><span class="line">Preference NodeSelectorTerm `json:&quot;preference&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type WeightedPodAffinityTerm struct &#123;</span><br><span class="line">Weight          int32           `json:&quot;weight&quot;`</span><br><span class="line">PodAffinityTerm PodAffinityTerm `json:&quot;pod_affinity_term&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type PodAffinityTerm struct &#123;</span><br><span class="line">LabelSelector *LabelSelector `json:&quot;label_selector,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">Namespaces []string `json:&quot;namespaces,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">TopologyKey string `json:&quot;topology_key&quot;`</span><br><span class="line">&#125;</span><br><span class="line">type NodeSelector struct &#123;</span><br><span class="line">NodeSelectorTerms []NodeSelectorTerm `json:&quot;node_selector_term&quot;`</span><br><span class="line">&#125;</span><br><span class="line">type NodeSelectorTerm struct &#123;</span><br><span class="line">MatchExpressions []corev1.NodeSelectorRequirement `json:&quot;match_expressions,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">MatchFields []corev1.NodeSelectorRequirement `json:&quot;match_fields,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type Volume struct &#123;</span><br><span class="line">Name         string `json:&quot;name&quot;`</span><br><span class="line">VolumeSource `json:&quot;,inline&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type VolumeSource struct &#123;</span><br><span class="line">PersistentVolumeClaim *PersistentVolumeClaimVolumeSource `json:&quot;persistent_volume_claim,omitempty&quot;`</span><br><span class="line">EmptyDir              *EmptyDirVolumeSource              `json:&quot;empty_dir,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line">type EmptyDirVolumeSource struct &#123;</span><br><span class="line">Medium    corev1.StorageMedium `json:&quot;medium,omitempty&quot;`</span><br><span class="line">SizeLimit *resource.Quantity   `json:&quot;size_limit,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type PersistentVolumeClaimVolumeSource struct &#123;</span><br><span class="line">ClaimName string `json:&quot;claim_name&quot;`</span><br><span class="line">ReadOnly  bool   `json:&quot;readOnly,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type Container struct &#123;</span><br><span class="line">Name string `json:&quot;name&quot;`</span><br><span class="line"></span><br><span class="line">Image string `json:&quot;image,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">Command []string `json:&quot;command,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">Args []string `json:&quot;args,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">WorkingDir string `json:&quot;working_dir,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">Ports []ContainerPort `json:&quot;port,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">Env []EnvVar `json:&quot;env,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">Resources corev1.ResourceRequirements `json:&quot;resources,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">VolumeMounts []VolumeMount `json:&quot;volume_mount,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">LivenessProbe *Probe `json:&quot;liveness_probe,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">ReadinessProbe *Probe `json:&quot;readiness_probe,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">ImagePullPolicy corev1.PullPolicy `json:&quot;image_pull_policy,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type ContainerPort struct &#123;</span><br><span class="line">Name string `json:&quot;name,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">HostPort int32 `json:&quot;host_port,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">ContainerPort int32 `json:&quot;container_port&quot;`</span><br><span class="line"></span><br><span class="line">Protocol corev1.Protocol `json:&quot;protocol,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">HostIP string `json:&quot;host_ip,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line">type EnvVar struct &#123;</span><br><span class="line">Name string `json:&quot;name&quot;`</span><br><span class="line"></span><br><span class="line">Value string `json:&quot;value,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line">type VolumeMount struct &#123;</span><br><span class="line">Name string `json:&quot;name&quot;`</span><br><span class="line"></span><br><span class="line">ReadOnly bool `json:&quot;readOnly,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">MountPath string `json:&quot;mount_path&quot;`</span><br><span class="line"></span><br><span class="line">SubPath string `json:&quot;sub_path,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type Probe struct &#123;</span><br><span class="line">ProbeHandler</span><br><span class="line"></span><br><span class="line">InitialDelaySeconds int32 `json:&quot;initial_delay_seconds,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">TimeoutSeconds int32 `json:&quot;timeout_seconds,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">PeriodSeconds int32 `json:&quot;period_seconds,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">SuccessThreshold int32 `json:&quot;success_threshold,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">FailureThreshold int32 `json:&quot;failure_threshold,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line">type ProbeHandler struct &#123;</span><br><span class="line">Exec *corev1.ExecAction `json:&quot;exec,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">HTTPGet *HTTPGetAction `json:&quot;http_get,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">TCPSocket *corev1.TCPSocketAction `json:&quot;tcp_socket,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type HTTPGetAction struct &#123;</span><br><span class="line">Path string `json:&quot;path,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">Port intstr.IntOrString `json:&quot;port&quot;`</span><br><span class="line"></span><br><span class="line">Host string `json:&quot;host,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">Scheme corev1.URIScheme `json:&quot;scheme,omitempty&quot;`</span><br><span class="line"></span><br><span class="line">HTTPHeaders []HTTPHeader `json:&quot;http_header,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line">type HTTPHeader struct &#123;</span><br><span class="line">Name  string `json:&quot;name&quot;`</span><br><span class="line">Value string `json:&quot;value&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;terraform提供了kubernetes provider(相当于kubectl),但是却不兼容k8s api库的json输出名,对整体输出名都进行了重构;&lt;br&gt;此处例举了Deployment Service Job 等Kind的golang struct,希望可以帮到golang开发人员;&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes-Dev" scheme="https://huisebug.github.io/categories/Kubernetes-Dev/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="kubernetes" scheme="https://huisebug.github.io/tags/kubernetes/"/>
    
    <category term="terraform" scheme="https://huisebug.github.io/tags/terraform/"/>
    
    <category term="golang" scheme="https://huisebug.github.io/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>kubernetesalias:kubernetes封装命令工具</title>
    <link href="https://huisebug.github.io/2023/08/03/kubernetesalias/"/>
    <id>https://huisebug.github.io/2023/08/03/kubernetesalias/</id>
    <published>2023-08-03T08:04:01.000Z</published>
    <updated>2024-06-17T02:04:01.000Z</updated>
    
    <content type="html"><![CDATA[<p><font color="#483D8B" size="6" face="黑体">愉快的执行kubernetes集群封装命令</font></p><span id="more"></span><h1 id="使用要求"><a href="#使用要求" class="headerlink" title="使用要求"></a>使用要求</h1><p><font size="3" face="黑体">工具需要在master节点进行运行,确保存在集群管理员kubeconfig(/etc/kubernetes/admin.conf)文件</font></p><h1 id="功能描述"><a href="#功能描述" class="headerlink" title="功能描述"></a>功能描述</h1><p><font size="6" face="黑体">点击工具名称即可进行下载</font></p><p><a href="/download/exe/kls">kls</a><br>列出整个集群的所有pod信息,包含pod ip,所在节点; 仅列出某个namespace,例如: kls kube-system(可选)</p><p><a href="/download/exe/klss">klss</a><br>列出整个集群的所有service信息; 仅列出某个namespace,例如: klss kube-system(可选)</p><p><a href="/download/exe/kdel">kdel</a><br>传递条件,筛选出符合条件的所有pod信息,传递编号进行删除对应编号的pod;支持删除单个pod,同时删除多个pod<br><img src="/2023/08/03/kubernetesalias/media/kdel.png"></p><p><a href="/download/exe/klog">klog</a><br>传递条件,筛选出符合条件的所有pod及容器信息,传递编号进行查看对应编号的容器日志; 例如: klog pod名 查看倒数行数(可选)<br><img src="/2023/08/03/kubernetesalias/media/klog.png"></p><p><a href="/download/exe/kdebug">kdebug</a><br>为了安全和缩小容器镜像，大部分容器是不会安装常用的linux命令的，需要查看服务端口运行情况和查看ip地址信息和进程，可通过kubernetes Ephemeral Containers功能进行查看<br>此容器只是共享了pod，并不是源pod，所以仅能查看系统信息，例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">查看进程</span></span><br><span class="line">ps aux</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">查看端口运行</span></span><br><span class="line">netstat -lntp</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">查看ip</span></span><br><span class="line">ifconfig  或者 ip addr</span><br></pre></td></tr></table></figure><p><img src="/2023/08/03/kubernetesalias/media/kdebug.png"></p><p><a href="/download/exe/kebash">kebash</a><br>传递条件,筛选出符合条件的第一个pod使用bash进行终端连接; kebash pod名 传递容器编号(可选)<br><img src="/2023/08/03/kubernetesalias/media/kebash.png"></p><p><a href="/download/exe/kesh">kesh</a><br>传递条件,筛选出符合条件的第一个pod使用bash进行终端连接; kesh pod名 传递容器编号(可选)<br><img src="/2023/08/03/kubernetesalias/media/kesh.png"></p><p><font size="4" face="黑体">当容器镜像使用centos、debian、ubuntu时使用bash进行终端登录，使用的是alpine时使用sh进行终端登录，当然都是支持sh登录，只是sh功能没有bash全面,有些容器镜像可能连sh都不允许</font></p><p><a href="/download/exe/kup">kup</a><br>当更新配置文件,需要pod进行生效,可是如果删除pod将会导致服务不可用,部署集中有配置了更新策略,此工具就是触发部署集的更新策略,比如是滚动更新,那么部署集将会进行滚动更新<br>传递条件,筛选出符合条件的pod并且列出所使用的部署集方式; 传递pod对应编号,属于同一个部署集的pod都会触发更新<br><img src="/2023/08/03/kubernetesalias/media/kup.png"></p><p><a href="/download/exe/k8sexec">k8sexec</a><br>整合了kebash和kesh的功能<br>传递条件,筛选出符合条件的所有pod及容器信息,传递编号进行exec对应编号的容器中,第一次尝试使用bash,如果失败尝试使用sh,都失败说明镜像不支持终端<br><img src="/2023/08/03/kubernetesalias/media/k8sexec.png"></p><p><a href="/download/exe/kimage">kimage</a><br>手动更新业务镜像时,需要去手动编辑传递新的镜像,大部分时候只是更新了对应容器的镜像tag,利用这种机制可以使用kimage进行筛选更新<br>传递条件,筛选出符合条件的所有容器信息,传递编号进行容器镜像更新,业务中大部分是deployment部署<br><img src="/2023/08/03/kubernetesalias/media/kimage.png"></p><p><a href="/download/exe/kedit">kedit</a><br>优化手动输入kubectl edit功能<br><img src="/2023/08/03/kubernetesalias/media/kedit.png"></p><p><a href="/download/exe/knorun">knorun</a><br>清理整个集群中非Runing状态的pod,保留Job启动的pod;支持指定特定namespace进行清理<br><img src="/2023/08/03/kubernetesalias/media/knorun.png"></p><h1 id="Download-All"><a href="#Download-All" class="headerlink" title="Download All"></a>Download All</h1><p>直接下载所有工具</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo -v ; curl https://huisebug.github.io/download/exe/down-kubernetesalias.sh | sudo bash</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;font color=&quot;#483D8B&quot; size=&quot;6&quot; face=&quot;黑体&quot;&gt;愉快的执行kubernetes集群封装命令&lt;/font&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes-Dev" scheme="https://huisebug.github.io/categories/Kubernetes-Dev/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="kubernetes" scheme="https://huisebug.github.io/tags/kubernetes/"/>
    
    <category term="client-go" scheme="https://huisebug.github.io/tags/client-go/"/>
    
    <category term="kubectl" scheme="https://huisebug.github.io/tags/kubectl/"/>
    
  </entry>
  
  <entry>
    <title>kube-prometheus+thanos 多集群prometheus方案存储方案</title>
    <link href="https://huisebug.github.io/2023/06/28/kube-prometheus-thanos/"/>
    <id>https://huisebug.github.io/2023/06/28/kube-prometheus-thanos/</id>
    <published>2023-06-28T03:04:01.000Z</published>
    <updated>2024-05-21T09:27:14.047Z</updated>
    
    <content type="html"><![CDATA[<p>多kubernetes集群中,Prometheus 负责将监控数据写入 Thanos 存储网关，而 Thanos 查询网关则允许从 Thanos 存储网关中读取和查询数据。这种集成允许 Prometheus 在持久化存储和跨集群查询方面获得扩展性和弹性。</p><span id="more"></span><h1 id="kube-prometheus"><a href="#kube-prometheus" class="headerlink" title="kube-prometheus"></a>kube-prometheus</h1><p>原名为prometheus-operator,可参考之前的<a href="https://huisebug.github.io/2019/08/27/Prometheus-Operator/">Prometheus-Operator监控k8s</a></p><p>官方github<br><a href="https://github.com/prometheus-operator/kube-prometheus.git">https://github.com/prometheus-operator/kube-prometheus.git</a></p><p>整体部署</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone &lt;https://github.com/prometheus-operator/kube-prometheus.git&gt;</span><br><span class="line"></span><br><span class="line">cd kube-prometheus/manifests</span><br></pre></td></tr></table></figure><p>执行crd创建，因crd内容比较多，使用apply会提示long错误，此处使用create进行创建</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f setup/</span><br></pre></td></tr></table></figure><p>创建kube-prometheus组件所有服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f .</span><br></pre></td></tr></table></figure><p>创建完毕后会提供如下新增Kind</p><ul><li><p>  <a href="https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.Alertmanager">Alertmanager</a></p></li><li><p>  <a href="https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.PodMonitor">PodMonitor</a></p></li><li><p>  <a href="https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.Probe">Probe</a></p></li><li><p>  <a href="https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.Prometheus">Prometheus</a></p></li><li><p>  <a href="https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.PrometheusRule">PrometheusRule</a></p></li><li><p>  <a href="https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.ServiceMonitor">ServiceMonitor</a></p></li><li><p>  <a href="https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.ThanosRuler">ThanosRuler</a></p></li></ul><p>如何进行对应Kind的创建，<a href="https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md">官方介绍</a></p><h1 id="thanos"><a href="#thanos" class="headerlink" title="thanos"></a>thanos</h1><p>开源、高度可用的 Prometheus 设置具有长期存储功能</p><p>全局查询视图</p><p>通过跨多个 Prometheus 服务器和集群查询 Prometheus 指标来扩展 Prometheus 设置。</p><p>无限保留</p><p>使用您选择的对象存储来扩展系统，以无限期地存储您的指标。支持GCP、S3、Azure、Swift和腾讯COS。</p><p>普罗米修斯兼容</p><p>使用您喜欢的相同工具，例如 Grafana 和其他支持 Prometheus Query API 的工具。</p><p>下采样和压缩</p><p>在查询大时间范围或配置复杂的保留策略时，对历史数据进行下采样以大幅提高查询速度。</p><h2 id="组件服务"><a href="#组件服务" class="headerlink" title="组件服务"></a>组件服务</h2><h3 id="thanos-sidecar"><a href="#thanos-sidecar" class="headerlink" title="thanos-sidecar"></a>thanos-sidecar</h3><ul><li><p>  prometheus pod会新增一个容器，使用thanos容器镜像，启动时传参为sidecar，代表这是thanos-sidecar服务，sidecar会默认间隔2小时上传Prometheus 数据到对象存储(阿里云oss/S3一类)。</p></li><li><p>  thanos-sidecar会代理prometheus，让thanos-query对prometheus服务的所有数据进行访问</p></li></ul><h3 id="thanos-query"><a href="#thanos-query" class="headerlink" title="thanos-query"></a>thanos-query</h3><ul><li><p>  与prometheus管理界面相同功能，实现对多个prometheus进行聚合，同样是使用thnaos容器镜像，指定参数为query，并且指定endpoint使用grpc协议向底层组件(边车thanos-sidecar,存储thanos-store）获取数据</p></li><li><p>  可以对监控数据自动去重</p></li></ul><h3 id="thanos-query-frontend"><a href="#thanos-query-frontend" class="headerlink" title="thanos-query-frontend"></a>thanos-query-frontend</h3><p>当查询的数据规模较大的时候，对thanos-query组件也会有很大的压力，thanos-query-frontend组件来提升查询性能，thanos-query-frontend组件连接对象是thanos-query</p><h3 id="thanos-store"><a href="#thanos-store" class="headerlink" title="thanos-store"></a>thanos-store</h3><ul><li><p>  thanos-sidecar将prometheus数据上传到了对象存储，需要进行查询就需要经过thanos-store的处理提供给thanos-query进行查询</p></li><li><p>  并且thanos-store提供了缓存，加快查询速度的功能</p></li></ul><h3 id="thanos-compactor"><a href="#thanos-compactor" class="headerlink" title="thanos-compactor"></a>thanos-compactor</h3><ul><li><p>  将云存储中的数据进行压缩和下采样和保留</p></li><li><p>  管理对象存储中的数据(管理、压缩、删除等)</p></li></ul><h3 id="thanos-ruler"><a href="#thanos-ruler" class="headerlink" title="thanos-ruler"></a>thanos-ruler</h3><ul><li><p>  连接对象是thanos-query，经过thanos-query组件定期地获取指标数据，主要是prometheus的记录规则(record)和报警(alert)规则，其本身不会抓取metrics接口数据</p></li><li><p>  可将记录规则(record)上传到对象存储中</p></li><li><p>  可连接alertmanager服务统一将告警信息发送至alertmanager</p></li><li><p>  建议：避免alertmanager服务告警过于复杂，报警(alert)规则还是由各kubernetes集群prometheus进行处理</p></li></ul><h3 id="thanos-receive"><a href="#thanos-receive" class="headerlink" title="thanos-receive"></a>thanos-receive</h3><ul><li><p>  存在2个小组件thanos-receive-router和thanos-receive-ingestor</p></li><li><p>  thanos-sidecar上传数据到对象存储，但是数据上传并不是实时的，而是默认2h上传一个数据块，所以对象存储的数据并不是实时的，prometheus需要对数据进行持久化，这也是thanos-sidecar模式的弊端</p></li><li><p>  prometheus使用remote-write协议调用thanos-receive-router的api(/api/v1/receive),thanos-receive-router会将thanos-receive-ingestor的集群节点信息告诉prometheus，prometheus将会往thanos-receive-ingestor进行数据推送</p></li><li><p>  thanos-receive-ingestor则会将数据上传到对象存储中</p></li><li><p>  thanos-query会关联thanos-receive-ingestor，以查询对应kubernetes集群的prometheus数据</p></li></ul><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>架构分为2种模式</p><ul><li>  sidecar模式</li><li>  receive模式</li></ul><h3 id="sidecar模式"><a href="#sidecar模式" class="headerlink" title="sidecar模式"></a>sidecar模式</h3><p><img src="/2023/06/28/kube-prometheus-thanos/media/sidecar.jpg"></p><h3 id="receive模式"><a href="#receive模式" class="headerlink" title="receive模式"></a>receive模式</h3><p><img src="/2023/06/28/kube-prometheus-thanos/media/receive.jpg"></p><h1 id="kube-prometheus-thanos部署"><a href="#kube-prometheus-thanos部署" class="headerlink" title="kube-prometheus-thanos部署"></a>kube-prometheus-thanos部署</h1><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>提供经过我进行优化部署后的kube-prometheus+thanos的部署文件:<a href="https://huisebug.github.io/download/zip/kube-prometheus-thanos.zip">kube-prometheus-thanos.zip</a><br>prometheus仅选择报警规则，thanos-ruler仅选择记录规则，由prometheus负责往alertmanager进行告警发送</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd kube-prometheus-thanos</span><br></pre></td></tr></table></figure><p>执行crd创建，因crd内容比较多，使用apply会提示long错误，此处使用create进行创建</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f setup/</span><br></pre></td></tr></table></figure><p>创建kube-prometheus组件所有服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f .</span><br></pre></td></tr></table></figure><p>创建优化过的kube-prometheus组件服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f new/</span><br></pre></td></tr></table></figure><p>创建thanos组件服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f new/thanos</span><br></pre></td></tr></table></figure><p>本次部署是sidecar模式的部署，receive模式的部署在yaml是注释了的</p><h3 id="sidecar模式和receive模式配置区别"><a href="#sidecar模式和receive模式配置区别" class="headerlink" title="sidecar模式和receive模式配置区别"></a>sidecar模式和receive模式配置区别</h3><ul><li><p>  prometheus-prometheus.yaml分别配置是thanos-sidecar的配置还是remote-write的配置</p></li><li><p>  thanos-query.yaml中配置是连接thanos-receive-ingestor还是连接thanos-sidecar</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">--endpoint=dnssrv+_grpc._tcp.thanos-receive-ingestor-default.monitoring.svc.cluster.local:10901</span><br><span class="line">or</span><br><span class="line">--endpoint=dnssrv+_grpc._tcp.prometheus-operated.monitoring.svc.cluster.local:10901</span><br><span class="line">of</span><br><span class="line">配置ip的话</span><br><span class="line">--endpoint=ip:10901</span><br></pre></td></tr></table></figure><h2 id="部署功能说明"><a href="#部署功能说明" class="headerlink" title="部署功能说明"></a>部署功能说明</h2><ul><li><p>  alert-rules.yaml：对kubernetes集群的常用alert告警</p></li><li><p>  alertmanager-temp-configmap.yaml：自定义的企业微信告警模板</p></li><li><p>  alertmanager-secret.yaml：修改alertmanager配置，具体介绍请参考<a href="https://huisebug.github.io/2019/08/27/Prometheus-Operator/#alertmanager%E5%91%8A%E8%AD%A6">alertmanager告警</a></p></li><li><p>  prometheus-prometheus.yaml：定义了是sidecar模式的thanos-sidecar还是receive模式的remote-write</p></li><li><p>  thanos-objectstorage-secret.yaml：thanos组件使用的对象存储配置,可参考<a href="https://thanos.io/tip/thanos/storage.md">官方介绍</a></p></li><li><p>  new目录下的thanos-receive目录是receive模式的部署文件</p></li></ul><h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><p>1.level=warn ts=2023-06-26T06:14:54.058940439Z caller=shipper.go:239 msg=”reading meta file failed, will override it” err=”failed to read /prometheus/thanos.shipper.json: open /prometheus/thanos.shipper.json: no such file or directory”</p><p>解决方案:<br>使用thanos sidecar模式时，sidecar 会读取prometheus数据目录下的thanos.shipper.json文件，文件主要作用是当sidecar上传到对象存储时，会更新记录到此文件中。<br>出现上面的原因是thanos.shipper.json文件的权限是root，sidecar权限不够没法读取和写入，sidecar自身又没有重试读取的机制所以就算进入容器将这个文件重新授权为可写入和读取都不行，只能将prometheus进行持久化后进行这个文件授权777，然后重启prometheus整个pod</p><p>2.kube-prometheus支持配置日志等级debug来查看具体错误 各组件支持 logLevel: debug</p><p>3.企业微信现在申请机器人需要配置可信IP了，不然无法调用机器人</p><h1 id="成功效果图"><a href="#成功效果图" class="headerlink" title="成功效果图"></a>成功效果图</h1><h3 id="sidecar模式下的query"><a href="#sidecar模式下的query" class="headerlink" title="sidecar模式下的query"></a>sidecar模式下的query</h3><p>点击store选项可以看到query关联了哪些组件</p><p><img src="/2023/06/28/kube-prometheus-thanos/media/query-store-sidecar.png"></p><h3 id="receive模式下的query"><a href="#receive模式下的query" class="headerlink" title="receive模式下的query"></a>receive模式下的query</h3><p>点击store选项可以看到query关联了哪些组件</p><p><img src="/2023/06/28/kube-prometheus-thanos/media/query-store-receive.png"></p><h3 id="compact管理对象存储"><a href="#compact管理对象存储" class="headerlink" title="compact管理对象存储"></a>compact管理对象存储</h3><p><img src="/2023/06/28/kube-prometheus-thanos/media/compact.png"></p><h3 id="ruler"><a href="#ruler" class="headerlink" title="ruler"></a>ruler</h3><p><img src="/2023/06/28/kube-prometheus-thanos/media/ruler.png"></p><h3 id="对象存储数据"><a href="#对象存储数据" class="headerlink" title="对象存储数据"></a>对象存储数据</h3><p><img src="/2023/06/28/kube-prometheus-thanos/media/aliyun-oss.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;多kubernetes集群中,Prometheus 负责将监控数据写入 Thanos 存储网关，而 Thanos 查询网关则允许从 Thanos 存储网关中读取和查询数据。这种集成允许 Prometheus 在持久化存储和跨集群查询方面获得扩展性和弹性。&lt;/p&gt;</summary>
    
    
    
    <category term="Prometheus" scheme="https://huisebug.github.io/categories/Prometheus/"/>
    
    
  </entry>
  
  <entry>
    <title>wait-injection服务依赖等待注入</title>
    <link href="https://huisebug.github.io/2023/05/05/wait-injection/"/>
    <id>https://huisebug.github.io/2023/05/05/wait-injection/</id>
    <published>2023-05-05T03:04:01.000Z</published>
    <updated>2023-05-05T07:04:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>在kubernetes集群中,服务之间存在依赖关系,wait-injection利用initcontainer功能快速便捷的为服务添加相应的依赖关系</p><span id="more"></span><h1 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h1><p><a href="https://github.com/huisebug/wait-injection.git">https://github.com/huisebug/wait-injection.git</a></p><h1 id="部署配置"><a href="#部署配置" class="headerlink" title="部署配置"></a>部署配置</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f cert-manager.yaml</span><br><span class="line">kubectl apply -f wait-injection.yaml</span><br></pre></td></tr></table></figure><p>为需要进行注入的服务所在命名空间打上label</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl label namespace 业务namespace wait-pod-admission-webhook-injection=enabled</span><br></pre></td></tr></table></figure><p>利用kubernetes pod注入对运行的服务进行注入容器，注入多个init容器<br>需要在以下进行注解<br><font color="#FF0000" size="6" face="黑体">pod.spec.template.metadata.annotations</font></p><p>tcpsocket.waiting.huisebug.org/dep数字: ip:端口号或者域名:端口号<br>tcpsocket使用telnet对依赖服务和端口进行telnet</p><p>httpget.waiting.huisebug.org/dep数字: api-url|成功标识<br>httpget对依赖服务给的api url进行curl,使用|作为分隔符,|后面是api请求成功标志</p><p><font color="#FF0000" size="4" face="黑体">相同类型使用多个数字进行区分</font><br><font color="#FF0000" size="4" face="黑体">依赖必须全部满足才会进行业务container创建</font></p><p>例如:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">    version: v1</span><br><span class="line">  name: sidecar-test</span><br><span class="line">  namespace: sidecar-test</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">      version: v1</span><br><span class="line">  strategy:</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge: 1</span><br><span class="line">      maxUnavailable: 0</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      annotations:</span><br><span class="line">        tcpsocket.waiting.huisebug.org/dep1: www.google.com:443</span><br><span class="line">        tcpsocket.waiting.huisebug.org/dep2: www.google.com:8080</span><br><span class="line">        httpget.waiting.huisebug.org/dep1: https://www.google.com|google</span><br><span class="line">        httpget.waiting.huisebug.org/dep2: https://www.google.com|Google</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">        version: v1</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;在kubernetes集群中,服务之间存在依赖关系,wait-injection利用initcontainer功能快速便捷的为服务添加相应的依赖关系&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes-Dev" scheme="https://huisebug.github.io/categories/Kubernetes-Dev/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="kubebuilder" scheme="https://huisebug.github.io/tags/kubebuilder/"/>
    
    <category term="kubernetes" scheme="https://huisebug.github.io/tags/kubernetes/"/>
    
    <category term="inincontainer" scheme="https://huisebug.github.io/tags/inincontainer/"/>
    
    <category term="pod-injection" scheme="https://huisebug.github.io/tags/pod-injection/"/>
    
  </entry>
  
  <entry>
    <title>kube-deploymentimage:k8simage-operator改良版本</title>
    <link href="https://huisebug.github.io/2023/02/07/kube-deploymentimage/"/>
    <id>https://huisebug.github.io/2023/02/07/kube-deploymentimage/</id>
    <published>2023-02-07T03:04:01.000Z</published>
    <updated>2023-02-07T07:04:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>为kubernetes集群中，由deployment方式进行部署的服务提供image版本回滚功能</p><span id="more"></span><h1 id="需求场景"><a href="#需求场景" class="headerlink" title="需求场景"></a>需求场景</h1><ol><li><p> 当master节点上存放的yaml无法在使用kubect set image时进行同步更新</p></li><li><p> 官方提供的—record参数仅可能在kubectl set image时使用，回滚还需要额外的回滚操作命令，没有统一的UI界面</p></li></ol><h1 id="服务描述"><a href="#服务描述" class="headerlink" title="服务描述"></a>服务描述</h1><ol><li><p> 提供展示部署后所有命名空间下的Deployment的容器镜像信息</p></li><li><p> 提供回滚到对应版本的按钮update</p></li><li><p> 提供本地存储的yaml文件预览（鼠标滑动到对应的yaml路径即可）</p></li><li><p> 清理不需要的镜像版本信息delete（假删除，数据库中还有记录）</p></li><li><p> 修复k8simage-operator中Create,Update event多次触发Reconcile方法</p></li><li><p> 增加对容器列表增加和减少场景的处理</p></li></ol><h1 id="部署配置"><a href="#部署配置" class="headerlink" title="部署配置"></a>部署配置</h1><p>需要准备一个mysql5.7的数据库服务，需提前建立好数据库，项目数据库名为：kube-deploymentimage,可使用下面sql进行创建即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE DATABASE IF NOT EXISTS `kube-deploymentimage` DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br></pre></td></tr></table></figure><p>由部署文件中的变量MYSQL_HOST、MYSQL_USER、MYSQL_PASSWORD进行传递，例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">env:</span><br><span class="line">- name: MYSQL_HOST</span><br><span class="line">  value: **</span><br><span class="line">- name: MYSQL_USER</span><br><span class="line">  value: **</span><br><span class="line">- name: MYSQL_PASSWORD</span><br><span class="line">  value: **       </span><br></pre></td></tr></table></figure><p>如果需要开启密码认证,由部署文件中的变量auth.user、auth.password进行传递，如果不需要则去掉此两项配置，例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">env:     </span><br><span class="line">- name: auth.user</span><br><span class="line">  value: **</span><br><span class="line">- name: auth.password</span><br><span class="line">  value: **</span><br></pre></td></tr></table></figure><p>kube-deploymentimage默认会统计集群中所有的Deployment，yamlfile文件在部署kube-deploymentimage容器时，此处默认挂载了/etc/kubernetes目录到容器中的/etc/kubernetes，使用nodeselector默认调度到master1，即存放yaml的机器。不存在目录也不影响使用</p><p>想展示yaml的本地文件内容，需要在deployment中添加  annotations信息，例如：   </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: test-k8s-nginx</span><br><span class="line">  annotations:    </span><br><span class="line">    yamlfile.huisebug.io/yamlfile: /etc/kubernetes/test-k8s-nginx/test-k8s-nginx.yaml</span><br><span class="line">  name: test-k8s-nginx</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>yamlfile.huisebug.io/yamlfile是固定字段，后面是对应的yaml的文件路径（注意：填写挂载到kube-deploymentimage容器中的路径）</p><h1 id="UI效果预览"><a href="#UI效果预览" class="headerlink" title="UI效果预览"></a>UI效果预览</h1><p><img src="/2023/02/07/kube-deploymentimage/media/kube-deployment-ui.png"></p><h1 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h1><p><a href="https://github.com/huisebug/kube-deploymentimage.git">https://github.com/huisebug/kube-deploymentimage.git</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;为kubernetes集群中，由deployment方式进行部署的服务提供image版本回滚功能&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes-Dev" scheme="https://huisebug.github.io/categories/Kubernetes-Dev/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="kubebuilder" scheme="https://huisebug.github.io/tags/kubebuilder/"/>
    
    <category term="kubernetes" scheme="https://huisebug.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>k8s版本回滚工具k8simage-operator</title>
    <link href="https://huisebug.github.io/2022/06/16/k8simage-operator/"/>
    <id>https://huisebug.github.io/2022/06/16/k8simage-operator/</id>
    <published>2022-06-16T03:04:01.000Z</published>
    <updated>2022-06-16T03:04:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>为kubernetes集群部署提供版本回滚功能</p><span id="more"></span><h1 id="需求场景"><a href="#需求场景" class="headerlink" title="需求场景"></a>需求场景</h1><ol><li><p> 当master节点上存放的yaml无法在使用kubect set image时进行同步更新</p></li><li><p> 官方提供的—record参数仅可能在kubectl set image时使用，回滚还需要额外的回滚操作命令，没有统一的UI界面</p></li></ol><h1 id="功能描述"><a href="#功能描述" class="headerlink" title="功能描述"></a>功能描述</h1><ol><li><p> 提供展示部署后所有命名空间下的Deployment的容器镜像信息</p></li><li><p> 提供回滚到对应版本的按钮update</p></li><li><p> 提供本地存储的yaml文件预览（鼠标滑动到对应的yaml路径即可）</p></li><li><p> 清理不需要的镜像版本信息delete（假删除，数据库中还有记录）</p></li></ol><h1 id="UI效果预览"><a href="#UI效果预览" class="headerlink" title="UI效果预览"></a>UI效果预览</h1><p><img src="/2022/06/16/k8simage-operator/media/3a30ddbcf61f5ef4340432b7ff10f67a.png"></p><h1 id="部署要求"><a href="#部署要求" class="headerlink" title="部署要求"></a>部署要求</h1><p>K8simage-operator默认会统计集群中所有的Deployment，yamlfile文件在部署k8simage-operator容器时，此处默认挂载了/etc/kubernetes目录到容器中的/etc/kubernetes，使用nodeselector默认调度到master1，即存放yaml的机器。</p><p>想展示yaml的本地文件内容，需要在deployment中添加  annotations信息，例如：   </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">    version: v1</span><br><span class="line">  annotations:    </span><br><span class="line">    yamlfile.huisebug.io/yamlfile: /etc/kubernetes/nginx/test-k8s-nginx.yaml</span><br><span class="line">  name: nginxredis</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>yamlfile.huisebug.io/yamlfile是固定字段，后面是对应的yaml的文件路径（注意：挂载到k8simage-operator后路径是容器中的路径）</p><h1 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h1><p><a href="https://github.com/huisebug/k8simage-operator.git">https://github.com/huisebug/k8simage-operator.git</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;为kubernetes集群部署提供版本回滚功能&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes-Dev" scheme="https://huisebug.github.io/categories/Kubernetes-Dev/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="kubebuilder" scheme="https://huisebug.github.io/tags/kubebuilder/"/>
    
    <category term="kubernetes" scheme="https://huisebug.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>greenplum集群操作指令及解决方法</title>
    <link href="https://huisebug.github.io/2021/09/10/greenplum%E9%9B%86%E7%BE%A4%E6%93%8D%E4%BD%9C%E6%8C%87%E4%BB%A4%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"/>
    <id>https://huisebug.github.io/2021/09/10/greenplum%E9%9B%86%E7%BE%A4%E6%93%8D%E4%BD%9C%E6%8C%87%E4%BB%A4%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</id>
    <published>2021-09-10T02:04:01.000Z</published>
    <updated>2021-09-10T02:04:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>gp集群如何部署就不过多介绍，主要是结合一下工作中要用到的指令和遇到的一些坑</p><span id="more"></span><h1 id="操作规范"><a href="#操作规范" class="headerlink" title="操作规范"></a>操作规范</h1><p>gp集群的所有操作都是由master节点进行的,由master机器ssh到其他节点上进行自动化操作</p><p>安装部署gp集群时已经给master角色机器和standby角色机器添加了gpadmin用户.bashrc脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;&quot;&quot;</span><br><span class="line">source /usr/local/greenplum-db/greenplum_path.sh</span><br><span class="line">export MASTER_DATA_DIRECTORY=/data/master/gpseg-1</span><br><span class="line">&quot;&quot;&quot; &gt;&gt; /home/gpadmin/.bashrc</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>必须在gpadmin用户下进行,登录master机器后需要切换到gpadmin用户下</p><p>su – gpadmin</p><p>下面的所有操作都是在gpadmin用户下的操作</p><h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><p>恢复mirror segment (primary复制到mirror)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gprecoverseg -qo ./recov</span><br><span class="line">gprecoverseg -i ./recov</span><br></pre></td></tr></table></figure><p>恢复到原来初始化时的角色设置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gprecoverseg -r</span><br></pre></td></tr></table></figure><p>恢复primary segment (mirror复制到primary)</p><p>快速恢复镜像，可以多次执行，让集群中的mirror端口7000的数据同步到primary端口6000，可以用于segment节点宕机后重新启动，期间可能修复失败，多次执行修复即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gprecoverseg -av #不会创建primary数据目录</span><br></pre></td></tr></table></figure><p>强制恢复</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gprecoverseg -Fav #会创建primary数据目录，（相当于把mirror的整个文件目录给复制到primary目录）</span><br></pre></td></tr></table></figure><p>启动master节点和segment</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpstart -a</span><br></pre></td></tr></table></figure><p>显示具有镜像状态问题的片段，如果集群正常则显示为running，可用于健康检查</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpstate -e</span><br></pre></td></tr></table></figure><p>查看端口分配情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpstate -p</span><br></pre></td></tr></table></figure><p>查看集群中的角色</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpstate -b</span><br></pre></td></tr></table></figure><p>显示主镜像mirror映射</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpstate -c</span><br></pre></td></tr></table></figure><p>显示镜像实例同步状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpstate -m</span><br></pre></td></tr></table></figure><p>停止所有实例，然后重启系统</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpstop -M fast -ar</span><br></pre></td></tr></table></figure><p>停止集群，包含master和segment</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpstop -a</span><br></pre></td></tr></table></figure><p>激活standby为master，在standby机器上执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpactivatestandby -a</span><br></pre></td></tr></table></figure><h1 id="Master角色故障转移"><a href="#Master角色故障转移" class="headerlink" title="Master角色故障转移"></a>Master角色故障转移</h1><p>问题场景：</p><p>Master机器故障</p><p>修复方法：</p><p>master机器故障时，无法自动故障转移，需要将standby机器升级为master角色，原有的master机器会被踢出集群master角色，此时集群中只有master角色(standby机器)，没有standby角色。</p><p>操作命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">登录standby机器</span></span><br><span class="line">su – gpadmin</span><br><span class="line">gpactivatestandby -a</span><br></pre></td></tr></table></figure><p>要想master机器重新成为master角色，需要移除master机器的MASTER_DATA_DIRECTORY目录数据(rm -rf $MASTER_DATA_DIRECTORY)或者备份，然后在standby机器上操作命令：gpinitstandby -s master机器； 让master机器重新以standby角色加入集群：</p><p>操作命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">登录master机器</span></span><br><span class="line">rm -rf $MASTER_DATA_DIRECTORY</span><br><span class="line"><span class="meta">#</span><span class="bash">登录standby机器</span></span><br><span class="line">su – gpadmin</span><br><span class="line">gpinitstandby -s master机器主机名</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>操作完成后，master机器就是集群中的standby角色; 经过gpstate -b确认master机器成为了集群的standby角色，然后停止standby机器(pg_ctl stop -D $MASTER_DATA_DIRECTORY)，在master机器上执行gpactivatestandby -a命令，抢夺standby机器的master角色并踢出集群，当master机器重新成为master角色后，再移除standby机器的$MASTER_DATA_DIRECTORY目录数据<br>然后再master机器上执行命令：gpinitstandby -s standby机器。让其成为standby角色。<br>操作命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">登录standby机器</span></span><br><span class="line">su – gpadmin</span><br><span class="line">pg_ctl stop -D $MASTER_DATA_DIRECTORY</span><br><span class="line">rm -rf $MASTER_DATA_DIRECTORY</span><br><span class="line"><span class="meta">#</span><span class="bash">登录master机器</span></span><br><span class="line">su – gpadmin</span><br><span class="line">gpactivatestandby -a</span><br><span class="line">gpinitstandby -s standby机器主机名</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="Segment角色故障修复"><a href="#Segment角色故障修复" class="headerlink" title="Segment角色故障修复"></a>Segment角色故障修复</h1><p>问题场景</p><p>segment节点机器重启</p><p>修复方法：</p><p>gp集群自带segment故障转移，故障segment节点的primary业务转移其对应的mirror业务上（一般是其他的segment节点上），此时不影响集群运行。</p><p>gp提供了segment节点修复工具gprecoverseg</p><p>当整个集群正常状态下，primary提供数据读写，mirror提供备份功能，当primary异常时，mirror接替primary的工作。使用命令gpstate -c命令进行查看</p><p>例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Status                             Data State     Primary        Datadir           Port   Mirror         Datadir           Port</span><br><span class="line">Primary Active, Mirror Available   Synchronized   gp-mdw1-sdw1   /data/p1/gpseg0   6000   gp-mdw2-sdw2   /data/m1/gpseg0   7000</span><br><span class="line">Primary Active, Mirror Available   Synchronized   gp-mdw1-sdw1   /data/p2/gpseg1   6001   gp-sdw3        /data/m2/gpseg1   7001</span><br><span class="line">Primary Active, Mirror Available   Synchronized   gp-mdw2-sdw2   /data/p1/gpseg2   6000   gp-sdw3        /data/m1/gpseg2   7000</span><br><span class="line">Primary Active, Mirror Available   Synchronized   gp-mdw2-sdw2   /data/p2/gpseg3   6001   gp-mdw1-sdw1   /data/m2/gpseg3   7001</span><br><span class="line">Primary Active, Mirror Available   Synchronized   gp-sdw3        /data/p1/gpseg4   6000   gp-mdw1-sdw1   /data/m1/gpseg4   7000</span><br><span class="line">Primary Active, Mirror Available   Synchronized   gp-sdw3        /data/p2/gpseg5   6001   gp-mdw2-sdw2   /data/m2/gpseg5   7001</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>方法1：</p><p>如果发现上面的DataState不是Synchronized，可以执行自动修复命令gprecoverseg，这条命令会检测集群的运行情况，如果segment节点的服务未启动也会进行启动</p><p>操作命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">登录master机器</span></span><br><span class="line">su – gpadmin</span><br><span class="line">gprecoverseg -a   #此命令可以多次执行，可以加上-F 强制进行修复，直到gpsta -c查看到的state正常</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>方法2：:</p><p>查询集群中的问题节点，进行修复</p><p>操作命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">登录master机器</span></span><br><span class="line">su – gpadmin</span><br><span class="line">gprecoverseg -qo ./recov     #查询集群中需要修复的节点，将内容写入到文件recov</span><br><span class="line">gprecoverseg -i ./recov     #读取recov文件中需要修复的节点</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="集群无法启动"><a href="#集群无法启动" class="headerlink" title="集群无法启动"></a>集群无法启动</h1><p>如果在执行gpstart -a时集群时，因为segment的数据目录问题导致无法启动整个集群</p><p>那么就只启动master的维护模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpstart -m</span><br></pre></td></tr></table></figure><p>可以支持gpstate的命令查询，可以查询对应的primary和mirror目录（Mirror segment采用物理文件复制的方案—primary segment中数据文件I / O被复制到mirror segment上，因此mirror segment的文件与primary segment上的文件相同）<br>使用scp命令进行文件一致<br>备份segment节点上的文件夹，例如<br>将mirror的数据目录完整发送到primary的目录，要复制整个文件夹，不要创建目标文件夹后复制文件，不然会提示文件类型无法识别（postgresql的数据目录检测机制）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv /data/primary/gpseg2 /data/primary/gpseg2-bak</span><br><span class="line">scp -r 另一台segment机器:/data/mirror/gpseg2 /data/primary/gpseg2</span><br></pre></td></tr></table></figure><h1 id="为primary-master和standby-master配置一个虚拟IP地址"><a href="#为primary-master和standby-master配置一个虚拟IP地址" class="headerlink" title="为primary master和standby master配置一个虚拟IP地址"></a>为primary master和standby master配置一个虚拟IP地址</h1><p>可以利用keepalived产生一个VIP，利用keepalived的检测进行权重增减，实现VIP漂移</p><h1 id="备份还原"><a href="#备份还原" class="headerlink" title="备份还原"></a><strong>备份还原</strong></h1><ol><li>Greenplum早期通过使用PG数据库的备份工具实现备份。但随着数据量不断增大，基于PG备份工具的串行备份模式无法满足用户对备份时效的需求。</li><li>Greenplum开始了第二代备份工具gp_dump的研发，并在2005年左右正式发布，gp_dump通过引入并行备份解决了串行备份时效低的问题，但使用上相对比较复杂。</li><li>为了进一步的完善了用户使用体验，官方基于gp_dump，开发了gpcrodump备份工具。gpcrodump非常的成功，推出以来，为用户服务了10多年，当前还有大量的用户在用（GPDB 4.22以前的版本建议的备份工具）。<br>但gpcrodump有一个非常大的限制，就是备份时长时间锁表，因此需要用户预留相应的时间窗口。随着用户数据量的持续提升，以及HTAP混合负载应用的需求，企业对在线联机备份（备份时不需要中断业务）的需求愈发强烈。</li><li>因此大概2015年，Pivotal开始研究新一代的备份工具，并在2017年正式发布，这就是新一代备份工具gpbackup。<br>gpbackup消除了锁表（专有锁）机制，同时，创新性的把GP的原有的外部表技术（一种高效的大规模并行数据加载和卸载技术）引入，用户无需为备份预留单独的时间窗口，同时扩展了备份存储的支持，<br>当前可用使用本地文件系统、Dell Data Domain、NBU、以及分布式存储，也可用使用公有云对象存储（S3），同时还支持用户自定义备份的存储接口。</li></ol><p>完整全量备份：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpbackup --dbname wyf --backup-dir /data/backup --leaf-partition-data</span><br></pre></td></tr></table></figure><p>创建增量备份,增量备份要确保之前的备份不呢丢失，不然无法还原：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpbackup --dbname wyf --backup-dir /data/backup --leaf-partition-data --incremental</span><br></pre></td></tr></table></figure><p>基于某个备份做增量备份，–from-timestamp 后面跟的是已经存在的备份时间戳（例如：/data/backup/gpseg-1/backups/备份日期/下查看）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpbackup --dbname wyf --backup-dir /mybackup --leaf-partition-data --incremental --from-timestamp 20210909145418</span><br></pre></td></tr></table></figure><p>恢复（不创建库）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gprestore --backup-dir /data/backup --timestamp 20210909152128</span><br></pre></td></tr></table></figure><p>恢复（创建库）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gprestore --backup-dir /data/backup --create-db --timestamp 20210909152128</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;gp集群如何部署就不过多介绍，主要是结合一下工作中要用到的指令和遇到的一些坑&lt;/p&gt;</summary>
    
    
    
    <category term="大数据" scheme="https://huisebug.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="greenplum" scheme="https://huisebug.github.io/tags/greenplum/"/>
    
  </entry>
  
  <entry>
    <title>kata容器的一些分享</title>
    <link href="https://huisebug.github.io/2021/07/06/kata%E5%AE%B9%E5%99%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%88%86%E4%BA%AB/"/>
    <id>https://huisebug.github.io/2021/07/06/kata%E5%AE%B9%E5%99%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%88%86%E4%BA%AB/</id>
    <published>2021-07-06T06:04:01.000Z</published>
    <updated>2021-07-07T09:49:20.673Z</updated>
    
    <content type="html"><![CDATA[<p>容器运行时kata的一些知识分享</p><span id="more"></span><h1 id="kata运行时的地位"><a href="#kata运行时的地位" class="headerlink" title="kata运行时的地位"></a>kata运行时的地位</h1><p>kata-runtime的地位等同于runc运行时，所以可以替换containerd使用的运行时，在k8s中通过调用containerd，containerd使用kata达到使用。</p><p>docker使用kata是声明默认的运行时，以达到containerd替换默认运行时为kata</p><h1 id="kata的一些功能无法实现"><a href="#kata的一些功能无法实现" class="headerlink" title="kata的一些功能无法实现"></a>kata的一些功能无法实现</h1><p>使用kata会无法使用docker network的自动发现功能。</p><p>但是IP地址是可以通信的</p><h1 id="kata最新版"><a href="#kata最新版" class="headerlink" title="kata最新版"></a>kata最新版</h1><p>最新版</p><p>Release 2.1.0</p><h1 id="kata在VMware-个人桌面的一些问题解决"><a href="#kata在VMware-个人桌面的一些问题解决" class="headerlink" title="kata在VMware 个人桌面的一些问题解决"></a>kata在VMware 个人桌面的一些问题解决</h1><p>错误：ERROR: System is not capable of running Kata Containers</p><p>解决方法：</p><p>VMware虚拟机设置，处理器–》虚拟化引擎</p><p>勾选 虚拟化Intel VT-x/EPT</p><p>错误：ERRO[0000] kernel property not found arch=amd64 description=”Host Support<br>for Linux VM Sockets” name=vhost_vsock pid=16727 source=runtime type=module</p><p>System is capable of running Kata Containers</p><p>System can currently create Kata Containers</p><p>解决方法：</p><p>是linux 检测到在 vmware 环境中运行时，会加载一些 vmware 的模块并使用 vsock<br>从而产生了冲突</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo tee /etc/modules-load.d/blacklist-vmware.conf &lt;&lt; EOF</span><br><span class="line"></span><br><span class="line">blacklist vmw_vsock_virtio_transport_common</span><br><span class="line"></span><br><span class="line">blacklist vmw_vsock_vmci_transport</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>然后重启linux</p><h1 id="kata命令"><a href="#kata命令" class="headerlink" title="kata命令"></a>kata命令</h1><p>kata-runtime help</p><p>kata检查</p><p>kata-runtime kata-check</p><p>v2额外支持</p><p>kata-runtime check</p><p>版本</p><p>kata-runtime -v</p><p>环境变量</p><p>kata-runtime kata-env</p><p>v2额外支持</p><p>kata-runtime env</p><h1 id="kata配置文件"><a href="#kata配置文件" class="headerlink" title="kata配置文件"></a>kata配置文件</h1><p>查看配置文件存放位置，默认位置是/usr/share/defaults/kata-containers/configuration.toml标准系统的位置<br>。但是，如果/etc/kata-containers/configuration.toml存在，则优先</p><p>命令：kata-runtime –kata-show-default-config-paths</p><p>/etc/kata-containers/configuration.toml</p><p>/usr/share/defaults/kata-containers/configuration.toml ###centos7存放位置</p><p>/usr/share/kata-containers/defaults/configuration.toml ###centos8存放位置</p><p>kata存在基于alpine系统下运行java会提示library initialization failed - unable to<br>allocate file descriptor table - out of memory</p><p>在Linux的最新版本中，打开文件数量的默认限制已大大增加。Java 8在尝试为此数量的文件描述符预先分配内存方面做了错误的事情（请参阅<a href="https://bugs.openjdk.java.net/browse/JDK-8150460%EF%BC%89%E3%80%82%E4%BB%A5%E5%89%8D%EF%BC%8C%E5%BD%93%E9%BB%98%E8%AE%A4%E9%99%90%E5%88%B6%E8%A6%81%E4%BD%8E%E5%BE%97%E5%A4%9A%E6%97%B6%EF%BC%8C%E6%AD%A4%E6%96%B9%E6%B3%95%E6%9C%89%E6%95%88%EF%BC%8C%E4%BD%86%E7%8E%B0%E5%9C%A8%EF%BC%8C%E5%AE%83%E5%B0%9D%E8%AF%95%E5%88%86%E9%85%8D%E8%BF%87%E5%A4%9A%E8%80%8C%E5%A4%B1%E8%B4%A5%E3%80%82%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%E6%98%AF%E8%AE%BE%E7%BD%AE%E6%89%93%E5%BC%80%E6%96%87%E4%BB%B6%E6%95%B0%E7%9A%84%E4%B8%8B%E9%99%90%EF%BC%88%E6%88%96%E4%BD%BF%E7%94%A8%E8%BE%83%E6%96%B0%E7%9A%84java%EF%BC%89%EF%BC%9A">https://bugs.openjdk.java.net/browse/JDK-8150460）。以前，当默认限制要低得多时，此方法有效，但现在，它尝试分配过多而失败。解决方法是设置打开文件数的下限（或使用较新的java）：</a></p><h1 id="各操作系统安装kata"><a href="#各操作系统安装kata" class="headerlink" title="各操作系统安装kata"></a>各操作系统安装kata</h1><h2 id="cnetos7-kata-v1"><a href="#cnetos7-kata-v1" class="headerlink" title="cnetos7 kata v1"></a>cnetos7 kata v1</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum-config-manager --add-repo</span><br><span class="line">http://download.opensuse.org/repositories/home:/katacontainers:/releases:/x86_64:/stable-1.11/CentOS_7/home:katacontainers:releases:x86_64:stable-1.11.repo</span><br></pre></td></tr></table></figure><h2 id="centos8-kata-v1"><a href="#centos8-kata-v1" class="headerlink" title="centos8 kata v1"></a>centos8 kata v1</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">tee /etc/yum.repos.d/advanced-virt.repo &lt;&lt;-&#x27;EOF&#x27;</span><br><span class="line">[advanced-virt]</span><br><span class="line">name=Advanced Virtualization</span><br><span class="line">baseurl=https://mirrors.huaweicloud.com/centos/$releasever/virt/$basearch/advanced-virtualization</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">skip_if_unavailable=1</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">tee /etc/yum.repos.d/kata-containers.repo &lt;&lt;-&#x27;EOF&#x27;</span><br><span class="line">[kata-containers]</span><br><span class="line">name=Kata Containers</span><br><span class="line">baseurl=https://mirrors.huaweicloud.com/centos/$releasever/virt/$basearch/kata-containers</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">skip_if_unavailable=1</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">dnf module disable -y virt:rhel</span><br><span class="line">dnf install -y kata-runtime </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="centos8-kata-v2"><a href="#centos8-kata-v2" class="headerlink" title="centos8 kata v2"></a>centos8 kata v2</h2><p>v1和v2的差异在于，安装kata-containers即会安装v2，安装kata-runtime即会安装v1</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sudo -E dnf install -y centos-release-advanced-virtualization</span><br><span class="line">sudo -E dnf module disable -y virt:rhel</span><br><span class="line">source /etc/os-release</span><br><span class="line">cat &lt;&lt;EOF | sudo -E tee /etc/yum.repos.d/kata-containers.repo</span><br><span class="line">[kata-containers]</span><br><span class="line">name=Kata Containers</span><br><span class="line">baseurl=http://mirror.centos.org/\$contentdir/\$releasever/virt/\$basearch/kata-containers</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">skip_if_unavailable=1</span><br><span class="line">EOF</span><br><span class="line">sudo -E dnf install -y kata-containers</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>由于Docker尚不直接支持容器化的shimv2架构，因此Kata 2.0.0不适用于Docker。</p><p>Kata 1.x运行时可与docker一起使用</p><h1 id="kata中使用alpine系统存在的java问题"><a href="#kata中使用alpine系统存在的java问题" class="headerlink" title="kata中使用alpine系统存在的java问题"></a>kata中使用alpine系统存在的java问题</h1><p>当前都是kata运行时，并且版本都是1.11+</p><p>经过我的测试是文件打开数太大导致java无法运行</p><p>当image是FROM alpine时无法运行，</p><p>当image是FROM slim时可以运行，</p><p>Centos8系统安装的docker，所有容器的文件打开数默认是1048576</p><p>Centos7系统安装的docker，所有容器的文件打开数默认是1073741816</p><p>并且docker版本都是最新的20.10.6</p><p>解决方法：</p><p>方法1，不使用基于alpine为基础image的docker镜像，很显然是当文件打开数太大，kata下的alpine会运行错误。很多服务都是以alpine运行的，这个方法不推荐</p><p>方法2，使用kata运行时，使用centos8系统运行docker，不使用centos7。这个是推荐方法</p><p>方法3，可以在docker系统服务文件加上dockerd启动参数</p><p>--default-ulimit nofile=65535:65535 设置所有容器的文件打开数</p><p>设置LimitNOFILE=65535是无效的，并不会更改docker所有容器的文件打开数。</p><h1 id="containerd下可以指定是否使用kata-runtime运行时"><a href="#containerd下可以指定是否使用kata-runtime运行时" class="headerlink" title="containerd下可以指定是否使用kata-runtime运行时"></a>containerd下可以指定是否使用kata-runtime运行时</h1><p>使用ctr命令行启动容器</p><p>要通过容器命令行使用Kata Containers运行容器，可以运行以下命令：</p><p>sudo ctr image pull docker.io/library/busybox:latest</p><p>sudo ctr run –runtime io.containerd.run.kata.v2 -t –rm<br>docker.io/library/busybox:latest hello sh</p><p>这将启动一个名为的BusyBox容器hello，–rm退出后将被删除。</p><h1 id="在k8s1-12-下利用RuntimeClass选择性使用kata"><a href="#在k8s1-12-下利用RuntimeClass选择性使用kata" class="headerlink" title="在k8s1.12+下利用RuntimeClass选择性使用kata"></a>在k8s1.12+下利用RuntimeClass选择性使用kata</h1><p>查看当前机器下containerd容器使用了什么运行时来运行容器</p><p>ctr -n k8s.io c ls</p><p>kata-runtime是隔断网络访问的，所以为了k8s组件可以正常运行，containerd默认的运行时不可以设置为kata，可以使用下面的2种方式指定运行时运行容器</p><p>containerd支持kata-runtimev2的接口，在配置untrusted_workload_runtime时，直接使用katav2的类型io.containerd.kata.v2，没有io.containerd.kata.v1这种类型</p><p>因为不存在 runtime_type = “io.containerd.kata.v1”，我们可以使用runtime_type =<br>“io.containerd.kata.v2”来调用kata-runtime，containerd并不关心运行的kata是v1还是v2</p><p>1、untrusted_workload_runtime 的方式</p><p>kata-v1</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.untrusted_workload_runtime]</span><br><span class="line">  runtime_type = &quot;io.containerd.runtime.v1.linux&quot;</span><br><span class="line">  runtime_engine = &quot;/usr/bin/kata-runtime&quot;</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.untrusted_workload_runtime]</span><br><span class="line">  runtime_type = &quot;io.containerd.kata.v2&quot;</span><br></pre></td></tr></table></figure><p>kata-v2</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.untrusted_workload_runtime]</span><br><span class="line">  runtime_type = &quot;io.containerd.kata.v2&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>k8s调用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-untrusted</span><br><span class="line">  annotations:</span><br><span class="line">    io.kubernetes.cri.untrusted-workload: &quot;true&quot;</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>2、RuntimeClass 方式</p><p>首先需要在containerd配置文件中增加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">kata-v2</span><br><span class="line">        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.kata]</span><br><span class="line">          runtime_type = &quot;io.containerd.kata.v2&quot;  </span><br><span class="line">          [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.kata.options]</span><br><span class="line">            SystemdCgroup = true</span><br><span class="line"></span><br><span class="line">kata-v1</span><br><span class="line">        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.kata]</span><br><span class="line">          runtime_type = &quot;io.containerd.kata.v2&quot;  </span><br><span class="line">          [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.kata.options]</span><br><span class="line">            SystemdCgroup = true</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>因为不存在 runtime_type = “io.containerd.kata.v1”，我们可以使用runtime_type =<br>“io.containerd.kata.v2”来调用kata-runtime，containerd并不关心运行的kata是v1还是v2</p><p>在 K8s 中创建 RuntimeClass</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: node.k8s.io/v1beta1  # RuntimeClass is defined in the node.k8s.io API group</span><br><span class="line">kind: RuntimeClass</span><br><span class="line">metadata:</span><br><span class="line">  name: kata  </span><br><span class="line">handler: kata  # 这里与containerd配置文件中的 [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.&#123;handler&#125;] 匹配</span><br><span class="line">创建pod</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: kata-nginx</span><br><span class="line">spec:</span><br><span class="line">  runtimeClassName: kata</span><br><span class="line">  containers:</span><br><span class="line">    - name: nginx</span><br><span class="line">      image: nginx</span><br><span class="line">      ports:</span><br><span class="line">      - containerPort: 80</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="查看kata建立的容器kata-v1命令"><a href="#查看kata建立的容器kata-v1命令" class="headerlink" title="查看kata建立的容器kata-v1命令"></a>查看kata建立的容器kata-v1命令</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kata-runtime list</span><br></pre></td></tr></table></figure><h1 id="docker指定kata"><a href="#docker指定kata" class="headerlink" title="docker指定kata"></a>docker指定kata</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --runtime=kata-runtime nginx:1.9</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;容器运行时kata的一些知识分享&lt;/p&gt;</summary>
    
    
    
    <category term="容器运行时" scheme="https://huisebug.github.io/categories/%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6/"/>
    
    
    <category term="containerd" scheme="https://huisebug.github.io/tags/containerd/"/>
    
    <category term="docker" scheme="https://huisebug.github.io/tags/docker/"/>
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="kata" scheme="https://huisebug.github.io/tags/kata/"/>
    
    <category term="runtime" scheme="https://huisebug.github.io/tags/runtime/"/>
    
  </entry>
  
  <entry>
    <title>containerd基础概念</title>
    <link href="https://huisebug.github.io/2021/04/19/containerd%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"/>
    <id>https://huisebug.github.io/2021/04/19/containerd%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/</id>
    <published>2021-04-19T06:04:01.000Z</published>
    <updated>2023-04-12T08:04:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>containerd的前世今生，与docker的关系、如何部署containerd、如何接入k8s中</p><span id="more"></span><h1 id="OCI"><a href="#OCI" class="headerlink" title="OCI"></a>OCI</h1><p>OCI（Open Container Initiative）开放容器倡议，OCI由docker以及其他容器行业领导者创建于2015年，目前主要有两个标准：容器运行时标准(runtime-spec)和容器镜像标准(image-spec)</p><p>这两个标准通过OCI runtime filesytem bundle的标准格式连接在一起,OCI镜像可以通过工具转换成bundle,然后 OCI 容器引擎能够识别这个bundle来运行容器</p><h1 id="CRI"><a href="#CRI" class="headerlink" title="CRI"></a>CRI</h1><p>CRI（Container Runtime Interface）容器运行时接口，CRI是kubernetes推出的一个标准,推出标准可见其在容器编排领域的地位</p><p>runc、containerd 等运行时都去支持此接口。</p><h1 id="runc"><a href="#runc" class="headerlink" title="runc"></a>runc</h1><p>runC的前身是docker的libcontainer项目,在libcontainer的基础上做了封装,捐赠给OCI的一个符合标准的runtime实现,docker引擎内部也是基于runC构建的</p><p>runC只做一件事情就是运行容器,提供创建和运行容器的CLI(command-line interface)工具, runC直接与容器所依赖的cgroup/namespace linux kernel等进行交互，</p><p>负责为容器配置cgroup/namespace等启动容器所需的环境，创建启动容器的相关进程</p><h1 id="shim"><a href="#shim" class="headerlink" title="shim"></a>shim</h1><p>中文意思 ： 楔子</p><p>例如： dockershim和containershim</p><h2 id="containerd-shim"><a href="#containerd-shim" class="headerlink" title="containerd-shim"></a>containerd-shim</h2><p>containerd-shim进程由containerd进程拉起,即containerd进程是containerd-shim的父进程,容器进程由containerd-shim进程拉起,</p><p>这样的优点比如升级,重启docker或者containerd 不会影响已经running的容器进程,而假如这个父进程就是containerd,那每次containerd挂掉或升级,整个宿主机上所有的容器都得退出了. 而引入了 containerd-shim 就规避了这个问题(当containerd 退出或重启时, shim 会 re-parent 到 systemd 这样的 1 号进程上)</p><h1 id="部署containerd服务"><a href="#部署containerd服务" class="headerlink" title="部署containerd服务"></a>部署containerd服务</h1><h2 id="安装runc"><a href="#安装runc" class="headerlink" title="安装runc"></a>安装runc</h2><p>yum -y install runc</p><p>如果未安装则会ctr run 容器时会提示如下：</p><p>ctr: OCI runtime create failed: unable to retrieve OCI runtime error (open /run/containerd/io.containerd.runtime.v2.task/default/nginx/log.json: no such file or directory): exec: “runc”: executable file not found in $PATH: unknown</p><h2 id="安装containerd"><a href="#安装containerd" class="headerlink" title="安装containerd"></a>安装containerd</h2><h3 id="二进制分发"><a href="#二进制分发" class="headerlink" title="二进制分发"></a>二进制分发</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf containerd-1.4.4-linux-amd64.tar.gz -C /usr/local/</span><br><span class="line">mkdir -p /etc/containerd</span><br><span class="line">containerd config default &gt; /etc/containerd/config.toml</span><br></pre></td></tr></table></figure><h3 id="创建系统服务"><a href="#创建系统服务" class="headerlink" title="创建系统服务"></a>创建系统服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF | sudo tee /usr/lib/systemd/system/containerd.service</span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description=containerd container runtime</span><br><span class="line">Documentation=https://containerd.io</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">ExecStartPre=/sbin/modprobe overlay</span><br><span class="line">ExecStart=/usr/local/bin/containerd</span><br><span class="line">Delegate=yes</span><br><span class="line">KillMode=process</span><br><span class="line">LimitNOFILE=1048576</span><br><span class="line"><span class="meta">#</span><span class="bash"> Having non-zero Limit*s causes performance problems due to accounting overhead</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">in</span> the kernel. We recommend using cgroups to <span class="keyword">do</span> container-local accounting.</span></span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br><span class="line">EOF</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="配置介绍："><a href="#配置介绍：" class="headerlink" title="配置介绍："></a>配置介绍：</h4><p>Delegate : 这个选项允许 Containerd 以及运行时自己管理自己创建的容器的 cgroups。</p><p>如果不设置这个选项，systemd 就会将进程移到自己的 cgroups 中，从而导致 Containerd 无法正确获取容器的资源使用情况。</p><p>KillMode : 这个选项用来处理 Containerd 进程被杀死的方式。</p><p>默认情况下，systemd 会在进程的 cgroup 中查找并杀死 Containerd 的所有子进程，这肯定不是我们想要的。KillMode字段可以设置的值如下。</p><p>我们需要将 KillMode 的值设置为 process，这样可以确保升级或重启 Containerd时不杀死现有的容器。</p><p>control-group（默认值）：当前控制组里面的所有子进程，都会被杀掉</p><p>process：只杀主进程</p><p>mixed：主进程将收到 SIGTERM 信号，子进程收到 SIGKILL 信号</p><p>none：没有进程会被杀掉，只是执行服务的 stop 命令。</p><h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable containerd</span><br><span class="line">systemctl start containerd</span><br><span class="line">systemctl status containerd</span><br></pre></td></tr></table></figure><h1 id="Container命令ctr-crictl的用法"><a href="#Container命令ctr-crictl的用法" class="headerlink" title="Container命令ctr,crictl的用法"></a>Container命令ctr,crictl的用法</h1><p>containerd 相比于docker , 多了namespace概念, 每个image和container<br>都会在各自的namespace下可见, 目前k8s会使用k8s.io 作为命名空间</p><h2 id="image"><a href="#image" class="headerlink" title="image"></a>image</h2><h3 id="查看ctr-image"><a href="#查看ctr-image" class="headerlink" title="查看ctr image"></a>查看ctr image</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ctr image list</span><br><span class="line">ctr i list</span><br><span class="line">ctr i ls</span><br></pre></td></tr></table></figure><h3 id="镜像标记tag"><a href="#镜像标记tag" class="headerlink" title="镜像标记tag"></a>镜像标记tag</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io i tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2 k8s.gcr.io/pause:3.2</span><br></pre></td></tr></table></figure><p>注意: 若新镜像reference 已存在, 需要先删除新reference, 或者如下方式强制替换</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io i tag --force registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2 k8s.gcr.io/pause:3.2</span><br></pre></td></tr></table></figure><h3 id="删除镜像"><a href="#删除镜像" class="headerlink" title="删除镜像"></a>删除镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io i rm k8s.gcr.io/pause:3.2</span><br></pre></td></tr></table></figure><h3 id="拉取镜像"><a href="#拉取镜像" class="headerlink" title="拉取镜像"></a>拉取镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io i pull -k k8s.gcr.io/pause:3.2</span><br></pre></td></tr></table></figure><h3 id="推送镜像"><a href="#推送镜像" class="headerlink" title="推送镜像"></a>推送镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io i push -k k8s.gcr.io/pause:3.2</span><br></pre></td></tr></table></figure><h3 id="导出镜像"><a href="#导出镜像" class="headerlink" title="导出镜像"></a>导出镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io i export pause.tar k8s.gcr.io/pause:3.2</span><br></pre></td></tr></table></figure><h3 id="导入镜像"><a href="#导入镜像" class="headerlink" title="导入镜像"></a>导入镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io i import pause.tar</span><br></pre></td></tr></table></figure><h3 id="不支持-build-commit-镜像"><a href="#不支持-build-commit-镜像" class="headerlink" title="不支持 build,commit 镜像"></a>不支持 build,commit 镜像</h3><h2 id="运行容器"><a href="#运行容器" class="headerlink" title="运行容器"></a>运行容器</h2><p>例子:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io run --null-io --net-host -d \</span><br><span class="line">–env PASSWORD=$drone_password \</span><br><span class="line">–mount type=bind,src=/etc,dst=/host-etc,options=rbind:rw \</span><br><span class="line">–mount type=bind,src=/root/.kube,dst=/root/.kube,options=rbind:rw \</span><br><span class="line"><span class="meta">$</span><span class="bash">image sysreport bash /sysreport/run.sh</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--null-io: 将容器内标准输出重定向到/dev/null</span><br><span class="line">--net-host: 主机网络</span><br></pre></td></tr></table></figure><p>-d:<br>当task执行后就进行下一步shell命令,如没有选项,则会等待用户输入,并定向到容器内建立容器,ctr run 必须提供容器ID</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr c delete down</span><br></pre></td></tr></table></figure><p>ctr run 镜像全名称 容器名称 命令 （相对于docker的建立：docker run –name=容器名称 镜像名 命令）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例如：ctr run -t --rm docker.io/huisebug/down:latest down ls</span><br></pre></td></tr></table></figure><p>建立容器，同时运行了task，并后台运行-d</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr run -d docker.io/library/nginx:latest nginx</span><br></pre></td></tr></table></figure><p><font color="#FF0000" size="7" face="黑体">注意点</font><br>ctr run 时,最后如果传递了命令,此命令会覆盖Dockerfile中 ENTRYPOINT 和 CMD; 最后传递命令不受影响,和docker run是有区别的<br>Dockerfile如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine</span><br><span class="line">WORKDIR /etc</span><br><span class="line">ENTRYPOINT [&quot;ls&quot;]</span><br><span class="line">CMD [&quot;/&quot;]</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr run --rm --net-host docker.io/huisebug/containerd-entrypoint:v1 containerd-entrypoint ls</span><br></pre></td></tr></table></figure><p>上面执行结果为:列出/etc目录下所有文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr run --rm --net-host docker.io/huisebug/containerd-entrypoint:v1 containerd-entrypoint pwd</span><br></pre></td></tr></table></figure><p>上面执行结果为:获取当前工作目录，因在Dockerfile中声明了WORKDIR，所以结果为/etc</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr run --rm --net-host docker.io/huisebug/containerd-entrypoint:v1 containerd-entrypoint</span><br></pre></td></tr></table></figure><p>上面执行结果为:列出/根目录下所有文件</p><h3 id="过程总结"><a href="#过程总结" class="headerlink" title="过程总结"></a>过程总结</h3><p>容器container是分配和附加资源的元数据对象。其实就是相当于docker create，此时容器并未运行<br>任务task是系统上正在运行的实时过程。每次运行ctr t start会启动容器<br>（相当于docker start）后都应删除任务ctr t kill （相当于docker stop），而容器可以多次使用，更新和查询。<br>一般我们都是直接docker run 就是相当于执行了ctr run，就是执行了ctr c create 创建容器后马上执行ctr t start<br>如果ctr run （docker run）时没有加上-d参数后台运行，执行后会在终端进行输出。中断终端后task结束<br>命令为ctr t ls 相当于使用docker ps -a（会列出所有的容器）<br>docker rm就是相当于执行了ctr t rm和ctr c rm删除task和容器，一般删除容器，对应的task也会删除</p><h2 id="进入容器"><a href="#进入容器" class="headerlink" title="进入容器"></a>进入容器</h2><p>进入容器–exec-id 0必须填写，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr t exec --exec-id 0 -t nginx sh</span><br></pre></td></tr></table></figure><h2 id="停止容器"><a href="#停止容器" class="headerlink" title="停止容器,"></a>停止容器,</h2><p>需要先停止容器内的task, 再删除容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io tasks kill -a -s 9 &#123;id&#125;</span><br><span class="line">ctr -n k8s.io c rm &#123;id&#125;</span><br></pre></td></tr></table></figure><h2 id="容器日志"><a href="#容器日志" class="headerlink" title="容器日志"></a>容器日志</h2><p>注意: 容器默认使用fifo创建日志文件,如果不读取日志文件,会因为fifo容量导致业务运行阻塞</p><p>如要创建日志文件,建议如下方式创建:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr -n k8s.io run --log-uri file:///var/log/xx.log …</span><br></pre></td></tr></table></figure><h1 id="ctr命令与docker命令"><a href="#ctr命令与docker命令" class="headerlink" title="ctr命令与docker命令"></a>ctr命令与docker命令</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctr image ls</span><br><span class="line">docker images</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctr image pull</span><br><span class="line">docker pull</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctr image tag</span><br><span class="line">docker tag</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctr image push</span><br><span class="line">docker push</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctr image import 不支持压缩</span><br><span class="line">docker load &lt;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctr run 镜像名 容器名称</span><br><span class="line">docker run --name=容器名称 镜像名</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctr task ls 会显示stop的task和running的task</span><br><span class="line">docker ps -a</span><br></pre></td></tr></table></figure><h1 id="crictl用法"><a href="#crictl用法" class="headerlink" title="crictl用法"></a>crictl用法</h1><p>crictl 工具 是为k8s使用containerd而制作的, 其他非k8s的创建的crictl是无法看到和调试的, 也就是说用ctr run 运行的容器无法使用crictl 看到</p><p>crictl 使用命名空间 k8s.io.</p><p>cri plugin区别对待pod和container</p><p>ps: 列出在k8s.io 命名空间下的业务容器</p><p>pods: 列出在k8s.io 命名空间下的sandbox容器,在k8s里,通常是pause容器</p><p>logs: 打印业务容器日志</p><p>create: 创建容器,这里需要先创建sandbox,获取sandbox容器的id后,再用此id创建业务容器</p><p>inspect: 列出业务容器状态</p><p>inspectp: 列出sandbox容器状态</p><p>crictl 是cri的控制命令<br>因为暂时还未完全舍弃docker，所以docker.sock是排在第一位的，指定使用containerd.sock</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crictl -r unix:///run/containerd/containerd.sock  images</span><br></pre></td></tr></table></figure><p>ctr指定命名空间必须跟在ctr命令后面</p><p>crictl所做的操作是在ctr 的 k8s.io命名空间下<br>例如镜像， crictl -r unix:///run/containerd/containerd.sock images 和 ctr -n k8s.io i ls<br>docker所做的操作是在ctr的moby命名空间下，例如docker正在运行的容器，docker ps 和 ctr -n moby t ls，镜像不是一致的<br>crictl使用镜像docker.io仓库的镜像不用写全名</p><p>containerd配置文件/etc/containerd/config.toml所做的修改是给k8s（或者说是crictl命令使用）使用，可以利用下面的命令看到当前containerd基于k8s的配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crictl -r unix:///run/containerd/containerd.sock info | jq</span><br></pre></td></tr></table></figure><p>清理k8s集群中未使用的镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crictl -r unix:///run/containerd/containerd.sock rmi  --prune </span><br></pre></td></tr></table></figure><h1 id="docker在containerd的命名空间"><a href="#docker在containerd的命名空间" class="headerlink" title="docker在containerd的命名空间"></a>docker在containerd的命名空间</h1><p>除了k8s有命名空间以外，containerd也支持命名空间。docker创建的默认的都在moby空间，而k8s默认是k8s.io这个空间下面，不同空间的容器互相隔离。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr -n moby c ls</span><br></pre></td></tr></table></figure><p>上面只会显示正在运行的docker容器，没有运行的不会显示</p><h1 id="拉取dockerhub镜像"><a href="#拉取dockerhub镜像" class="headerlink" title="拉取dockerhub镜像"></a>拉取dockerhub镜像</h1><p>containerd要求必须先pull镜像，并且镜像需要写全镜像信息，包含仓库地址、用户、镜像tag</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例如： ctr i pull docker.io/huisebug/down:latest</span><br></pre></td></tr></table></figure><p>docker官方维护的镜像，用户名是library，比如nginx、redis</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctr i pull docker.io/library/nginx:latest</span><br></pre></td></tr></table></figure><h1 id="login自建仓库"><a href="#login自建仓库" class="headerlink" title="login自建仓库"></a>login自建仓库</h1><p>ctr 不支持http,必须使用https, 所有需要添加 –plain-http 参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctr images pull --plain-http --user admin:123456 192.168.137.101:5000/debug/registry:latest</span><br><span class="line">ctr images pull --plain-http 192.168.137.101:5000/debug/registry:latest</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;containerd的前世今生，与docker的关系、如何部署containerd、如何接入k8s中&lt;/p&gt;</summary>
    
    
    
    <category term="容器运行时" scheme="https://huisebug.github.io/categories/%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6/"/>
    
    
    <category term="containerd" scheme="https://huisebug.github.io/tags/containerd/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes集群服务部署yaml</title>
    <link href="https://huisebug.github.io/2020/12/15/k8s%E9%AB%98%E7%89%88%E6%9C%AC%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2yaml/"/>
    <id>https://huisebug.github.io/2020/12/15/k8s%E9%AB%98%E7%89%88%E6%9C%AC%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2yaml/</id>
    <published>2020-12-15T09:09:01.000Z</published>
    <updated>2024-06-06T06:51:10.561Z</updated>
    
    <content type="html"><![CDATA[<p>包含常见的中间件、数据库、服务工具等的部署yaml<br>nfs动态pv、<br>metrics-server、<br>rabbitmq集群、<br>mongodb集群、<br>elasticsearch集群、<br>kafka集群、<br>zookeeper集群、<br>ingress-nginx、<br>ingress-traefik、<br>rocketmq集群–双主双从模式、<br>kube-prometheus-thanos、<br>clickhouse集群、<br>vector+clickhouse、<br>redis集群 </p><span id="more"></span><script type="text/javascript">  window.location.href = '/download/';</script>]]></content>
    
    
    <summary type="html">&lt;p&gt;包含常见的中间件、数据库、服务工具等的部署yaml&lt;br&gt;nfs动态pv、&lt;br&gt;metrics-server、&lt;br&gt;rabbitmq集群、&lt;br&gt;mongodb集群、&lt;br&gt;elasticsearch集群、&lt;br&gt;kafka集群、&lt;br&gt;zookeeper集群、&lt;br&gt;ingress-nginx、&lt;br&gt;ingress-traefik、&lt;br&gt;rocketmq集群–双主双从模式、&lt;br&gt;kube-prometheus-thanos、&lt;br&gt;clickhouse集群、&lt;br&gt;vector+clickhouse、&lt;br&gt;redis集群 &lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes" scheme="https://huisebug.github.io/categories/Kubernetes/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Prometheus-Operator告警场景</title>
    <link href="https://huisebug.github.io/2020/12/15/prometheus-operator%E5%91%8A%E8%AD%A6%E5%9C%BA%E6%99%AF/"/>
    <id>https://huisebug.github.io/2020/12/15/prometheus-operator%E5%91%8A%E8%AD%A6%E5%9C%BA%E6%99%AF/</id>
    <published>2020-12-15T09:00:01.000Z</published>
    <updated>2021-07-07T08:49:41.212Z</updated>
    
    <content type="html"><![CDATA[<p>利用Prometheus-Operator来监控k8s集群并进行告警</p><span id="more"></span><h1 id="prometheus-operator告警场景"><a href="#prometheus-operator告警场景" class="headerlink" title="prometheus-operator告警场景"></a>prometheus-operator告警场景</h1><h1 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h1><p>准备环境k8s1.12+以上，k8s的部署就不过多介绍，有kube-admin，也有二进制部署，当然也可以参考我的<a href="https://huisebug.github.io/2019/08/24/k8s/">k8s集群部署</a></p><p>基本上环境准备好了就如我的集群如下</p><p><img src="/2020/12/15/prometheus-operator%E5%91%8A%E8%AD%A6%E5%9C%BA%E6%99%AF/media/7738d57508ef6a61185e1e3b1f5adf0f.png"></p><p>各组件已经成功安装。</p><h1 id="安装Prometheus-Operator"><a href="#安装Prometheus-Operator" class="headerlink" title="安装Prometheus-Operator"></a>安装Prometheus-Operator</h1><p>Prometheus-Operator原理介绍：参考我的<a href="https://huisebug.github.io/2019/08/27/Prometheus-Operator/">Prometheus-Operator监控k8s</a></p><p>此处演示的Prometheus-Operator场景的安装yaml，下载地址：<a href="https://github.com/huisebug/Prometheus-Operator-Rules.git">https://github.com/huisebug/Prometheus-Operator-Rules.git</a></p><h2 id="manifests目录介绍"><a href="#manifests目录介绍" class="headerlink" title="manifests目录介绍"></a>manifests目录介绍</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f manifests/</span><br></pre></td></tr></table></figure><p>多次执行上面的初始化安装确保prometheus-operator可用</p><p>注：<font color="red" size="3">上面的yaml是官方4月份发布的版本，经过多次测试可以适用于多个集群，其中修改了一些镜像地址为国内可快速拉取的地址和移除了prometheus-adapter的部署。</font></p><h2 id="new目录介绍"><a href="#new目录介绍" class="headerlink" title="new目录介绍"></a>new目录介绍</h2><p>alertmanager-alertmanager.yaml:增加告警模板自定义位置的配置</p><p>alertmanager-secret.yaml:alertmanager服务的配置文件，包含企业微告警，印制告警，告警模板，告警间隔时间。</p><p>alertmanager-temp-configmap.yaml:告警模板</p><p>auth:访问prometheus、alertmanager、grafana的http认证文件</p><p>ingress.yaml:访问prometheus、alertmanager、grafana的域名配置，并增加http密码校验</p><p>prometheus-prometheus.yaml:增加prometheus数据保留时间，抓取metrics的间隔时间</p><p>state-rules.yaml:告警规则，也是本文章的主要介绍内容。</p><h2 id="执行安装"><a href="#执行安装" class="headerlink" title="执行安装"></a>执行安装</h2><p>将auth认证文件传递到secret</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl -n monitoring create secret generic basic-auth --from-file=. /new/auth</span><br></pre></td></tr></table></figure><p>应用更改后的配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f new/</span><br></pre></td></tr></table></figure><h1 id="告警模板"><a href="#告警模板" class="headerlink" title="告警模板"></a>告警模板</h1><p>在之前的文章中写过<a href="https://huisebug.github.io/2019/08/27/Prometheus-Operator/#%E5%BB%BA%E7%AB%8B%E8%87%AA%E5%AE%9A%E4%B9%89%E5%91%8A%E8%AD%A6%E6%A8%A1%E6%9D%BF">自定义告警模板</a>，后续进行了模板的不断完善，告警信息我认为不必要的信息没必要展示，精确告警，新的告警模板如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#123;- define &quot;__text_alert_list&quot; -&#125;&#125;</span><br><span class="line">&#123;&#123;- range .Alerts.Firing -&#125;&#125;</span><br><span class="line">错误告警:</span><br><span class="line">告警消息: &#123;&#123; .Annotations.message &#125;&#125;</span><br><span class="line">告警级别: &#123;&#123; .Labels.severity &#125;&#125;</span><br><span class="line">告警alert: &#123;&#123; .Labels.alertname &#125;&#125;</span><br><span class="line">触发时间: &#123;&#123; (.StartsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125;</span><br><span class="line">&#123;&#123; end &#125;&#125;   </span><br><span class="line">&#123;&#123;- range .Alerts.Resolved -&#125;&#125;</span><br><span class="line">服务已恢复:</span><br><span class="line">原告警消息: &#123;&#123; .Annotations.message &#125;&#125;</span><br><span class="line">恢复时间: &#123;&#123; (.EndsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125;</span><br><span class="line">&#123;&#123; end &#125;&#125;</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;&#123;- define &quot;wechat.default.message&quot; -&#125;&#125;</span><br><span class="line">&#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;</span><br><span class="line">&#123;&#123; template &quot;__text_alert_list&quot; . &#125;&#125;</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br><span class="line">&#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;</span><br><span class="line">&#123;&#123; template &quot;__text_alert_list&quot; . &#125;&#125;</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br></pre></td></tr></table></figure><p>将告警信息精简为从声明annotations中取值自定义的键值对，键名为messages一条即可，根据不同的rules定义每一个rule的messages告警内容</p><h1 id="Rules"><a href="#Rules" class="headerlink" title="Rules"></a>Rules</h1><h2 id="已监听job状态"><a href="#已监听job状态" class="headerlink" title="已监听job状态"></a>已监听job状态</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">- name: up</span><br><span class="line">  rules:</span><br><span class="line">  - alert: 已监听job状态</span><br><span class="line">    expr: up == 0</span><br><span class="line">    for: 30s</span><br><span class="line">    labels:</span><br><span class="line">      severity: Erroring</span><br><span class="line">    annotations:</span><br><span class="line">      message: &quot;&#123;&#123;$labels.job&#125;&#125;服务所在实例&#123;&#123;$labels.instance&#125;&#125;关闭&quot;</span><br></pre></td></tr></table></figure><p>监控prometheus的target下的所有服务</p><h2 id="pod等待状态错误原因"><a href="#pod等待状态错误原因" class="headerlink" title="pod等待状态错误原因"></a>pod等待状态错误原因</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">- name: kube_pod_container_status_waiting_reason</span><br><span class="line">  rules:</span><br><span class="line">  - alert: pod等待状态错误原因</span><br><span class="line">    expr: kube_pod_container_status_waiting_reason&#123;reason=~&quot;ErrImagePull|CrashLoopBackOff|ImagePullBackOff&quot;&#125; &gt; 0</span><br><span class="line">    for: 30s</span><br><span class="line">    labels:</span><br><span class="line">      severity: critical</span><br><span class="line">      reason: waiting</span><br><span class="line">    annotations:</span><br><span class="line">      message: &quot;&#123;&#123;$labels.namespace&#125;&#125;.&#123;&#123;$labels.pod&#125;&#125;等待错误原因:&#123;&#123;$labels.reason&#125;&#125;&quot;</span><br></pre></td></tr></table></figure><p>监控pod非running状态的原因进行告警</p><h2 id="pod容器内存不足"><a href="#pod容器内存不足" class="headerlink" title="pod容器内存不足"></a>pod容器内存不足</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">- name: kube_pod_container_status_last_terminated_reason</span><br><span class="line">  rules:</span><br><span class="line">  - alert: pod容器内存不足</span><br><span class="line">    expr: kube_pod_container_status_last_terminated_reason&#123;reason=~&quot;OOMKilled&quot;&#125; &gt; 0</span><br><span class="line">    for: 3s</span><br><span class="line">    labels:</span><br><span class="line">      severity: critical</span><br><span class="line">      reason: terminated</span><br><span class="line">    annotations:</span><br><span class="line">      message: &quot;&#123;&#123;$labels.namespace&#125;&#125;.&#123;&#123;$labels.pod&#125;&#125;容器内存不足&quot;</span><br></pre></td></tr></table></figure><p>如果pod设置了resource内存限制，当pod超过内存被kill的时候进行告警，提醒使用者是否该调整该pod的resource</p><h2 id="pod状态有变动"><a href="#pod状态有变动" class="headerlink" title="pod状态有变动"></a>pod状态有变动</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">- name: kube_pod_container_status_ready</span><br><span class="line">  rules:</span><br><span class="line">  - alert: pod状态有变动</span><br><span class="line">    expr: kube_pod_container_status_ready != 1</span><br><span class="line">    for: 0s</span><br><span class="line">    labels:</span><br><span class="line">      severity: critical</span><br><span class="line">      reason: waiting</span><br><span class="line">    annotations:</span><br><span class="line">      message: &quot;&#123;&#123;$labels.namespace&#125;&#125;.&#123;&#123;$labels.pod&#125;&#125;容器&#123;&#123;$labels.container&#125;&#125;状态有变动&quot;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;利用Prometheus-Operator来监控k8s集群并进行告警&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes" scheme="https://huisebug.github.io/categories/Kubernetes/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="prometheus" scheme="https://huisebug.github.io/tags/prometheus/"/>
    
    <category term="prometheus-operator" scheme="https://huisebug.github.io/tags/prometheus-operator/"/>
    
    <category term="alertmanager" scheme="https://huisebug.github.io/tags/alertmanager/"/>
    
    <category term="grafana" scheme="https://huisebug.github.io/tags/grafana/"/>
    
    <category term="企业微信" scheme="https://huisebug.github.io/tags/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes集群搭建</title>
    <link href="https://huisebug.github.io/2020/08/26/k8s/"/>
    <id>https://huisebug.github.io/2020/08/26/k8s/</id>
    <published>2020-08-26T05:04:01.000Z</published>
    <updated>2023-04-20T05:40:06.665Z</updated>
    
    <content type="html"><![CDATA[<p>这是一个k8s集群，从零开始搭建服务器环境，keepalived+haproxy实现集群高可用VIP；glusterfs搭建实现可持续存储；k8s1.12版本集群；efk日志系统；prometheus-operator告警系统；HPA v2横向pod扩容；k8s1.11+以上的版本部署基本没什么区别，无非就是优化了一些参数的配置和增加一些功能，舍弃一些api，此文档适用后续发布的其他版本的部署，提供一些部署思路。</p><span id="more"></span><h1 id="硬件环境准备"><a href="#硬件环境准备" class="headerlink" title="硬件环境准备"></a>硬件环境准备</h1><ul><li>  服务器三台centos7.6</li><li>  每台服务器两块硬盘，一块作为服务器硬盘，一块作为glusterFS硬盘</li><li>  关闭firewalld防火墙和selinux</li><li>  IP地址：192.168.137.10—12/24；主机名api、node1、node2.huisebug.com</li><li>  使用阿里云yum源</li><li>  下面的操作是之前使用k8s1.12.4搭建的，其中的参数同样适用于k8s.12.*,因为hpa的原因替换为了k8s1.11.8。</li><li>  k8s版本1.11.8（2019年3月1号发布）；</li><li>  hpa在如下版本会会存在hpa无法获取内存的使用情况的bug，所以不推荐使用这些版本，其他版本未测试。如果你在使用了1.11.*版本后切换到了如下版本，是因为数据在etcd还没更新，一段时间后就会提示获取不到<blockquote><p>  k8s1.13.3<br>  k8s1.12.4<br>  k8s1.13.4<br>  k8s1.12.5<br>  k8s1.12.6</p></blockquote></li><li>  总结出k8s1.12+版本的HPA是无法获取内存使用情况，进一步跟进官方，k8s1.11.*版本是不支持autoscaling/v2beta2.</li></ul><h1 id="进行系统配置并开启IPVS"><a href="#进行系统配置并开启IPVS" class="headerlink" title="进行系统配置并开启IPVS"></a>进行系统配置并开启IPVS</h1><ul><li>  关闭防火墙、selinux</li><li>  关闭系统的Swap，Kubernetes 1.8开始要求。</li><li>  关闭linux swap空间的swappiness</li><li>  配置L2网桥在转发包时会被iptables的FORWARD规则所过滤，该配置被CNI插件需要，更多信息请参考<a href="https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/#network-plugin-requirements">NetworkPluginRequirements</a></li><li>  开启IPVS，将会后续应用到kube-proxy中去</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 所有主机：基本系统配置</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 关闭Selinux/firewalld</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config</span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 关闭交换分区</span></span><br><span class="line">swapoff -a</span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 增加个谷歌dns，方便访问外网</span></span><br><span class="line">echo &quot;nameserver 8.8.8.8&quot; &gt;&gt; /etc/resolv.conf</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置网桥包经IPTables，core文件生成路径</span></span><br><span class="line">echo &quot;&quot;&quot;</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">vm.overcommit_memory=1</span><br><span class="line">vm.panic_on_oom=0</span><br><span class="line">&quot;&quot;&quot; &gt; /etc/sysctl.conf</span><br><span class="line">modprobe br_netfilter</span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure><p>从Linux内核3.18-rc1开始，你必须使用modprobe br_netfilter来启用bridge-netfilter。，不然会提示以下错误<br><img src="/2020/08/26/k8s/media/10033b726854f2231957b8048571e073.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 所有主机：基本系统配置</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 同步时间</span></span><br><span class="line">yum install -y ntpdate</span><br><span class="line">ntpdate -u ntp.api.bz</span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装内核组件</span></span><br><span class="line">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm ;yum --enablerepo=elrepo-kernel install kernel-lt-devel kernel-lt -y</span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查默认内核版本高于4.1，否则请调整默认启动参数</span></span><br><span class="line">grub2-editenv list</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 确认内核版本</span></span><br><span class="line">uname -a</span><br><span class="line">注解：一般centos7的内核是3.10.当你升级后重启服务器，显示的内核还是之前的，因为你没切换到新内核，需要在主机启动页面选择，如果是云服务器，是看不到选择界面的，这时候就需要将新安装的内核设定为操作系统的默认内核，或者说如何将新版本的内核设置为重启后的默认内核？ </span><br><span class="line">仅需两步，之后重启即可。</span><br><span class="line">grub2-set-default 0</span><br><span class="line">grub2-mkconfig -o /etc/grub2.cfg</span><br><span class="line">reboot</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 重启后再次执行以下命令，确认内核版本</span></span><br><span class="line">uname –a</span><br><span class="line"><span class="meta">#</span><span class="bash"> 确认内核高于4.1后，开启IPVS</span></span><br><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">ipvs_modules=&quot;ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4&quot;</span><br><span class="line">for kernel_module in \$&#123;ipvs_modules&#125;; do</span><br><span class="line"> /sbin/modinfo -F filename \$&#123;kernel_module&#125; &gt; /dev/null 2&gt;&amp;1</span><br><span class="line"> if [ \$? -eq 0 ]; then</span><br><span class="line"> /sbin/modprobe \$&#123;kernel_module&#125;</span><br><span class="line"> fi</span><br><span class="line">done</span><br><span class="line">EOF</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 授权并执行</span></span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs</span><br></pre></td></tr></table></figure><p>Kubernetes要求集群中所有机器具有不同的Mac地址、产品uuid、Hostname。可以使用如下命令查看Mac和uuid</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 所有主机：检查UUID和Mac</span></span><br><span class="line">cat /sys/class/dmi/id/product_uuid</span><br><span class="line">ip link</span><br></pre></td></tr></table></figure><h1 id="GlusterFS"><a href="#GlusterFS" class="headerlink" title="GlusterFS"></a>GlusterFS</h1><p>搭建glusterfs来作为可持续存储k8s的CSI（可以跳过不部署）</p><h2 id="安装glusterfs"><a href="#安装glusterfs" class="headerlink" title="安装glusterfs"></a>安装glusterfs</h2><p><font color="red" size="3">三台服务器都要执行</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 先安装 gluster 源</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> yum install centos-release-gluster -y</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装 glusterfs 组件（这里包含了server和client）</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建 glusterfs服务运行目录</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir /opt/glusterd</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改 glusterd 目录，将/var/lib改成/opt</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sed -i &amp;<span class="comment">#39;s/var/lib/opt/g&amp;#39; /etc/glusterfs/glusterd.vol</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动 glusterfs（为挂载提供服务）</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl start glusterd.service</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置开机启动</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl <span class="built_in">enable</span> glusterd.service</span></span><br><span class="line"><span class="meta">#</span><span class="bash">查看状态</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl status glusterd.service</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="配置-glusterfs"><a href="#配置-glusterfs" class="headerlink" title="配置 glusterfs"></a>配置 glusterfs</h2><p><font color="red" size="3">三台服务器都要执行</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置本地解析文件hosts</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vi /etc/hosts</span></span><br><span class="line">192.168.137.10 api.huisebug.com</span><br><span class="line">192.168.137.11 node1.huisebug.com</span><br><span class="line">192.168.137.12 node2.huisebug.com</span><br><span class="line"><span class="meta">#</span><span class="bash"> 开放端口（24007是gluster服务运行所需的端口号）如果关闭了防火墙就省略此步操作。</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> iptables -I INPUT -p tcp --dport 24007 -j ACCEPT</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建存储目录</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir /opt/gfs_data</span></span><br><span class="line"></span><br><span class="line">为了方便管理可使用一块新硬盘新建一个分区将其挂载到 /opt/gfs_data</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看硬盘</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> fdisk -l /dev/sdb</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 建立分区并格式化为xfs类型</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> fdisk /dev/sdb</span></span><br><span class="line">1.  n 建立新分区</span><br><span class="line">2.  p 建立主分区</span><br><span class="line">3.  1 分区号为1</span><br><span class="line">4.  一直回车默认，将整块磁盘建立为一个分区，使用全部空间</span><br><span class="line">5.  w 保存退出</span><br><span class="line"><span class="meta">#</span><span class="bash"> 使分区生效</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> partprobe /dev/sdb</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 强制格式化为xfs</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkfs.xfs -f /dev/sdb1</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 手动将/dev/sdb1分区挂载到/opt/gfs_data</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mount /dev/sdb1 /opt/gfs_data</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 开机自动挂载</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">&quot;/dev/sdb1 /opt/gfs_data xfs defaults 0 0&quot;</span> &gt;&gt; /etc/fstab</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看挂载状态</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> df –hT</span></span><br></pre></td></tr></table></figure><h2 id="添加节点到集群"><a href="#添加节点到集群" class="headerlink" title="添加节点到集群"></a>添加节点到集群</h2><p>三台服务器都安装好服务并成功启动后</p><p>执行probe操作即将三台服务器的gluster服务建立集群，选择其中任意一台执行probe操作，将会建立一个集群，执行完成后在任意一台执行以下命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gluster peer status</span><br></pre></td></tr></table></figure><p>将会看到其他两台的信息<br>执⾏操作的本机不需要 probe本机，并且只需执行一次,此处我在api.huisebug.com上操作。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gluster peer probe node1.huisebug.com</span><br><span class="line">gluster peer probe node2.huisebug.com</span><br></pre></td></tr></table></figure><p>查看状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gluster peer status </span><br></pre></td></tr></table></figure><h2 id="volume-类型"><a href="#volume-类型" class="headerlink" title="volume 类型"></a>volume 类型</h2><p>GlusterFS中的volume的模式有很多中，包括以下⼏种：</p><ul><li>  分布卷（默认模式）：即DHT, 也叫 分布卷: 将⽂件已hash算法随机分布到⼀台服务器节点中存储。</li><li>  复制模式：即AFR, 创建volume 时带 replica x 数量: 将⽂件复制到replica x个节点中。</li><li>  条带模式：即Striped, 创建volume 时带 stripe x 数量：将⽂件切割成数据块，分别存储到 stripe x 个节点中 ( 类似raid 0 )。</li><li>  分布式条带模式：最少需要4台服务器才能创建。 创建volume 时stripe 2 server = 4个节点： 是DHT 与 Striped 的组合型。</li><li>  分布式复制模式：最少需要4台服务器才能创建。 创建volume 时replica 2 server =4 个节点：是DHT 与 AFR 的组合型。</li><li>  条带复制卷模式：最少需要4台服务器才能创建。 创建volume 时stripe 2 replica 2 server = 4 个节点： 是 Striped 与 AFR 的组合型。</li><li>  三种模式混合： ⾄少需要8台 服务器才能创建。 stripe 2 replica 2 ,每4个节点组成⼀个组。<br>可参考以下链接<br><a href="http://www.cnblogs.com/jicki/p/5801712.html">http://www.cnblogs.com/jicki/p/5801712.html</a></li></ul><p><font color="red" size="5">为了项目要求，我这里建立复制模式的volume</font></p><h2 id="建立复制卷"><a href="#建立复制卷" class="headerlink" title="建立复制卷"></a>建立复制卷</h2><p>现在整个gluster集群是一个整体，任意节点执行都可以</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 建立复制卷，卷名为test-volume</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume create test-volume replica 3 \</span></span><br><span class="line"><span class="bash">api.huisebug.com:/opt/gfs_data \</span></span><br><span class="line"><span class="bash">node1.huisebug.com:/opt/gfs_data \</span></span><br><span class="line"><span class="bash">node2.huisebug.com:/opt/gfs_data \</span></span><br><span class="line"><span class="bash">force</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 开启复制卷</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume start test-volume</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看volume状态</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume info</span></span><br><span class="line">Volume Name: test-volume &lt;br&gt;Type: Replicate &lt;br&gt;Volume ID: b52e2b36-0a88-43c5-b596-5ec0e01ca529 &lt;br&gt;Status: Started &lt;br&gt;Snapshot Count: 0 &lt;br&gt;Number of Bricks: 1 x 3 = 3&lt;br&gt; Transport-type: tcp &lt;br&gt;Bricks: &lt;br&gt;Brick1: api.huisebug.com:/opt/gfs_data &lt;br&gt;Brick2: node1.huisebug.com:/opt/gfs_data &lt;br&gt;Brick3: node2.huisebug.com:/opt/gfs_data &lt;br&gt;Options Reconfigured: &lt;br&gt;transport.address-family: inet &lt;br&gt;nfs.disable: on &lt;br&gt;performance.client-io-threads: off </span><br></pre></td></tr></table></figure><h2 id="配额限制"><a href="#配额限制" class="headerlink" title="配额限制"></a>配额限制</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开启 指定 volume 的配额</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume quota test-volume <span class="built_in">enable</span></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 限制 指定 volume 的配额，我们这里的硬盘是50G，所以分配给他10G</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume quota test-volume limit-usage / 10GB</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 cache ⼤⼩, 默认32MB,这个千万别设置太大了，不然会导致挂载不上</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume <span class="built_in">set</span> test-volume performance.cache-size 160MB</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 io 线程, 太⼤会导致进程崩溃</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume <span class="built_in">set</span> test-volume performance.io-thread-count 16</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 ⽹络检测时间, 默认42s</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume <span class="built_in">set</span> test-volume network.ping-timeout 10</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 写缓冲区的⼤⼩, 默认1M</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume <span class="built_in">set</span> test-volume performance.write-behind-window-size 1024MB</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 回写 (写数据时间，先写入缓存内，再写入硬盘)</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume <span class="built_in">set</span> test-volume performance.write-behind on</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置好了voleme的配额后要重新启动卷</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume stop test-volume</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> gluster volume start test-volume</span></span><br></pre></td></tr></table></figure><h2 id="挂载使用"><a href="#挂载使用" class="headerlink" title="挂载使用"></a>挂载使用</h2><p><font color="red" size="3">三台服务器都要执行</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 建立使用服务器的目录</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p /opt/gfs_datause</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 挂载关联</span></span><br><span class="line">mount –t 指定挂载的卷类型 主机地址:卷名 挂载到本地的哪个目录（主机地址输入哪台服务器的地址都可以）</span><br><span class="line"><span class="meta">$</span><span class="bash"> mount -t glusterfs api.huisebug.com:test-volume /opt/gfs_datause</span></span><br><span class="line">如果挂载失败，查看日志文件/var/log/glusterfs/opt-gfs_datause.log解决问题所在，我遇到是我把gluster volume set test-volume performance.cache-size 160MB设置为了4GB，导致无法挂载，然后我修改为了160MB，就可以了</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">查看挂载情况，如果要持续使用记得设置自动挂载</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> df -hT</span></span><br><span class="line">文件系统 类型 容量 已用 可用 已用% 挂载点 </span><br><span class="line">api.huisebug.com:test-volume fuse.glusterfs 10G 0 10G 0% /opt/gfs_datause </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">自动挂载</span></span><br><span class="line">echo &quot;api.huisebug.com:test-volume /opt/gfs_datause glusterfs defaults 0 0&quot; &gt;&gt; /etc/fstab</span><br></pre></td></tr></table></figure><p><font color="red" size="3">记得执行node1和node2上述操作</font></p><h2 id="测试同步效果"><a href="#测试同步效果" class="headerlink" title="测试同步效果"></a>测试同步效果</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@api gfs_datause]# touch &#123;1..10&#125; </span><br><span class="line">[root@api gfs_datause]# ls </span><br><span class="line">1 10 2 3 4 5 6 7 8 9 </span><br><span class="line"></span><br><span class="line">[root@node1 opt]# ls gfs_datause/</span><br><span class="line">1 10 2 3 4 5 6 7 8 9 </span><br><span class="line"></span><br><span class="line">[root@node2 gfs_datause]# ls</span><br><span class="line">1 10 2 3 4 5 6 7 8 9 </span><br></pre></td></tr></table></figure><p>GlusterFS搭建完毕！！！</p><h1 id="Keepalived"><a href="#Keepalived" class="headerlink" title="Keepalived"></a>Keepalived</h1><p>整个集群的高可用VIP</p><p><font color="red" size="3">三台服务器都要执行</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 安装keepalived和ipvs、socat</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> yum install -y socat keepalived ipvsadm</span></span><br></pre></td></tr></table></figure><h2 id="启动Keepalived服务"><a href="#启动Keepalived服务" class="headerlink" title="启动Keepalived服务"></a>启动Keepalived服务</h2><p>修改配置文件keepalived.conf为如下三台服务器的，这里我们暂时先让VIP运行起来，后续监听端口和进程后续再增加。</p><p>api</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim /etc/keepalived/keepalived.conf</span> </span><br><span class="line"></span><br><span class="line">! Configuration File for keepalived </span><br><span class="line">global_defs &#123;</span><br><span class="line"> router_id vip1 </span><br><span class="line">&#125; </span><br><span class="line">vrrp_instance VI_1 &#123; </span><br><span class="line"><span class="meta">#</span><span class="bash">三台配置此处均是BACKUP</span> </span><br><span class="line">state BACKUP </span><br><span class="line">interface ens33 </span><br><span class="line">virtual_router_id 55 </span><br><span class="line"><span class="meta">#</span><span class="bash">优先级</span> </span><br><span class="line">priority 95 </span><br><span class="line">advert_int 1 </span><br><span class="line">authentication &#123; </span><br><span class="line"> auth_type PASS </span><br><span class="line"> auth_pass 123456 </span><br><span class="line">&#125; </span><br><span class="line">virtual_ipaddress &#123;</span><br><span class="line"> 192.168.137.13 </span><br><span class="line">&#125; </span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure><p>node1</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim /etc/keepalived/keepalived.conf</span> </span><br><span class="line"></span><br><span class="line">! Configuration File for keepalived </span><br><span class="line">global_defs &#123;</span><br><span class="line"> router_id vip2</span><br><span class="line"> &#125; </span><br><span class="line">vrrp_instance VI_1 &#123; </span><br><span class="line"><span class="meta">#</span><span class="bash">三台配置此处均是BACKUP</span> </span><br><span class="line">state BACKUP</span><br><span class="line">interface ens33 </span><br><span class="line">virtual_router_id 55 </span><br><span class="line"><span class="meta">#</span><span class="bash">优先级</span> </span><br><span class="line">priority 90 </span><br><span class="line">advert_int 1 </span><br><span class="line">authentication &#123;</span><br><span class="line"> auth_type PASS </span><br><span class="line"> auth_pass 123456 </span><br><span class="line">&#125; </span><br><span class="line">virtual_ipaddress &#123; </span><br><span class="line">192.168.137.13</span><br><span class="line">&#125;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure><p>node2</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim /etc/keepalived/keepalived.conf</span> </span><br><span class="line"></span><br><span class="line">! Configuration File for keepalived </span><br><span class="line">global_defs &#123;</span><br><span class="line"> router_id vip3</span><br><span class="line"> &#125;</span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line"><span class="meta"> #</span><span class="bash">三台配置此处均是BACKUP</span> </span><br><span class="line">state BACKUP </span><br><span class="line">interface ens33 </span><br><span class="line">virtual_router_id 55 </span><br><span class="line"><span class="meta">#</span><span class="bash">优先级</span> </span><br><span class="line">priority 85 </span><br><span class="line">advert_int 1 </span><br><span class="line">authentication &#123;</span><br><span class="line"> auth_type PASS</span><br><span class="line"> auth_pass 123456 </span><br><span class="line">&#125; </span><br><span class="line">virtual_ipaddress &#123;</span><br><span class="line"> 192.168.137.13 </span><br><span class="line">&#125;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure><p>重启服务，自启</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> systemctl restart keepalived</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl <span class="built_in">enable</span> keepalived</span></span><br></pre></td></tr></table></figure><p>在优先级最高的那台查看VIP地址是否建立</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip addr</span></span><br></pre></td></tr></table></figure><p>验证可以使用新建一个终端来验证是否会连接到优先级高的那台服务器</p><h1 id="证书"><a href="#证书" class="headerlink" title="证书"></a>证书</h1><p>Kubernetes系统的各组件需要使⽤TLS证书对通信进⾏加密，本⽂档使⽤CloudFlare的PKI⼯具集cfssl来⽣成Certificate Authority (CA)和其它证书；⽣成的CA证书和秘钥⽂件如下：</p><ul><li>  ca-key.pem</li><li>  ca.pem</li><li>  kubernetes-key.pem</li><li>  kubernetes.pem</li><li>  kube-proxy.pem</li><li>  kube-proxy-key.pem</li><li>  admin.pem</li><li>  admin-key.pem</li></ul><p>使⽤证书的组件如下：</p><ul><li>  etcd：使⽤ ca.pem、kubernetes-key.pem、kubernetes.pem；</li><li>  kube-apiserver：使⽤ ca.pem、kubernetes-key.pem、kubernetes.pem；</li><li>  kubelet：使⽤ ca.pem；</li><li>  kube-proxy：使⽤ ca.pem、kube-proxy-key.pem、kubeproxy.pem；</li><li>  kubectl：使⽤ ca.pem、admin-key.pem、admin.pem；</li><li>  kube-controller-manager：使⽤ ca-key.pem、ca.pem</li></ul><p>注意：以下操作都在 <strong>api</strong> 节点即 <strong>192.168.137.10</strong>这台主机上执⾏，证书只需要创建⼀次即可，以后在向集群中添加新节点时只要将**/etc/kubernetes/**⽬录下的证书拷⻉到新节点上即可。</p><h2 id="安装cfssl"><a href="#安装cfssl" class="headerlink" title="安装cfssl"></a>安装cfssl</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64</span><br><span class="line">chmod +x cfssl_linux-amd64</span><br><span class="line">mv cfssl_linux-amd64 /usr/local/bin/cfssl</span><br><span class="line">wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64</span><br><span class="line">chmod +x cfssljson_linux-amd64</span><br><span class="line">mv cfssljson_linux-amd64 /usr/local/bin/cfssljson</span><br><span class="line">wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64</span><br><span class="line">chmod +x cfssl-certinfo_linux-amd64</span><br><span class="line">mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo</span><br><span class="line">export PATH=/usr/local/bin:$PATH</span><br></pre></td></tr></table></figure><h2 id="创建ca配置文件"><a href="#创建ca配置文件" class="headerlink" title="创建ca配置文件"></a>创建ca配置文件</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir /root/ssl</span><br><span class="line">cd /root/ssl</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下面的操作是将模板文件复制过来，可以不操作，直接建立。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cfssl print-defaults config &gt; config.json</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cfssl print-defaults csr &gt; csr.json</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cat config.json &gt; ca-config.json</span></span><br></pre></td></tr></table></figure><p>根据config.json⽂件的格式创建如下的ca-config.json⽂件<br>过期时间设置成了 87600h(10年)<br>修改 ca-config.json内容为如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">vim ca-config.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;signing&quot;: &#123;</span><br><span class="line">        &quot;default&quot;: &#123;</span><br><span class="line">            &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;profiles&quot;: &#123;</span><br><span class="line">            &quot;kubernetes&quot;: &#123;</span><br><span class="line">                &quot;usages&quot;: [</span><br><span class="line">                    &quot;signing&quot;,</span><br><span class="line">                    &quot;key encipherment&quot;,</span><br><span class="line">                    &quot;server auth&quot;,</span><br><span class="line">                    &quot;client auth&quot;</span><br><span class="line">                ],</span><br><span class="line">                &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>字段说明:</p><ul><li>  ca-config.json ：可以定义多个profiles，分别指定不同的过期时间、使⽤场景等参数；后续在签名证书时使⽤某个profile；</li><li>  signing ：表示该证书可⽤于签名其它证书；⽣成的 ca.pem 证书中CA=TRUE ；</li><li>  server auth ：表示client可以⽤该 CA 对server提供的证书进⾏验证；</li><li>  client auth ：表示server可以⽤该CA对client提供的证书进⾏验证；</li></ul><h2 id="创建CA证书"><a href="#创建CA证书" class="headerlink" title="创建CA证书"></a>创建CA证书</h2><h3 id="创建-CA-证书签名请求"><a href="#创建-CA-证书签名请求" class="headerlink" title="创建 CA 证书签名请求"></a>创建 CA 证书签名请求</h3><p>创建 ca-csr.json ⽂件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">vim ca-csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>  “CN”： Common Name ，kube-apiserver 从证书中提取该字段作为请求的⽤户名 (User Name)；浏览器使⽤该字段验证⽹站是否合法；</li><li>  “O”： Organization ，kube-apiserver 从证书中提取该字段作为请求⽤户所属的组(Group)；</li></ul><h3 id="⽣成-CA-证书和私钥ca-key-pem-ca-pem"><a href="#⽣成-CA-证书和私钥ca-key-pem-ca-pem" class="headerlink" title="⽣成 CA 证书和私钥ca-key.pem ca.pem"></a>⽣成 CA 证书和私钥ca-key.pem ca.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert -initca ca-csr.json | cfssljson -bare ca</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls</span></span><br><span class="line">ca-config.json ca.csr ca-csr.json ca-key.pem ca.pem config.json csr.json</span><br></pre></td></tr></table></figure><h2 id="创建-kubernetes-证书"><a href="#创建-kubernetes-证书" class="headerlink" title="创建 kubernetes 证书"></a>创建 kubernetes 证书</h2><h3 id="创建-kubernetes-证书签名请求⽂件-kubernetes-csr-json"><a href="#创建-kubernetes-证书签名请求⽂件-kubernetes-csr-json" class="headerlink" title="创建 kubernetes 证书签名请求⽂件 kubernetes-csr.json"></a>创建 kubernetes 证书签名请求⽂件 kubernetes-csr.json</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">cat ca-csr.json &gt; kubernetes-csr.json</span><br><span class="line">vim kubernetes-csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">        &quot;127.0.0.1&quot;,</span><br><span class="line">        &quot;192.168.137.10&quot;,</span><br><span class="line">        &quot;192.168.137.11&quot;,</span><br><span class="line">        &quot;192.168.137.12&quot;,</span><br><span class="line">        &quot;192.168.137.13&quot;,</span><br><span class="line">        &quot;10.254.0.1&quot;,</span><br><span class="line">        &quot;kubernetes&quot;,</span><br><span class="line">        &quot;kubernetes.default&quot;,</span><br><span class="line">        &quot;kubernetes.default.svc&quot;,</span><br><span class="line">        &quot;kubernetes.default.svc.cluster&quot;,</span><br><span class="line">        &quot;kubernetes.default.svc.cluster.local&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>  如果 hosts 字段不为空则需要指定授权使⽤该证书的 <strong>IP</strong>或域名列表，由于该证书后续被 etcd 集群和 kubernetes master集群使⽤，所以上⾯分别指定了 etcd 集群、 kubernetes master集群的主机 IP 和<strong>kubernetes</strong> 服务的集群 <strong>IP</strong>（⼀般是 kube-apiserver 指定的service-cluster-ip-range ⽹段的第⼀个IP，如 10.254.0.1。</li><li>  hosts中的内容可以为空，即使按照上⾯的置，向集群中增加新节点后也不需要重新⽣成证书。</li></ul><h3 id="⽣成-kubernetes-证书和私钥kubernetes-key-pem-kubernetes-pem"><a href="#⽣成-kubernetes-证书和私钥kubernetes-key-pem-kubernetes-pem" class="headerlink" title="⽣成 kubernetes 证书和私钥kubernetes-key.pem kubernetes.pem"></a>⽣成 kubernetes 证书和私钥kubernetes-key.pem kubernetes.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls kubernetes*</span></span><br><span class="line">kubernetes.csr kubernetes-csr.json kubernetes-key.pem kubernetes.pem</span><br></pre></td></tr></table></figure><h2 id="创建admin证书"><a href="#创建admin证书" class="headerlink" title="创建admin证书"></a>创建admin证书</h2><h3 id="创建-admin-证书签名请求⽂件-admin-csr-json-："><a href="#创建-admin-证书签名请求⽂件-admin-csr-json-：" class="headerlink" title="创建 admin 证书签名请求⽂件 admin-csr.json ："></a>创建 admin 证书签名请求⽂件 admin-csr.json ：</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cat ca-csr.json &gt; admin-csr.json</span><br><span class="line">vim admin-csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;admin&quot;,</span><br><span class="line">    &quot;hosts&quot;:[],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;system:masters&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>  后续 kube-apiserver 使⽤ RBAC 对客户端(如 kubelet 、 kubeproxy、 Pod)请求进⾏授权；</li><li>  kube-apiserver 预定义了⼀些 RBAC 使⽤的 RoleBindings ，如cluster-admin 将Group system:masters 与 Role cluster-admin绑定，该 Role 授予了调⽤kube-apiserver 的所有 <strong>API</strong>的权限；</li><li>  OU 指定该证书的 Group 为 system:masters ， kubelet 使⽤该证书访问kube-apiserver 时 ，由于证书被 CA签名，所以认证通过，同时由于证书⽤户组为经过预授权的 system:masters，所以被授予访问所有 API 的权限；</li></ul><h3 id="⽣成-admin-证书和私钥admin-key-pem-admin-pem"><a href="#⽣成-admin-证书和私钥admin-key-pem-admin-pem" class="headerlink" title="⽣成 admin 证书和私钥admin-key.pem admin.pem"></a>⽣成 admin 证书和私钥admin-key.pem admin.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls admin*</span></span><br><span class="line">admin.csr admin-csr.json admin-key.pem admin.pem</span><br></pre></td></tr></table></figure><h2 id="创建-kube-controller-manager-证书"><a href="#创建-kube-controller-manager-证书" class="headerlink" title="创建 kube-controller-manager 证书"></a>创建 kube-controller-manager 证书</h2><h3 id="创建-kube-controller-manager-证书签名请求⽂件-kube-controller-manager-csr-json"><a href="#创建-kube-controller-manager-证书签名请求⽂件-kube-controller-manager-csr-json" class="headerlink" title="创建 kube-controller-manager 证书签名请求⽂件 kube-controller-manager-csr.json"></a>创建 kube-controller-manager 证书签名请求⽂件 kube-controller-manager-csr.json</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim kube-controller-manager-csr.json</span></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;system:kube-controller-manager&quot;,</span><br><span class="line">    &quot;hosts&quot;: [],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;system:kube-controller-manager&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="⽣成kube-controller-manager证书和私钥kube-controller-manager-key-pem-kube-controller-manager-pem"><a href="#⽣成kube-controller-manager证书和私钥kube-controller-manager-key-pem-kube-controller-manager-pem" class="headerlink" title="⽣成kube-controller-manager证书和私钥kube-controller-manager-key.pem kube-controller-manager.pem"></a>⽣成kube-controller-manager证书和私钥kube-controller-manager-key.pem kube-controller-manager.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls kube-controller-manager*</span></span><br><span class="line">kube-controller-manager.csr kube-controller-manager-csr.json kube-controller-manager-key.pem kube-controller-manager.pem</span><br></pre></td></tr></table></figure><h2 id="创建-kube-scheduler-证书"><a href="#创建-kube-scheduler-证书" class="headerlink" title="创建 kube-scheduler 证书"></a>创建 kube-scheduler 证书</h2><h3 id="创建kube-scheduler-证书签名请求⽂件-kube-scheduler-csr-json"><a href="#创建kube-scheduler-证书签名请求⽂件-kube-scheduler-csr-json" class="headerlink" title="创建kube-scheduler 证书签名请求⽂件 kube-scheduler-csr.json"></a>创建kube-scheduler 证书签名请求⽂件 kube-scheduler-csr.json</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim kube-scheduler-csr.json</span></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;:&quot;system:kube-scheduler&quot;,</span><br><span class="line">    &quot;hosts&quot;: [],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;system:kube-scheduler&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="⽣成kube-scheduler证书和私钥kube-scheduler-key-pem-kube-scheduler-pem"><a href="#⽣成kube-scheduler证书和私钥kube-scheduler-key-pem-kube-scheduler-pem" class="headerlink" title="⽣成kube-scheduler证书和私钥kube-scheduler-key.pem kube-scheduler.pem"></a>⽣成kube-scheduler证书和私钥kube-scheduler-key.pem kube-scheduler.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls kube-scheduler*</span></span><br><span class="line">kube-scheduler.csr kube-scheduler-csr.json kube-scheduler-key.pem kube-scheduler.pem</span><br></pre></td></tr></table></figure><h2 id="创建-front-proxy-证书"><a href="#创建-front-proxy-证书" class="headerlink" title="创建 front-proxy 证书"></a>创建 front-proxy 证书</h2><h3 id="创建-front-proxy-证书签名请求⽂件front-proxy-ca-csr-json"><a href="#创建-front-proxy-证书签名请求⽂件front-proxy-ca-csr-json" class="headerlink" title="创建 front-proxy 证书签名请求⽂件front-proxy-ca-csr.json"></a>创建 front-proxy 证书签名请求⽂件<strong>front-proxy-ca-csr.json</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim front-proxy-ca-csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="生成-front-proxy-ca证书和私钥front-proxy-ca-key-pem-front-proxy-ca-pem"><a href="#生成-front-proxy-ca证书和私钥front-proxy-ca-key-pem-front-proxy-ca-pem" class="headerlink" title="生成 front-proxy-ca证书和私钥front-proxy-ca-key.pem  front-proxy-ca.pem"></a>生成 front-proxy-ca证书和私钥front-proxy-ca-key.pem  front-proxy-ca.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-ca</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls front-proxy-ca*</span></span><br><span class="line">front-proxy-ca.csr front-proxy-ca-csr.json front-proxy-ca-key.pem front-proxy-ca.pem</span><br></pre></td></tr></table></figure><h3 id="创建-front-proxy-client-证书签名请求⽂件-front-proxy-client-csr-json"><a href="#创建-front-proxy-client-证书签名请求⽂件-front-proxy-client-csr-json" class="headerlink" title="创建 front-proxy-client 证书签名请求⽂件 front-proxy-client-csr.json"></a>创建 front-proxy-client 证书签名请求⽂件 front-proxy-client-csr.json</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim front-proxy-client-csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;front-proxy-client&quot;,</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="⽣成front-proxy-client证书和私钥front-proxy-client-key-pem-front-proxy-client-pem"><a href="#⽣成front-proxy-client证书和私钥front-proxy-client-key-pem-front-proxy-client-pem" class="headerlink" title="⽣成front-proxy-client证书和私钥front-proxy-client-key.pem  front-proxy-client.pem"></a>⽣成front-proxy-client证书和私钥front-proxy-client-key.pem  front-proxy-client.pem</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cfssl gencert \</span></span><br><span class="line"><span class="bash">-ca=front-proxy-ca.pem \</span></span><br><span class="line"><span class="bash">-ca-key=front-proxy-ca-key.pem \</span></span><br><span class="line"><span class="bash">-config=ca-config.json \</span></span><br><span class="line"><span class="bash">-profile=kubernetes \</span></span><br><span class="line"><span class="bash">front-proxy-client-csr.json | cfssljson -bare front-proxy-client</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls front-proxy-client*</span></span><br><span class="line">front-proxy-client.csr front-proxy-client-csr.json front-proxy-client-key.pem front-proxy-client.pem</span><br></pre></td></tr></table></figure><h2 id="校验证书"><a href="#校验证书" class="headerlink" title="校验证书"></a>校验证书</h2><p>以 kubernetes 证书为例</p><h3 id="使⽤-opsnssl-命令"><a href="#使⽤-opsnssl-命令" class="headerlink" title="使⽤ opsnssl 命令"></a>使⽤ opsnssl 命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl x509 -noout -text -in kubernetes.pem</span><br></pre></td></tr></table></figure><ul><li>  确认 Issuer 字段的内容和 ca-csr.json ⼀致；</li><li>  确认 Subject 字段的内容和 kubernetes-csr.json ⼀致；</li><li>  确认 X509v3 Subject Alternative Name 字段的内容和 kubernetescsr.json ⼀致；</li><li>  确认 X509v3 Key Usage、Extended Key Usage 字段的内容和 caconfig.json 中kubernetes profile ⼀致；</li></ul><h3 id="使⽤-cfssl-certinfo-命令"><a href="#使⽤-cfssl-certinfo-命令" class="headerlink" title="使⽤ cfssl-certinfo 命令"></a>使⽤ cfssl-certinfo 命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cfssl-certinfo -cert kubernetes.pem</span><br></pre></td></tr></table></figure><h2 id="分发证书"><a href="#分发证书" class="headerlink" title="分发证书"></a>分发证书</h2><p><font color="red" size="3">三台服务器都要执行</font></p><p>将⽣成的证书和秘钥⽂件（后缀名为 .pem ）拷⻉到所有机器的/etc/kubernetes/ssl<br>⽬录下备⽤；具体复制方法自己操作</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/kubernetes/ssl</span><br><span class="line">cp -rf *.pem /etc/kubernetes/ssl</span><br></pre></td></tr></table></figure><h1 id="Haproxy"><a href="#Haproxy" class="headerlink" title="Haproxy"></a>Haproxy</h1><p>1.结合keepalived保证整个集群的HA（高可用）</p><p>2.使用 keepalived 和 haproxy 实现 kube-apiserver 高可用的步骤：</p><ul><li>  keepalived 提供 kube-apiserver 对外服务的 VIP；</li><li>  haproxy 监听 VIP，后端连接所有 kube-apiserver实例，提供健康检查和负载均衡功能；</li><li>  运行 keepalived 和 haproxy 的节点称为 LB 节点。由于 keepalived是一主多备运行模式，故至少两个 LB 节点。复用 master 节点的三台机器，haproxy监听的端口(9443) 需要与 kube-apiserver 的端口6443不同，避免冲突。</li><li>  keepalived 在运行过程中周期检查本机的 haproxy 进程状态，如果检测到 haproxy进程异常，则触发重新选主的过程，VIP 将飘移到新选出来的主节点，从而实现 VIP的高可用。</li><li>  所有组件（如kubectl、kube-apiserver、kube-controller-manager、kube-scheduler、kubelet等）都通过VIP和haproxy监听的9443 端口访问kube-apiserver服务。</li></ul><p><font color="red" size="3">三台服务器都要执行</font></p><h2 id="安装haproxy"><a href="#安装haproxy" class="headerlink" title="安装haproxy"></a>安装haproxy</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> yum -y install haproxy</span></span><br></pre></td></tr></table></figure><p>修改haproxy配置文件如下，这里将9443端口（这个端口将代理到各k8s集群api端口6443）和haproxy进程绑定在一起，并定义后端服务器。并且开启100端口监听haproxy进程状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">haproxy 配置文件：</span></span><br><span class="line"><span class="meta">cat&gt;</span><span class="bash">haproxy.cfg&lt;&lt;<span class="string">EOF</span></span></span><br><span class="line">global</span><br><span class="line">    log /dev/log    local0</span><br><span class="line">    log /dev/log    local1 notice</span><br><span class="line">    chroot /var/lib/haproxy</span><br><span class="line">    stats socket /run/haproxy/admin.sock mode 660 level admin</span><br><span class="line">    stats timeout 30s</span><br><span class="line">    user haproxy</span><br><span class="line">    group haproxy</span><br><span class="line">    daemon</span><br><span class="line">    nbproc 1</span><br><span class="line">    </span><br><span class="line">defaults</span><br><span class="line">    log     global</span><br><span class="line">    timeout connect 5000</span><br><span class="line">    timeout client  10m</span><br><span class="line">    timeout server  10m</span><br><span class="line"></span><br><span class="line">listen  admin_stats</span><br><span class="line">    bind 0.0.0.0:100</span><br><span class="line">    mode http</span><br><span class="line">    log 127.0.0.1 local0 err</span><br><span class="line">    stats refresh 30s </span><br><span class="line">    stats uri /status</span><br><span class="line">    stats realm welcome login\ Haproxy</span><br><span class="line">    stats auth admin:123456</span><br><span class="line">    stats hide-version</span><br><span class="line">    stats admin if TRUE</span><br><span class="line"></span><br><span class="line"> listen kube-master</span><br><span class="line">     bind 0.0.0.0:9443</span><br><span class="line">     mode tcp</span><br><span class="line">     option tcplog</span><br><span class="line">     balance source</span><br><span class="line">     server api 192.168.137.10:6443 check inter 2000 fall 2 rise 2 weight 1</span><br><span class="line">     server node1 192.168.137.11:6443 check inter 2000 fall 2 rise 2 weight 1</span><br><span class="line">     server node2 192.168.137.12:6443 check inter 2000 fall 2 rise 2 weight 1</span><br><span class="line">EOF</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>建立haproxy的工作目录并启动服务，开机自启，工作目录/run/haproxy重启服务器会丢失，所以将其加入到随系统启动而建立</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 赋予执行权限</span></span><br><span class="line">chmod +x /etc/rc.d/rc.local</span><br><span class="line">echo &quot;mkdir -p /run/haproxy&quot; &gt;&gt; /etc/rc.local </span><br><span class="line">mkdir -p /run/haproxy </span><br><span class="line">systemctl start haproxy </span><br><span class="line">systemctl enable haproxy</span><br></pre></td></tr></table></figure><p>查看是否监听9443端口。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">netstat -lntp | grep 9443</span><br><span class="line">tcp 0 0 0.0.0.0:9443 0.0.0.0:* LISTEN 10019/haproxy</span><br></pre></td></tr></table></figure><h2 id="增加keepalived配置，使其嗅探haproxy的状态"><a href="#增加keepalived配置，使其嗅探haproxy的状态" class="headerlink" title="增加keepalived配置，使其嗅探haproxy的状态"></a>增加keepalived配置，使其嗅探haproxy的状态</h2><p><font color="red" size="3">三台服务器都要执行</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vrrp_script check-haproxy &#123;</span><br><span class="line">    script &quot;killall -0 haproxy&quot;</span><br><span class="line">    interval 1</span><br><span class="line">    weight -20</span><br><span class="line">&#125;</span><br><span class="line">并且在VIP下面配置track_script以对应上面的脚本</span><br><span class="line">  virtual_ipaddress &#123;</span><br><span class="line">         192.168.137.13</span><br><span class="line">     &#125;</span><br><span class="line">   track_script &#123;</span><br><span class="line">     check-haproxy</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>使用 killall -0 haproxy 命令检查所在节点的 haproxy<br>进程是否正常。如果异常则将权重减少（-20）,从而触发重新选主过程；</p><p>例如api节点配置<br><img src="/2020/08/26/k8s/media/07aa73fd92ffd1f9e62dbe8c48c64de2.png"></p><p>重启服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart keepalived</span><br></pre></td></tr></table></figure><p>验证HA效果，用户名和密码是之前开启的100端口中定义的admin:123456<br><img src="/2020/08/26/k8s/media/5f9ad336cd3c31746b5f7a08ac951d0a.png"><br><img src="/2020/08/26/k8s/media/bfbddf798678dc16cf6abd1a4f1514b0.png"></p><h1 id="kubectl"><a href="#kubectl" class="headerlink" title="kubectl"></a>kubectl</h1><p>kubectl作为k8s集群的客户端工具，首先需要安装来生成集群配置文件kubeconfig<br>kubernetes二进制下载地址，为了快速下载，我们可以在机器上下载后再上传到其他主机<br>下载地址：<br><a href="https://dl.k8s.io/v1.12.4/kubernetes-server-linux-amd64.tar.gz">https://dl.k8s.io/v1.12.4/kubernetes-server-linux-amd64.tar.gz</a></p><p>因为我们要做HA集群，所以这里三台服务器都会作为api服务器，所以我们直接将所有二进制程序放到指定地方</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget https://dl.k8s.io/v1.12.4/kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">tar zxf kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">cd kubernetes</span><br><span class="line">cp -rf server/bin/&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet&#125; /usr/local/bin/</span><br></pre></td></tr></table></figure><h2 id="创建-kubectl所需kubeconfig文件"><a href="#创建-kubectl所需kubeconfig文件" class="headerlink" title="创建 kubectl所需kubeconfig文件"></a>创建 kubectl所需kubeconfig文件</h2><p>主要是用于kubectl命令和kubelet服务进行获取apiserver信息并且集群角色cluster-admin与自建用户admin进行绑定</p><p>kubeconfig.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export KUBE_APISERVER=&quot;https://192.168.137.13:9443&quot;</span><br><span class="line">kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/ssl/ca.pem --embed-certs=true --server=$&#123;KUBE_APISERVER&#125;</span><br><span class="line">kubectl config set-credentials admin --client-certificate=/etc/kubernetes/ssl/admin.pem --embed-certs=true --client-key=/etc/kubernetes/ssl/admin-key.pem</span><br><span class="line">kubectl config set-context kubernetes --cluster=kubernetes --user=admin</span><br><span class="line">kubectl config use-context kubernetes</span><br></pre></td></tr></table></figure><ul><li>  KUBEAPISERVER变量中定义的是VIP地址。</li><li>  admin.pem证书OU字段值为 system:masters,kube-apiserver预定义的RoleBinding cluster-admin将Group system:masters与 Role cluster-admin绑定，该 Role 授予了调⽤ kube-apiserver相关 API 的权限；</li><li>  ⽣成的 kubeconfig 被保存到 ~/.kube/config ⽂件;</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source kubeconfig.sh</span><br></pre></td></tr></table></figure><h1 id="k8s集群建立前预备配置文件"><a href="#k8s集群建立前预备配置文件" class="headerlink" title="k8s集群建立前预备配置文件"></a>k8s集群建立前预备配置文件</h1><h2 id="创建-TLS-Bootstrapping-token文件"><a href="#创建-TLS-Bootstrapping-token文件" class="headerlink" title="创建 TLS Bootstrapping token文件"></a>创建 TLS Bootstrapping token文件</h2><h3 id="Token-auth-file"><a href="#Token-auth-file" class="headerlink" title="Token auth file"></a>Token auth file</h3><p>Token可以是任意的包涵128 bit的字符串，可以使⽤安全的随机数发⽣器⽣成</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x |tr -d &#x27; &#x27;)</span><br></pre></td></tr></table></figure><p>建立token.csv</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; token.csv &lt;&lt; EOF</span><br><span class="line"><span class="meta">$</span><span class="bash">&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,<span class="string">&quot;system:kubelet-bootstrap&quot;</span></span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat token.csv</span><br><span class="line">b156d4d29fe7a73e554a145fc996e3c6,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;</span><br></pre></td></tr></table></figure><p>注意：在进⾏后续操作前请检查 <strong>token.csv</strong>⽂件，确认其中的**${BOOTSTRAP_TOKEN}** 环境变量已经被真实的值替换。</p><p><strong>BOOTSTRAP_TOKEN</strong> 将被写⼊到 kube-apiserver 使⽤的 token.csv ⽂件和 kubelet使⽤的 bootstrap.kubeconfig ⽂件，如果后续重新⽣成了BOOTSTRAP_TOKEN，则需要：</p><ol><li>更新 token.csv ⽂件，分发到所有机器 (master 和 node）的/etc/kubernetes/⽬录下，分发到node节点上⾮必需；</li><li>重新⽣成 bootstrap.kubeconfig ⽂件，分发到所有 node 机器的/etc/kubernetes/⽬录下；</li><li>重启 kube-apiserver 和 kubelet 进程；</li><li>重新 approve kubelet 的 csr 请求；</li></ol><p>!!!注意，需要将token.csv文件复制到另外两台服务器，不要重新生成token.csv文件，文件目录是之前的ssl文件的上一级目录/etc/kubernetes</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp token.csv /etc/kubernetes/</span><br></pre></td></tr></table></figure><h2 id="创建服务需要的kubeconfig文件"><a href="#创建服务需要的kubeconfig文件" class="headerlink" title="创建服务需要的kubeconfig文件"></a>创建服务需要的kubeconfig文件</h2><p>在开启了 TLS 的集群中，每当与集群交互的时候少不了的是身份认证，使用kubeconfig（即证书） 和 token 两种认证方式是最简单也最通用的认证方式。</p><p><font color="red" size="6">总之kubeconfig就是为访问集群所作的配置。</font></p><h3 id="创建-kubelet服务所需-bootstrapping-kubeconfig文件"><a href="#创建-kubelet服务所需-bootstrapping-kubeconfig文件" class="headerlink" title="创建 kubelet服务所需 bootstrapping.kubeconfig文件"></a>创建 kubelet服务所需 bootstrapping.kubeconfig文件</h3><p>主要是用于生成的bootstrap.kubeconfig提供给kubelet进行node注册<br>bootstrap.kubeconfig.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/kubernetes</span><br><span class="line">export KUBE_APISERVER=&quot;https://192.168.137.13:9443&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置集群参数</span></span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">--certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">--embed-certs=true \</span><br><span class="line">--server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">--kubeconfig=bootstrap.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置客户端认证参数</span></span><br><span class="line">kubectl config set-credentials kubelet-bootstrap \</span><br><span class="line">--token=$&#123;BOOTSTRAP_TOKEN&#125; \</span><br><span class="line">--kubeconfig=bootstrap.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置上下文参数</span></span><br><span class="line">kubectl config set-context default \</span><br><span class="line">--cluster=kubernetes \</span><br><span class="line">--user=kubelet-bootstrap \</span><br><span class="line">--kubeconfig=bootstrap.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置默认上下文</span></span><br><span class="line">kubectl config use-context default --kubeconfig=bootstrap.kubeconfig</span><br></pre></td></tr></table></figure><ul><li>  上述也用到了变量BOOTSTRAP_TOKEN，所以kubeconfig生成后也是复制到其他服务</li><li>  –embed-certs 为 true 时表示将 certificate-authority 证书写⼊到⽣成的bootstrap.kubeconfig⽂件中；</li><li>  设置客户端认证参数时没有指定秘钥和证书，后续由 kubeapiserver⾃动⽣成；</li></ul><h3 id="创建-kubelet服务所需-kubelet-kubeconfig文件"><a href="#创建-kubelet服务所需-kubelet-kubeconfig文件" class="headerlink" title="创建 kubelet服务所需 kubelet.kubeconfig文件"></a>创建 kubelet服务所需 kubelet.kubeconfig文件</h3><p>这里我们直接使用/root/.kube/config文件，也可以自己建立kubelet证书后来生成</p><h3 id="创建-kube-controller-manager服务所需kube-controller-manager-kubeconfig-⽂件"><a href="#创建-kube-controller-manager服务所需kube-controller-manager-kubeconfig-⽂件" class="headerlink" title="创建 kube-controller-manager服务所需kube-controller-manager.kubeconfig ⽂件"></a>创建 kube-controller-manager服务所需kube-controller-manager.kubeconfig ⽂件</h3><p>kube-controller-manager.kubeconfig.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/kubernetes</span><br><span class="line">export KUBE_APISERVER=&quot;https://192.168.137.13:9443&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置集群参数</span></span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">--certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">--embed-certs=true \</span><br><span class="line">--server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">--kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置客户端认证参数</span></span><br><span class="line">kubectl config set-credentials system:kube-controller-manager \</span><br><span class="line">--client-certificate=/etc/kubernetes/ssl/kube-controller-manager.pem \</span><br><span class="line">--client-key=/etc/kubernetes/ssl/kube-controller-manager-key.pem \</span><br><span class="line">--embed-certs=true \</span><br><span class="line">--kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置上下文参数</span></span><br><span class="line">kubectl config set-context system:kube-controller-manager@kubernetes \</span><br><span class="line">--cluster=kubernetes \</span><br><span class="line">--user=system:kube-controller-manager \</span><br><span class="line">--kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置默认上下文</span></span><br><span class="line">kubectl config use-context system:kube-controller-manager@kubernetes --kubeconfig=kube-controller-manager.kubeconfig</span><br></pre></td></tr></table></figure><h3 id="创建-kube-scheduler服务所需kube-scheduler-kubeconfig-⽂件"><a href="#创建-kube-scheduler服务所需kube-scheduler-kubeconfig-⽂件" class="headerlink" title="创建 kube-scheduler服务所需kube-scheduler.kubeconfig ⽂件"></a>创建 kube-scheduler服务所需kube-scheduler.kubeconfig ⽂件</h3><p>kube-scheduler.kubeconfig.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/kubernetes</span><br><span class="line">export KUBE_APISERVER=&quot;https://192.168.137.13:9443&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置集群参数</span></span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">--certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">--embed-certs=true \</span><br><span class="line">--server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">--kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置客户端认证参数</span></span><br><span class="line">kubectl config set-credentials system:kube-scheduler \</span><br><span class="line">--client-certificate=/etc/kubernetes/ssl/kube-scheduler.pem \</span><br><span class="line">--client-key=/etc/kubernetes/ssl/kube-scheduler-key.pem \</span><br><span class="line">--embed-certs=true \</span><br><span class="line">--kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置上下文参数</span></span><br><span class="line">kubectl config set-context system:kube-scheduler@kubernetes \</span><br><span class="line">--cluster=kubernetes \</span><br><span class="line">--user=system:kube-scheduler \</span><br><span class="line">--kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置默认上下文</span></span><br><span class="line">kubectl config use-context system:kube-scheduler@kubernetes --kubeconfig=kube-scheduler.kubeconfig</span><br></pre></td></tr></table></figure><h3 id="执行脚本文件生成kubeconfig文件"><a href="#执行脚本文件生成kubeconfig文件" class="headerlink" title="执行脚本文件生成kubeconfig文件"></a>执行脚本文件生成kubeconfig文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">source /root/bootstrap.kubeconfig.sh</span><br><span class="line">source /root/kube-controller-manager.kubeconfig.sh</span><br><span class="line">source /root/kube-scheduler.kubeconfig.sh</span><br></pre></td></tr></table></figure><h3 id="分发-kubeconfig-⽂件"><a href="#分发-kubeconfig-⽂件" class="headerlink" title="分发 kubeconfig ⽂件"></a>分发 kubeconfig ⽂件</h3><p>将 kubeconfig ⽂件分发到其他服务器的 /etc/kubernetes/ ⽬录，具体怎么分发自己操作</p><h1 id="Etcd3-3-4"><a href="#Etcd3-3-4" class="headerlink" title="Etcd3.3.4"></a>Etcd3.3.4</h1><p><font color="red" size="3">三台服务器都要执行</font></p><h2 id="TLS-认证⽂件"><a href="#TLS-认证⽂件" class="headerlink" title="TLS 认证⽂件"></a>TLS 认证⽂件</h2><p>需要为 etcd 集群创建加密通信的 TLS 证书，这⾥复⽤以前创建的kubernetes 证书</p><p>ca.pem kubernetes-key.pem kubernetes.pem</p><p>kubernetes 证书的 hosts 字段列表中包含上⾯三台服务器的IP，否则后续证书校验会失败；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> wget https://github.com/coreos/etcd/releases/download/v3.3.4/etcd-v3.3.4-linux-amd64.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar zxf etcd-v3.3.4-linux-amd64.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mv etcd-v3.3.4-linux-amd64/etcd* /usr/<span class="built_in">local</span>/bin/</span></span><br></pre></td></tr></table></figure><h2 id="ectd服务services配置文件"><a href="#ectd服务services配置文件" class="headerlink" title="ectd服务services配置文件"></a>ectd服务services配置文件</h2><p>/usr/lib/systemd/system/etcd.service</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line"></span><br><span class="line">Description=Etcd Server</span><br><span class="line"></span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">After=network-online.target</span><br><span class="line"></span><br><span class="line">Wants=network-online.target</span><br><span class="line"></span><br><span class="line">Documentation=https://github.com/coreos</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line"></span><br><span class="line">Type=notify</span><br><span class="line"></span><br><span class="line">WorkingDirectory=/var/lib/etcd/</span><br><span class="line"></span><br><span class="line">EnvironmentFile=-/etc/etcd/etcd.conf</span><br><span class="line"></span><br><span class="line">ExecStart=/usr/local/bin/etcd </span><br><span class="line"></span><br><span class="line">--name $&#123;ETCD_NAME&#125; </span><br><span class="line"></span><br><span class="line">--cert-file=/etc/kubernetes/ssl/kubernetes.pem </span><br><span class="line"></span><br><span class="line">--key-file=/etc/kubernetes/ssl/kubernetes-key.pem </span><br><span class="line"></span><br><span class="line">--peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem </span><br><span class="line"></span><br><span class="line">--peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem </span><br><span class="line"></span><br><span class="line">--trusted-ca-file=/etc/kubernetes/ssl/ca.pem </span><br><span class="line"></span><br><span class="line">--peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem </span><br><span class="line"></span><br><span class="line">--initial-advertise-peer-urls $&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; </span><br><span class="line"></span><br><span class="line">--listen-peer-urls $&#123;ETCD_LISTEN_PEER_URLS&#125; </span><br><span class="line"></span><br><span class="line">--listen-client-urls $&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 </span><br><span class="line"></span><br><span class="line">--advertise-client-urls $&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; </span><br><span class="line"></span><br><span class="line">--initial-cluster-token $&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; </span><br><span class="line"></span><br><span class="line">--initial-cluster-infra1=https://192.168.137.10:2380,infra2=https://192.168.137.11:2380,infra3=https://192.168.137.12:2380 </span><br><span class="line"></span><br><span class="line">--initial-cluster-state new </span><br><span class="line"></span><br><span class="line">--data-dir=$&#123;ETCD_DATA_DIR&#125;</span><br><span class="line"></span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">RestartSec=5</span><br><span class="line"></span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line"></span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>  指定 etcd 的⼯作⽬录为 /var/lib/etcd ，数据⽬录为/var/lib/etcd，需在启动服务前创建这两个⽬录；</li><li>  为了保证通信安全，需要指定 etcd 的公私钥(cert-file和key-file)、Peers通信的公私钥和 CA证书(peer-cert-file、peer-key-file、peertrusted-ca-file)、客户端的CA证书（trusted-ca-file)；</li><li>  创建 kubernetes.pem 证书时使⽤的 kubernetes-csr.json ⽂件的hosts字段包含所有 <strong>etcd</strong> 节点的<strong>IP</strong>，否则证书校验会出错；</li><li>  –initial-cluster-state 值为 new 时， –name 的参数值必须位于–initial-cluster 列表中；<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /var/lib/etcd</span><br><span class="line">mkdir /etc/etcd</span><br></pre></td></tr></table></figure></li></ul><h2 id="环境变量配置⽂件"><a href="#环境变量配置⽂件" class="headerlink" title="环境变量配置⽂件"></a>环境变量配置⽂件</h2><p>/etc/etcd/etcd.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> [member]</span></span><br><span class="line">ETCD_NAME=infra1</span><br><span class="line">ETCD_DATA_DIR=&quot;/var/lib/etcd&quot;</span><br><span class="line">ETCD_LISTEN_PEER_URLS=&quot;https://192.168.137.10:2380&quot;</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.137.10:2379&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash">[cluster]</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.137.10:2380&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=https://192.168.137.10:2379</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这是192.168.137.10节点的配置，其他两个etcd节点只要将上⾯的IP地址改成相应节点的IP地址即可。ETCD_NAME换成对应节点的infra1/2/3。<br>/etc/etcd/etcd.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> [member]</span></span><br><span class="line">ETCD_NAME=infra2</span><br><span class="line">ETCD_DATA_DIR=&quot;/var/lib/etcd&quot;</span><br><span class="line">ETCD_LISTEN_PEER_URLS=&quot;https://192.168.137.11:2380&quot;</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.137.11:2379&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash">[cluster]</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.137.11:2380&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=https://192.168.137.11:2379</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> [member]</span></span><br><span class="line">ETCD_NAME=infra3</span><br><span class="line">ETCD_DATA_DIR=&quot;/var/lib/etcd&quot;</span><br><span class="line">ETCD_LISTEN_PEER_URLS=&quot;https://192.168.137.12:2380&quot;</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.137.12:2379&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash">[cluster]</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.137.12:2380&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.137.12:2379&quot;</span><br></pre></td></tr></table></figure><h2 id="启动etcd服务"><a href="#启动etcd服务" class="headerlink" title="启动etcd服务"></a>启动etcd服务</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable etcd</span><br><span class="line">systemctl start etcd</span><br></pre></td></tr></table></figure><p>验证</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">etcdctl \</span><br><span class="line">--ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">--cert-file=/etc/kubernetes/ssl/kubernetes.pem \</span><br><span class="line">--key-file=/etc/kubernetes/ssl/kubernetes-key.pem \</span><br><span class="line">cluster-health</span><br></pre></td></tr></table></figure><p><font color="red" size="6">结果最后⼀⾏为 cluster is healthy 时表示集群服务正常。</font></p><h1 id="kube-apiserver"><a href="#kube-apiserver" class="headerlink" title="kube-apiserver"></a>kube-apiserver</h1><h2 id="创建-kube-apiserver的服务配置⽂件"><a href="#创建-kube-apiserver的服务配置⽂件" class="headerlink" title="创建 kube-apiserver的服务配置⽂件"></a>创建 kube-apiserver的服务配置⽂件</h2><p>/usr/lib/systemd/system/kube-apiserver.service</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes API Service</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=network.target</span><br><span class="line">After=etcd.service</span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/apiserver</span><br><span class="line">ExecStart=/usr/local/bin/kube-apiserver \</span><br><span class="line"><span class="meta">$</span><span class="bash">KUBE_API_ARGS</span></span><br><span class="line">Restart=on-failure</span><br><span class="line">Type=notify</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h2 id="apiserver服务配置⽂件-etc-kubernetes-apiserver-内容"><a href="#apiserver服务配置⽂件-etc-kubernetes-apiserver-内容" class="headerlink" title="apiserver服务配置⽂件 /etc/kubernetes/apiserver 内容"></a>apiserver服务配置⽂件 /etc/kubernetes/apiserver 内容</h2><p>此处是主机名api的服务器配置，另外两台服务器修改其中的IP地址即可。<br>/etc/kubernetes/apiserver</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">KUBE_API_ARGS=&quot;--advertise-address=192.168.137.10 \</span><br><span class="line">    --allow-privileged=true \</span><br><span class="line">    --client-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">    --disable-admission-plugins=PersistentVolumeLabel \</span><br><span class="line">--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \</span><br><span class="line">--authorization-mode=RBAC,Node \</span><br><span class="line">--token-auth-file=/etc/kubernetes/token.csv \</span><br><span class="line">    --enable-bootstrap-token-auth=true \</span><br><span class="line">    --etcd-cafile=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">    --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \</span><br><span class="line">    --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \</span><br><span class="line">    --etcd-servers=https://192.168.137.10:2379,https://192.168.137.11:2379,https://192.168.137.12:2379 \</span><br><span class="line">    --insecure-port=0 \</span><br><span class="line">    --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \</span><br><span class="line">    --secure-port=6443 \</span><br><span class="line">    --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \</span><br><span class="line">    --service-cluster-ip-range=10.254.0.0/16 \</span><br><span class="line">    --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem  \</span><br><span class="line">    --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \</span><br><span class="line">    --kubelet-client-certificate=/etc/kubernetes/ssl/admin.pem \</span><br><span class="line">    --kubelet-client-key=/etc/kubernetes/ssl/admin-key.pem\</span><br><span class="line">    --proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.pem \</span><br><span class="line">    --proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client-key.pem \</span><br><span class="line">    --requestheader-allowed-names=front-proxy-client \</span><br><span class="line">    --requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem \</span><br><span class="line">    --requestheader-extra-headers-prefix=X-Remote-Extra- \</span><br><span class="line">    --requestheader-group-headers=X-Remote-Group \</span><br><span class="line">    --requestheader-username-headers=X-Remote-User \</span><br><span class="line">    --v=2 \</span><br><span class="line">    --logtostderr=true \</span><br><span class="line">    --audit-log-maxage=30  \</span><br><span class="line">     --audit-log-maxbackup=3  \</span><br><span class="line">     --audit-log-maxsize=100  \</span><br><span class="line">     --audit-log-path=/var/log/kubernetes/audit.log  \</span><br><span class="line">     --audit-policy-file=/etc/kubernetes/audit-policy.yml  \</span><br><span class="line">     --experimental-encryption-provider-config=/etc/kubernetes/encryption.yaml  \</span><br><span class="line">     --event-ttl=1h&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>  –authorization-mode=Node,RBAC： 开启 Node 和 RBAC授权模式，拒绝未授权的请求；</li><li>  –enable-admission-plugins：启用 ServiceAccount 和 NodeRestriction；</li><li>  –service-account-key-file：签名 ServiceAccount Token的公钥文件，kube-controller-manager 的 –service-account-private-key-file指定私钥文件，两者配对使用；</li><li>  –tls-*-file：指定 apiserver 使用的证书、私钥和 CA 文件。–client-ca-file用于验证 client (kue-controller-manager、kube-scheduler、kubelet、kube-proxy等)请求所带的证书；</li><li>  –kubelet-client-certificate、–kubelet-client-key：如果指定，则使用 https访问 kubelet APIs；需要为 kubernete 用户定义 RBAC 规则，否则无权访问 kubeletAPI；</li><li>  –service-cluster-ip-range： 指定 Service Cluster IP 地址段；</li><li>  –service-node-port-range： 指定 NodePort 的端口范围；</li><li>  –runtime-config=api/all=true： 启用所有版本的 APIs，如autoscaling/v2alpha1；</li><li>  –enable-bootstrap-token-auth：启用 kubelet bootstrap 的 token 认证；</li><li>  –kubelet-client-certificate=/etc/kubernetes/ssl/admin.pem这里我为什么不重新生成kubelet的证书呢，因为后面安装kubelet的时候使用的kubeconfig就是将admin用户生成给kubectl使用的kubeconfig（即.kube/config文件）复制给他使用了，所以这里直接使用admin证书，当然你也可以去生成kubelet证书。</li><li>  –kubelet-client-key=/etc/kubernetes/ssl/admin-key.pem，同上述</li><li>–proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.pem<br>  –proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client-key.pem<br>  –requestheader-allowed-names=front-proxy-client<br>  –requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem<br>  kube-proxy客户端使用证书。 如果不指定这些文件，将会无法exec进去pod和无法logs –f 查看pod 日志</li><li>–requestheader-allowed-names=front-proxy-client<br>  –requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem<br>  –requestheader-extra-headers-prefix=X-Remote-Extra-<br>  –requestheader-group-headers=X-Remote-Group<br>  –requestheader-username-headers=X-Remote-User<br>  用于支持metrics-server，后续将会讲解。</li><li>  –audit-policy-file=/etc/kubernetes/audit-policy.yml ,文件内容如下<br>参考地址：<a href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/etc/kubernetes/audit-policy.yml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/etc/kubernetes/audit-policy.yml</a></li><li>–experimental-encryption-provider-config：启用加密特性；文件生成如下<br>参考地址：<a href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/etc/kubernetes/encryption.yaml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/etc/kubernetes/encryption.yaml</a><br>  生成 EncryptionConfig 所需的加密 key<br>文件下载地址：<a href="https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/etc/kubernetes/encryption-source.yaml">https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/etc/kubernetes/encryption-source.yaml</a><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">下载后将其更名为encryption.yaml</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mv encryption-source.yaml encryption.yaml</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ENCRYPT_SECRET=$( head -c 32 /dev/urandom | base64 )</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sed -ri <span class="string">&quot;/secret:/s#(: ).+#\1<span class="variable">$&#123;ENCRYPT_SECRET&#125;</span>#&quot;</span> encryption.yaml</span></span><br><span class="line"> </span><br></pre></td></tr></table></figure></li><li>  node1的apiserver服务配置文件只需要修改此处–advertise-address=192.168.137.11即可；</li><li>  node2的apiserver服务配置文件只需要修改此处–advertise-address=192.168.137.12即可；</li></ul><h2 id="启动kube-apiserver"><a href="#启动kube-apiserver" class="headerlink" title="启动kube-apiserver"></a>启动kube-apiserver</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-apiserver</span><br><span class="line">systemctl start kube-apiserver</span><br><span class="line">systemctl status kube-apiserver</span><br></pre></td></tr></table></figure><h1 id="kube-controller-manager"><a href="#kube-controller-manager" class="headerlink" title="kube-controller-manager"></a>kube-controller-manager</h1><p><font color="red" size="3">三台服务器都要执行</font></p><h2 id="创建-kube-controller-manager服务配置⽂件"><a href="#创建-kube-controller-manager服务配置⽂件" class="headerlink" title="创建 kube-controller-manager服务配置⽂件"></a>创建 kube-controller-manager服务配置⽂件</h2><p>/usr/lib/systemd/system/kube-controller-manager.service</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line"></span><br><span class="line">Description=Kubernetes Controller Manager</span><br><span class="line"></span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line"></span><br><span class="line">EnvironmentFile=-/etc/kubernetes/controller-manager</span><br><span class="line"></span><br><span class="line">ExecStart=/usr/local/bin/kube-controller-manager </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash">KUBE_CONTROLLER_MANAGER_ARGS</span></span><br><span class="line"></span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line"></span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h2 id="controller-manager服务配置⽂件-etc-kubernetes-controller-manager"><a href="#controller-manager服务配置⽂件-etc-kubernetes-controller-manager" class="headerlink" title="controller-manager服务配置⽂件 /etc/kubernetes/controller-manager "></a>controller-manager服务配置⽂件 /etc/kubernetes/controller-manager </h2><p>/etc/kubernetes/controller-manager</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">KUBE_CONTROLLER_MANAGER_ARGS=&quot;--address=0.0.0.0 \</span><br><span class="line">    --allocate-node-cidrs=true \</span><br><span class="line">    --cluster-cidr=10.10.0.0/16 \</span><br><span class="line">    --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">    --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  \</span><br><span class="line">    --controllers=*,bootstrapsigner,tokencleaner \</span><br><span class="line">    --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span><br><span class="line">    --leader-elect=true \</span><br><span class="line">    --node-cidr-mask-size=24 \</span><br><span class="line">    --root-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">    --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \</span><br><span class="line">--use-service-account-credentials=true &quot;</span><br></pre></td></tr></table></figure><ul><li>  –address=0.0.0.0此处为了方便后续的prometheus-operator访问服务端口设置为所有网卡可以访问，之前设置的是127.0.0.1</li><li>  –allocate-node-cidrs=true，您的Kubernetes控制器管理器配置为分配pod CIDR（即通过传递–allocate-node-cidrs=true给控制器管理器）</li><li>  –cluster-cidr=10.10.0.0/16，您的Kubernetes控制器管理器已经提供了一个cluster-cidr（即通过传递–cluster-cidr=10.10.0.0/16，默认情况下清单需要）。</li></ul><h2 id="启动-kube-controller-manager"><a href="#启动-kube-controller-manager" class="headerlink" title="启动 kube-controller-manager"></a>启动 kube-controller-manager</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-controller-manager</span><br><span class="line">systemctl start kube-controller-manager</span><br><span class="line">systemctl status kube-controller-manager</span><br></pre></td></tr></table></figure><h1 id="kube-scheduler"><a href="#kube-scheduler" class="headerlink" title="kube-scheduler"></a>kube-scheduler</h1><p><font color="red" size="3">三台服务器都要执行</font></p><h2 id="创建-kube-scheduler服务配置⽂件"><a href="#创建-kube-scheduler服务配置⽂件" class="headerlink" title="创建 kube-scheduler服务配置⽂件"></a>创建 kube-scheduler服务配置⽂件</h2><p>/usr/lib/systemd/system/kube-scheduler.service</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line"></span><br><span class="line">Description=Kubernetes Scheduler Plugin</span><br><span class="line"></span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line"></span><br><span class="line">EnvironmentFile=-/etc/kubernetes/scheduler</span><br><span class="line"></span><br><span class="line">ExecStart=/usr/local/bin/kube-scheduler </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash">KUBE_SCHEDULER_ARGS</span></span><br><span class="line"></span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line"></span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line"></span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h2 id="scheduler服务配置⽂件-etc-kubernetes-scheduler"><a href="#scheduler服务配置⽂件-etc-kubernetes-scheduler" class="headerlink" title="scheduler服务配置⽂件 /etc/kubernetes/scheduler "></a>scheduler服务配置⽂件 /etc/kubernetes/scheduler </h2><p>/etc/kubernetes/scheduler</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">KUBE_SCHEDULER_ARGS=&quot;--address=0.0.0.0 \</span><br><span class="line">    --leader-elect=true \</span><br><span class="line">--kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>  –address=0.0.0.0此处为了方便后续的prometheus-operator访问服务端口设置为所有网卡可以访问，之前设置的是127.0.0.1</li></ul><h2 id="启动-kube-scheduler"><a href="#启动-kube-scheduler" class="headerlink" title="启动 kube-scheduler"></a>启动 kube-scheduler</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-scheduler</span><br><span class="line">systemctl start kube-scheduler</span><br><span class="line">systemctl status kube-scheduler</span><br></pre></td></tr></table></figure><h1 id="至此Master三大组件安装完成"><a href="#至此Master三大组件安装完成" class="headerlink" title="至此Master三大组件安装完成"></a>至此Master三大组件安装完成</h1><p>查看运行状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for i in kube-apiserver kube-controller-manager kube-scheduler; do systemctl restart $i ; done</span><br><span class="line">for i in kube-apiserver kube-controller-manager kube-scheduler; do systemctl status $i -l ; done</span><br></pre></td></tr></table></figure><p>验证集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl cluster-info</span></span><br><span class="line">Kubernetes master is running at https://192.168.137.13:9443</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get cs</span></span><br><span class="line">NAME STATUS MESSAGE ERROR</span><br><span class="line">scheduler Healthy ok</span><br><span class="line">controller-manager Healthy ok</span><br><span class="line">etcd-0 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br><span class="line">etcd-1 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br><span class="line">etcd-2 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br></pre></td></tr></table></figure><h1 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h1><p>Docker从1.13版本开始调整了默认的防火墙规则，禁用了iptables filter表中FOWARD链，这样会引起Kubernetes集群中跨Node的Pod无法通信，因此docker安装完成后，还需要手动修改iptables规则。</p><p><font color="red" size="3">三台服务器都要执行</font></p><h2 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line">yum makecache fast</span><br><span class="line">yum install -y docker-ce</span><br></pre></td></tr></table></figure><h2 id="编辑systemctl的Docker启动文件"><a href="#编辑systemctl的Docker启动文件" class="headerlink" title="编辑systemctl的Docker启动文件"></a>编辑systemctl的Docker启动文件</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &quot;13i ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT&quot; /usr/lib/systemd/system/docker.service</span><br></pre></td></tr></table></figure><h2 id="修改docker参数"><a href="#修改docker参数" class="headerlink" title="修改docker参数"></a>修改docker参数</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/docker/</span><br><span class="line"><span class="meta">cat&gt;</span><span class="bash">/etc/docker/daemon.json&lt;&lt;<span class="string">EOF</span></span></span><br><span class="line">&#123;</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https://fz5yth0r.mirror.aliyuncs.com&quot;],</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;storage-opts&quot;: [</span><br><span class="line">    &quot;overlay2.override_kernel_check=true&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;100m&quot;,</span><br><span class="line">    &quot;max-file&quot;: &quot;3&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="启动docker"><a href="#启动docker" class="headerlink" title="启动docker"></a>启动docker</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable docker</span><br><span class="line">systemctl start docker</span><br></pre></td></tr></table></figure><h1 id="建立不加密的docker私有仓库方式"><a href="#建立不加密的docker私有仓库方式" class="headerlink" title="建立不加密的docker私有仓库方式"></a>建立不加密的docker私有仓库方式</h1><h2 id="建立最简单的私库"><a href="#建立最简单的私库" class="headerlink" title="建立最简单的私库"></a>建立最简单的私库</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --restart=always --name dockerregistry \</span><br><span class="line"> -p 5000:5000 \</span><br><span class="line"> -v /huisebug/dockerimagestorehouse/registry:/var/lib/registry \</span><br><span class="line"> -v /huisebug/dockerimagestorehouse/config.yml:/etc/docker/registry/config.yml  registry</span><br></pre></td></tr></table></figure><p>config.yml内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">version: 0.1</span><br><span class="line">log:</span><br><span class="line">  fields:</span><br><span class="line">    service: registry</span><br><span class="line">storage:</span><br><span class="line">  delete:</span><br><span class="line">    enabled: true</span><br><span class="line">  cache:</span><br><span class="line">    blobdescriptor: inmemory</span><br><span class="line">  filesystem:</span><br><span class="line">    rootdirectory: /var/lib/registry</span><br><span class="line">http:</span><br><span class="line">  addr: :5000</span><br><span class="line">  headers:</span><br><span class="line">    X-Content-Type-Options: [nosniff]</span><br><span class="line">health:</span><br><span class="line">  storagedriver:</span><br><span class="line">    enabled: true</span><br><span class="line">    interval: 10s</span><br><span class="line">    threshold: 3</span><br></pre></td></tr></table></figure><p>更改docker的service文件增加–insecure-registry 192.168.137.10:5000<br>ExecStart=/usr/bin/dockerd –insecure-registry 192.168.137.10:5000</p><p>重启docker</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure><h1 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h1><p><font color="red" size="3">三台服务器都要执行</font></p><h2 id="创建-kubelet服务配置⽂件"><a href="#创建-kubelet服务配置⽂件" class="headerlink" title="创建 kubelet服务配置⽂件"></a>创建 kubelet服务配置⽂件</h2><p> /usr/lib/systemd/system/kubelet.service</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line"></span><br><span class="line">Description=Kubernetes Kubelet Server</span><br><span class="line"></span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">After=docker.service</span><br><span class="line"></span><br><span class="line">Requires=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line"></span><br><span class="line">WorkingDirectory=/var/lib/kubelet</span><br><span class="line"></span><br><span class="line">EnvironmentFile=-/etc/kubernetes/kubelet</span><br><span class="line"></span><br><span class="line">ExecStart=/usr/local/bin/kubelet \</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash">KUBELET_ARGS</span></span><br><span class="line"></span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line"></span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>创建工作目录，需要手动创建</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /var/lib/kubelet</span><br></pre></td></tr></table></figure><h2 id="kubelet服务配置文件-etc-kubernetes-kubelet"><a href="#kubelet服务配置文件-etc-kubernetes-kubelet" class="headerlink" title="kubelet服务配置文件/etc/kubernetes/kubelet"></a>kubelet服务配置文件/etc/kubernetes/kubelet</h2><p>/etc/kubernetes/kubelet</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">KUBELET_ARGS=&quot;--bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><br><span class="line">--kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><br><span class="line">--config=/var/lib/kubelet/config.yaml \</span><br><span class="line">--cni-bin-dir=/opt/cni/bin \</span><br><span class="line">--cni-conf-dir=/etc/cni/net.d \</span><br><span class="line">--network-plugin=cni&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>  bootstrap.kubeconfig之前我们已经建立了。kubelet 使⽤该⽂件中的⽤户名和 token向 kube-apiserver 发送 TLS Bootstrapping 请求；</li><li>  –kubeconfig=/etc/kubernetes/kubelet.kubeconfig 中指定的 kubelet.kubeconfig⽂件在第⼀次启动kubelet之前并不存在，请看下⽂，当通过CSR请求后会⾃动⽣成kubelet.kubeconfig ⽂件，如果你的节点上已经⽣成了 ~/.kube/config⽂件，你可以将该⽂件拷⻉到该路径下，并重命名为 kubelet.kubeconfig，所有node节点可以共⽤同⼀个kubelet.kubeconfig⽂件，这样新添加的节点就不需要再创建CSR请求就能⾃动添加到kubernetes集群中。同样，在任意能够访问到kubernetes集群的主机上使⽤kubectl –kubeconfig 命令操作集群时，只要使⽤ ~/.kube/config⽂件就可以通过权限认证，因为这⾥⾯已经有认证信息并认为你是admin⽤户，对集群拥有所有权限。<br>执行以下命令<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp -rf /root/.kube/config /etc/kubernetes/kubelet.kubeconfig</span><br></pre></td></tr></table></figure></li></ul><p><font color="red" size="3">并且分发到所有安装kubelet的服务器</font></p><ul><li>  –config=/var/lib/kubelet/config.yaml中声明了kubelet服务的端口信息等，参考地址：<a href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/var/lib/kubelet/config.yaml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/var/lib/kubelet/config.yaml</a></li><li>–cni-bin-dir=/opt/cni/bin<br> –cni-conf-dir=/etc/cni/net.d<br> –network-plugin=cni，你有一个Kubernetes集群配置为使用CNI网络插件（即通过传递–network-plugin=cni给kubelet）</li></ul><p>建立文件目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /etc/kubernetes/manifests</span><br><span class="line">echo &quot; mkdir /etc/kubernetes/manifests&quot; &gt;&gt; /etc/rc.local</span><br></pre></td></tr></table></figure><p><font color="red" size="6">!!!注意</font></p><ul><li>  hostPort不适用于CNI</li><li>  使用<em>hostPort</em>和CNI插件的组合将导致Kubernetes静默忽略<em>hostPort</em>属性。</li></ul><h2 id="启动kubelet"><a href="#启动kubelet" class="headerlink" title="启动kubelet"></a>启动kubelet</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kubelet</span><br><span class="line">systemctl start kubelet</span><br><span class="line">systemctl status kubelet</span><br></pre></td></tr></table></figure><h2 id="检查node状态"><a href="#检查node状态" class="headerlink" title="检查node状态"></a>检查node状态</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node</span></span><br><span class="line">NAME STATUS ROLES AGE VERSION</span><br><span class="line">api.huisebug.com NotReady &lt;none&gt; 6m8s v1.12.4</span><br><span class="line">node1.huisebug.com NotReady &lt;none&gt; 4s v1.12.4</span><br><span class="line">node2.huisebug.com NotReady &lt;none&gt; 1s v1.12.4</span><br></pre></td></tr></table></figure><p>可以看到状态为NotReady 因为缺少cni配置文件而导致kubelet服务（systemctl status kubelet -l）在日志中提示找不到配置而无法变为Ready</p><p>我们可以将/etc/cni/net.d/下的配置文件暂时建立10-calico.conflist，使状态变为Ready。后续再做修改或者删除。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p /etc/cni/net.d/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vim /etc/cni/net.d/10-calico.conflist</span></span><br></pre></td></tr></table></figure><p>参考下载地址：<br><a href="https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/etc/cni/net.d/10-calico.conflist">https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/etc/cni/net.d/10-calico.conflist</a></p><p>再次查看状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node</span></span><br><span class="line">NAME STATUS ROLES AGE VERSION</span><br><span class="line">api.huisebug.com Ready &lt;none&gt; 34m v1.12.4</span><br><span class="line">node1.huisebug.com Ready &lt;none&gt; 28m v1.12.4</span><br><span class="line">node2.huisebug.com Ready &lt;none&gt; 28m v1.12.4</span><br></pre></td></tr></table></figure><h1 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h1><p>这里我们采用daemonsets方式建立</p><p>这里的代理模式使用的是ipvs，不再使用iptables，并且ipvs依赖于nf_conntrack_ipv4，所以需要将服务器的内核目录挂载到pod中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 建立serviceaccount</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl -n kube-system create serviceaccount kube-proxy</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 建立集群角色绑定</span></span><br><span class="line">kubectl create clusterrolebinding system:kube-proxy \</span><br><span class="line">--clusterrole system:node-proxier \</span><br><span class="line">--serviceaccount kube-system:kube-proxy</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 建立daemonsets方式建立的pod和需要的参数configmap，参考地址:</span></span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/kube-proxy/kube-proxy.yaml</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 建立集群的第一个pod，会需要基础pod镜像，每台服务器执行一遍。拉取基础镜像。</span></span><br><span class="line">docker pull huisebug/sec_re:pause3.1 &amp;&amp; docker tag huisebug/sec_re:pause3.1 k8s.gcr.io/pause:3.1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看状态</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod --all-namespaces -o wide</span></span><br><span class="line">![](media/3bf05a168dbd6d583e0c0d5dbb65b226.png)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">通过ipvsadm查看 proxy 规则</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ipvsadm -ln</span></span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> RemoteAddress:Port Forward Weight ActiveConn InActConn</span></span><br><span class="line">TCP 10.254.0.1:443 rr</span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> 192.168.137.10:6443 Masq 1 0 0</span></span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> 192.168.137.11:6443 Masq 1 0 0</span></span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> 192.168.137.12:6443 Masq 1 0 0</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 确认使用ipvs模式</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> curl localhost:10249/proxyMode</span></span><br><span class="line">ipvs</span><br></pre></td></tr></table></figure><h1 id="至此Node三大组件安装完成"><a href="#至此Node三大组件安装完成" class="headerlink" title="至此Node三大组件安装完成"></a>至此Node三大组件安装完成</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">查看状态</span><br><span class="line">for i in etcd kube-apiserver kube-controller-manager kube-scheduler kubelet docker; do systemctl status $i -l; done</span><br><span class="line"></span><br><span class="line">重启所有二进制安装的</span><br><span class="line">for i in etcd kube-apiserver kube-controller-manager kube-scheduler kubelet docker; do systemctl restart $i; done</span><br></pre></td></tr></table></figure><h1 id="Calico"><a href="#Calico" class="headerlink" title="Calico"></a>Calico</h1><h2 id="环境要求，原打算是使用3-4-0版本，奈何失败了。"><a href="#环境要求，原打算是使用3-4-0版本，奈何失败了。" class="headerlink" title="环境要求，原打算是使用3.4.0版本，奈何失败了。"></a>环境要求，原打算是使用3.4.0版本，奈何失败了。</h2><ul><li>  您的Kubernetes控制器管理器配置为分配pod CIDR（即通过传递–allocate-node-cidrs=true给控制器管理器）</li><li>  您的Kubernetes控制器管理器已经提供了一个cluster-cidr（即通过传递–cluster-cidr=10.10.0.0/16，默认情况下清单需要）。</li><li>  你有一个Kubernetes集群配置为使用CNI网络插件（即通过传递–network-plugin=cni给kubelet）<blockquote><p>  我犯了一个错误就是将cluster-cidr和cluster-range-ip配置成相同了，修改后如果出现pod无法建立，<br>  那么就将etcd的数据目录清空，然后重新启动所有服务，让apiserver重新请求etcd并写入数据。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /var/lib/etcd/*</span><br><span class="line">for i in etcd kube-apiserver kube-controller-manager kube-scheduler kubelet docker; do systemctl restart $i ; done</span><br></pre></td></tr></table></figure>参考我的yaml文件,calico3.1.3<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export interface=ens33</span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Calico/calico3.1.3node-ctl.yaml</span><br></pre></td></tr></table></figure></li><li>  记得上面要将cid地址修改为你的cidr地址</li><li>  记得查看interface名称，系统不同网卡名称也不同</li></ul><p>执行后检测pod建立情况<br><img src="/2020/08/26/k8s/media/6ffc2f19054e181ed137799a29bd742e.png"><br>检测网卡情况<br><img src="/2020/08/26/k8s/media/e94fcb31b73adab699fc0580db9a037b.png"><br>会发现缺少cali*开头的网卡名称，那是我们现在还没建立过不使用hostNetwork: true的pod，我们这里使用建立一个普通的nginx pod</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f https://kubernetes.io/docs/user-guide/nginx-deployment.yaml --record</span><br></pre></td></tr></table></figure><p>再次查看网卡信息<br><img src="/2020/08/26/k8s/media/27573e66d33decbf6a61f9f972f9ce11.png"></p><h2 id="验证不同节点的容器之间能否ping通"><a href="#验证不同节点的容器之间能否ping通" class="headerlink" title="验证不同节点的容器之间能否ping通"></a>验证不同节点的容器之间能否ping通</h2><p><img src="/2020/08/26/k8s/media/c45fba5757f980bff79e58f73ff0456c.png"></p><h2 id="验证能否访问外网"><a href="#验证能否访问外网" class="headerlink" title="验证能否访问外网"></a>验证能否访问外网</h2><p>建立一个测试工具pod，busybox需要一个持续输出，这里我将网关地址赋予它</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl run busybox --image=busybox --command -- ping 192.168.137.1</span><br><span class="line">kubectl exec -it busybox-759d8dbd98-tf9hd ping 61.139.2.69</span><br><span class="line">kubectl exec -it busybox-759d8dbd98-tf9hd nslookup baidu.com 61.139.2.69</span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/dc12719300e4ac7400b7b62617a2ce72.png"></p><p>至此calico安装完成。</p><h2 id="问题汇总"><a href="#问题汇总" class="headerlink" title="问题汇总"></a>问题汇总</h2><p>如果kubectl get node一直是notready，</p><p>执行下面步骤</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Calico/calico3.1.3node-ctl.yaml</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">三台服务器都执行</span></span><br><span class="line">rm -rf /etc/cni/net.d/*</span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Calico/calico3.1.3node-ctl.yaml</span><br></pre></td></tr></table></figure><h1 id="Coredns"><a href="#Coredns" class="headerlink" title="Coredns"></a>Coredns</h1><p>参考地址,记得将其中的集群ip地址修改为你在kubelet中定义的地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Coredns/coredns.yaml</span><br></pre></td></tr></table></figure><h2 id="验证效果"><a href="#验证效果" class="headerlink" title="验证效果"></a>验证效果</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">查看是否成功建立coredns pod</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod --all-namespaces -o wide 查看是否成功建立coredns pod</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc</span></span><br><span class="line">NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</span><br><span class="line">kubernetes ClusterIP 10.254.0.1 &lt;none&gt; 443/TCP 16m</span><br><span class="line"></span><br><span class="line">可以看到default命名空间下现在就一个service</span><br><span class="line">将之前建立的nginx-deployment暴露为service</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl expose deploy nginx-deployment</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc</span></span><br><span class="line">NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</span><br><span class="line">kubernetes ClusterIP 10.254.0.1 &lt;none&gt; 443/TCP 16m</span><br><span class="line">nginx-deployment ClusterIP 10.254.136.63 &lt;none&gt; 80/TCP 3s</span><br></pre></td></tr></table></figure><p>测试能否解析成对应的IP地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -it nginx-deployment-5c689d88bb-2mzx4 ping nginx-deployment</span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/82e7d16fd70b1d760463183eeafd14c6.png"></p><p>测试不同命名空间</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc --all-namespaces</span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/0080be720975ed9265171ad7919a5b22.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -it nginx-deployment-5c689d88bb-2mzx4 ping calico-typha.kube-system</span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/172614abc7d8086efdaa1aa3e170edbb.png"></p><p>至此，coredns安装完成。</p><h1 id="再次验证整个集群的proxy负载机制效果"><a href="#再次验证整个集群的proxy负载机制效果" class="headerlink" title="再次验证整个集群的proxy负载机制效果"></a>再次验证整个集群的proxy负载机制效果</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -ln</span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/23eea17b92a059fc23388c2672365cb2.png"></p><h1 id="Helm"><a href="#Helm" class="headerlink" title="Helm"></a>Helm</h1><p>相当于centos的yum，Ubuntu的apt-get命令</p><p>此处安装的helm是2.12.2版本，之前的2.6版本在安装后续的chart会出现：</p><p>Error: parse error in *** function “genCA” not defined 错误</p><h2 id="Helm-client安装"><a href="#Helm-client安装" class="headerlink" title="Helm client安装"></a>Helm client安装</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 下载</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> wget https://storage.googleapis.com/kubernetes-helm/helm-v2.12.2-linux-amd64.tar.gz</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 解包并将二进制文件helm拷贝到/usr/<span class="built_in">local</span>/bin目录下</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar -zxvf helm-v2.12.2-linux-amd64.tar.gz &amp;&amp; mv linux-amd64/helm /usr/<span class="built_in">local</span>/bin/helm</span></span><br></pre></td></tr></table></figure><h2 id="安装socat"><a href="#安装socat" class="headerlink" title="安装socat"></a>安装socat</h2><p><font color="red" size="3">用于端口转发，在准备初始环境安装keepalived已经安装，必须在所有Node服务器安装</font></p><h2 id="Helm-server安装"><a href="#Helm-server安装" class="headerlink" title="Helm server安装"></a>Helm server安装</h2><p>创建tiller的 serviceaccount 和 clusterrolebinding</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl create serviceaccount --namespace kube-system tiller</span><br><span class="line">kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller</span><br></pre></td></tr></table></figure><p>然后安装helm服务端tiller<br>这里因为一些原因，使用的是阿里云的镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.12.2 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</span><br></pre></td></tr></table></figure><p>修改serviceAccount ：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl patch deploy -n kube-system tiller-deploy -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;serviceAccount&quot;:&quot;tiller&quot;&#125;&#125;&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure><p>检查是否安装成功，等待一段时间后。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> helm version</span></span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/f858112ba11661d4a205598f9c0a426f.png"></p><h1 id="Traefik"><a href="#Traefik" class="headerlink" title="Traefik"></a>Traefik</h1><p>k8s集群中的http反向代理服务</p><p>直接下载官方的charts，然后找到traefik就可以执行安装了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> &lt;https://github.com/helm/charts.git&gt;</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> charts/stable/traefik</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 执行默认配置，显然是不符合我这里的需求。</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> helm install .</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 列出helm安装的资源类型</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> helm ls</span></span><br><span class="line">NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE</span><br><span class="line">gilded-skunk 1 Fri Jan 18 10:36:42 2019 DEPLOYED traefik-1.59.0 1.7.6 default</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">删除helm安装的资源类型</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> helm delete gilded-skunk --purge</span></span><br><span class="line">release &quot;gilded-skunk&quot; deleted</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装我想要的一些功能</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认的template目录下的deployment.yaml文件没有开启hostNetwork: <span class="literal">true</span>，执行下面语句开启（主要是我这儿环境配置了hostPort不生效，（因为CNI插件的原因）配置hostNetwork就生效）</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /root/charts/stable/traefik/templates</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> Lnum=$(sed -n <span class="string">&#x27;/spec/=&#x27;</span> deployment.yaml |sed -n <span class="string">&quot;2&quot;</span>p)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下面的格式为sed -ie 单引单引双引变量名双引单引a六个空格hostNetwork: <span class="literal">true</span>单引 deployment.yaml</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sed -ie &amp;<span class="comment">#39;&amp;#39;&amp;#34;$Lnum&amp;#34;&amp;#39;a&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;hostNetwork: true&amp;#39;  deployment.yaml</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">中间是6个空格，不能少也不能多</span></span><br></pre></td></tr></table></figure><p>也可以参考我已经修改好的charts，参考地址：<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/Traefik">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/Traefik</a><br>参考地址仅做参考，因为官方charts是不断更新的，或许后续会不断添加新功能，例如后续可能会添加支持hostNetwork: true功能，推荐还是根据官方charts来设置。</p><h2 id="安装traefik"><a href="#安装traefik" class="headerlink" title="安装traefik"></a>安装traefik</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> helm delete traefik --purge</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> helm install --name traefik --namespace kube-system \</span></span><br><span class="line"><span class="bash">--<span class="built_in">set</span> replicas=3,cpuLimit=1000m,memoryLimit=1Gi,rbac.enabled=<span class="literal">true</span>,\</span></span><br><span class="line"><span class="bash">dashboard.enabled=<span class="literal">true</span>,dashboard.domain=traefik.huisebug.com,\</span></span><br><span class="line"><span class="bash">metrics.prometheus.enabled=<span class="literal">true</span> \</span></span><br><span class="line"><span class="bash">/root/charts/stable/traefik</span></span><br></pre></td></tr></table></figure><p>确保服务已经运行<br><img src="/2020/08/26/k8s/media/9e90faf47ed6c8b79f1a213d8f1865ad.png"><br><img src="/2020/08/26/k8s/media/12dffb5c85ff5fea46833c01e4a7a070.png"></p><p>重启服务器后如果traefik没有正确启动，那就删除后重新建立。</p><h2 id="验证效果-1"><a href="#验证效果-1" class="headerlink" title="验证效果"></a>验证效果</h2><p>之前我们验证coredns时候将一个nginx暴露为service，现在建立一个ingress来对应这个service</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc --all-namespaces</span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/76bd44213cfe4012bf900395b9497aa7.png"></p><p>参考yaml地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Traefik/nginx-ingress.yaml</span><br></pre></td></tr></table></figure><p>在本地解析文件中追加域名解析<br>192.168.137.10 traefik.huisebug.com nginx-deployment.huisebug.com</p><p>访问验证效果<br><img src="/2020/08/26/k8s/media/1089f58ff64fe2e1987d9d72af840686.png"><br><img src="/2020/08/26/k8s/media/bacc338b37613e8ee8c525253c1c6a42.png"></p><p>至此，traefik安装完成</p><h1 id="Dashboard"><a href="#Dashboard" class="headerlink" title="Dashboard"></a>Dashboard</h1><p>会额外建立一个名称为anonymous-dashboard-proxy的 Cluster Role(Binding)来让system:anonymous这个匿名使用者能够通过 API Server 来 proxy 到 KubernetesDashboard,而这个 RBAC规则仅能够存取services/proxy资源,以及https:kubernetes-dashboard:资源名称同时在1.7 版本以后的 Dashboard 将不再提供所有权限,因此需要建立一个 service account来绑定 cluster-admin role</p><p>具体yaml参考</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Dashboard/dashboard.yaml</span><br></pre></td></tr></table></figure><h2 id="访问验证"><a href="#访问验证" class="headerlink" title="访问验证"></a>访问验证</h2><ul><li>  方式一：可以通过浏览器读取Dashboard</li><li>  方式二：ingress既然上述我们已经搭建了traefik，所以我们建立一个ingress来访问。</li></ul><h3 id="直接https访问方式"><a href="#直接https访问方式" class="headerlink" title="直接https访问方式"></a>直接https访问方式</h3><h4 id="导⼊证书"><a href="#导⼊证书" class="headerlink" title="导⼊证书"></a>导⼊证书</h4><p>将⽣成的admin.pem证书转换格式（/etc/kubernetes/ssl目录下）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/kubernetes/ssl</span><br><span class="line">openssl pkcs12 -export -in admin.pem -out admin.p12 -inkey admin-key.pem</span><br></pre></td></tr></table></figure><p>将⽣成的 admin.p12<br>证书导⼊的你的电脑的浏览器，导出的时候记住你设置的密码，导⼊的时候还要⽤到。</p><h4 id="访问"><a href="#访问" class="headerlink" title="访问"></a>访问</h4><p>这里不能写成VIP+代理端口来跳转，我们三台服务器都安装了api-server，可以任意如下一个访问</p><p><a href="https://192.168.137.10:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/">https://192.168.137.10:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</a></p><p><a href="https://192.168.137.11:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/">https://192.168.137.11:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</a></p><p><a href="https://192.168.137.12:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/">https://192.168.137.12:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</a></p><p>成功向浏览器导入证书后访问成功效果</p><p><img src="/2020/08/26/k8s/media/778d54e157730e8122dfc3f5ae727361.png"></p><p>使用令牌（token）进行访问</p><p>令牌获取。</p><p>$ kubectl -n kube-system describe secrets | sed -rn &#34;/sdashboard-token-/,/^token/{/^token/s#S+s+##p}&#34;</p><p><img src="/2020/08/26/k8s/media/49791fb732caf260cf373180c5f08891.png"></p><p>将token值复制到令牌一栏即可成功访问dashboard</p><p><img src="/2020/08/26/k8s/media/de528414d705dc0816e623fb1c5725d4.png"></p><p><img src="/2020/08/26/k8s/media/1eeb76fe6060a353e0fd0548d47b94e4.png"></p><h3 id="ingress方式"><a href="#ingress方式" class="headerlink" title="ingress方式"></a>ingress方式</h3><p>参考yaml文件地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Dashboard/kubernetes-dashboard-ingress.yaml</span><br></pre></td></tr></table></figure><p>这里我们需要开启traefik的https功能，并且开启ssl.insecureSkipVerify=true跳过验证SSL连接上的证书，如果不开启此处，就无法使用ingress进行访问。此处是相当于使用了https连接，没有进行证书验证。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">helm delete traefik --purge</span><br><span class="line">helm install --name traefik --namespace kube-system \</span><br><span class="line">--set imageTag=1.6.5,replicas=3,\</span><br><span class="line">cpuLimit=1000m,memoryLimit=1Gi,rbac.enabled=true,\</span><br><span class="line">dashboard.enabled=true,dashboard.domain=traefik.huisebug.com,\</span><br><span class="line">metrics.prometheus.enabled=true,\</span><br><span class="line">ssl.enabled=true,ssl.insecureSkipVerify=true \</span><br><span class="line">/root/charts/stable/traefik</span><br></pre></td></tr></table></figure><p>如果出现traefik出现问题就降低配置中的traefik镜像版本。</p><h4 id="验证访问效果"><a href="#验证访问效果" class="headerlink" title="验证访问效果"></a>验证访问效果</h4><p><img src="/2020/08/26/k8s/media/1a75a341166a1a3c3fc59aff8d3b53c1.png"></p><h1 id="Scope监控"><a href="#Scope监控" class="headerlink" title="Scope监控"></a>Scope监控</h1><p>用于监控整个k8s集群的网络TOP</p><h2 id="安装scope"><a href="#安装scope" class="headerlink" title="安装scope"></a>安装scope</h2><p>直接使用官方yaml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f &quot;https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d &#x27;\n&#x27;)&quot;</span><br></pre></td></tr></table></figure><p>注意上面的namespace是weave</p><h2 id="暴露访问两种方式"><a href="#暴露访问两种方式" class="headerlink" title="暴露访问两种方式"></a>暴露访问两种方式</h2><h3 id="Service的nodeport方式"><a href="#Service的nodeport方式" class="headerlink" title="Service的nodeport方式"></a>Service的nodeport方式</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">wget https://cloud.weave.works/k8s/scope.yaml</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改service的值</span></span><br><span class="line">    spec:</span><br><span class="line">      type: NodePort</span><br><span class="line">      ports:</span><br><span class="line">        - name: app</span><br><span class="line">          port: 80</span><br><span class="line">          protocol: TCP</span><br><span class="line">          targetPort: 4040</span><br><span class="line">          nodePort: 30040</span><br></pre></td></tr></table></figure><p>注意：Nodeport只能暴露30000到32767，不然会报错如下：</p><p>The Service “weave-scope-app” is invalid: spec.ports[0].nodePort: Invalid value:<br>4040: provided port is not in the valid range. The range of valid ports is<br>30000-32767</p><h3 id="Traefik代理"><a href="#Traefik代理" class="headerlink" title="Traefik代理"></a>Traefik代理</h3><p>编写一个ingress</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">参考yaml地址</span><br><span class="line">kubectl create -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Scope/scope-ingress.yaml</span><br></pre></td></tr></table></figure><h2 id="验证效果-2"><a href="#验证效果-2" class="headerlink" title="验证效果"></a>验证效果</h2><p><img src="/2020/08/26/k8s/media/3be2bb4c074a23be29d02276b0ce7390.png"></p><h1 id="EFK"><a href="#EFK" class="headerlink" title="EFK"></a>EFK</h1><p>此栏目已经迁移到<a href="https://huisebug.github.io/2019/05/08/gluster-heketi-efk/">gluster-heketi-efk</a></p><h1 id="Prometheus-operator"><a href="#Prometheus-operator" class="headerlink" title="Prometheus-operator "></a>Prometheus-operator </h1><p>此栏目已经迁移到<a href="https://huisebug.github.io/2019/08/27/Prometheus-Operator/">Prometheus-operator</a> </p><h1 id="Nginx-ingress"><a href="#Nginx-ingress" class="headerlink" title="Nginx-ingress"></a>Nginx-ingress</h1><p>同样的是整个集群的反向代理，可支持四层代理</p><p>此处安装是建立在prometheus-operator的基础上的。</p><p>直接下载官方的charts，然后找到traefik就可以执行安装了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone &lt;https://github.com/helm/charts.git&gt;</span><br></pre></td></tr></table></figure><p>详细参数介绍请参考官方地址<br><a href="https://github.com/helm/charts/tree/master/stable/nginx-ingress#configuration">https://github.com/helm/charts/tree/master/stable/nginx-ingress#configuration</a></p><p>具体的镜像下载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker pull mirrorgooglecontainers/defaultbackend:1.4</span><br><span class="line">docker tag mirrorgooglecontainers/defaultbackend:1.4 k8s.gcr.io/defaultbackend:1.4</span><br><span class="line"></span><br><span class="line">docker pull huisebug/sec_re:nginx-ingress-controller-0.21.0</span><br><span class="line">docker tag huisebug/sec_re:nginx-ingress-controller-0.21.0 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0</span><br></pre></td></tr></table></figure><h2 id="安装nginx-ingress"><a href="#安装nginx-ingress" class="headerlink" title="安装nginx-ingress"></a>安装nginx-ingress</h2><p>首先需要删除traefik</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">helm delete traefik --purge</span><br><span class="line">helm delete nginx-ingress --purge</span><br></pre></td></tr></table></figure><p>官方的可配置参数各种坑不断，不推荐用命令行参数，直接修改values.yaml比较方便。下面是我修改的参数值：</p><ul><li>  部署类型为daemonset</li><li>  开启hostNetwork功能</li><li>  资源配额</li><li>  更改service type： LoadBalancer为ClusterIP</li><li>  rbac开启</li><li>  serviceAccount名称为nginx-ingress-controller</li><li>  开启metrics</li><li>  开启servicemonitor，设置namespace为monitoring便于prometheus-operator监控</li></ul><p>部署<br>参考我的valuesdaemonset.yaml文件地址：<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/nginx-ingress/valuesdaemonset.yaml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/nginx-ingress/valuesdaemonset.yaml</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helm delete nginx-ingress --purge</span><br><span class="line">helm install --name nginx-ingress --namespace kube-system /root/charts/stable/nginx-ingress </span><br><span class="line">-f /root/charts/stable/nginx-ingress/valuesdaemonset.yaml</span><br></pre></td></tr></table></figure><h2 id="验证效果-3"><a href="#验证效果-3" class="headerlink" title="验证效果"></a>验证效果</h2><p><img src="/2020/08/26/k8s/media/341628f03517a9fa7797699c65852ce4.png"></p><p>访问prometheus查看是否添加了nginx-ingress-controller的metrics</p><p><img src="/2020/08/26/k8s/media/28a8d1fc8edc5fad645b1dada6f8cc4d.png"></p><p>如何验证默认后端（nginx-ingress-default-backend）效果呢？</p><p>首先我们在本地hosts文件中建立一个整个k8s集群中没有定义ingress，不存在的域名并指向nginx-ingress-controller，访问测试即可</p><p><img src="/2020/08/26/k8s/media/58ed9d31362103f188537195c5a8dda3.png"></p><p>访问测试</p><p><img src="/2020/08/26/k8s/media/b6e69420019de9c1fcdc79edb8fc1af3.png"></p><h6 id><a href="#" class="headerlink" title></a></h6><p>也可以参考我已经修改好的charts，参考地址：<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/nginx-ingress">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/nginx-ingress</a></p><h6 id="-1"><a href="#-1" class="headerlink" title></a></h6><h2 id="nginx-ingress-controller-nginx功能"><a href="#nginx-ingress-controller-nginx功能" class="headerlink" title="nginx-ingress-controller nginx功能"></a>nginx-ingress-controller nginx功能</h2><p>nginx-ingress-controller 并不像traefik一样提供WEB界面功能</p><p>nginx-ingress-controller-ingress.yaml<br>参考地址：<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/nginx-ingress/ingress.yaml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/nginx-ingress/ingress.yaml</a></p><p>验证效果，这个返回的页面是nginx-ingress-controller返回的，并不是后端返回的</p><p><img src="/2020/08/26/k8s/media/c8bc815bef8e22e91dbb8e2779559c28.png"></p><h2 id="nginx-ingress-controller的四层代理"><a href="#nginx-ingress-controller的四层代理" class="headerlink" title="nginx-ingress-controller的四层代理"></a>nginx-ingress-controller的四层代理</h2><p>nginx从1.9.0开始，新增加了一个stream模块，用来实现四层协议的转发、代理或者负载均衡等。可以配置TCP或者UDP来实现这个功能</p><h3 id="代理集群的coredns"><a href="#代理集群的coredns" class="headerlink" title="代理集群的coredns"></a>代理集群的coredns</h3><p>只需要在values.yaml文件中增加UDP协议端口：coredns的service名：service_IP</p><p><img src="/2020/08/26/k8s/media/0c70598ca20e83a5f36f4444db4bdec4.png"></p><p>重新建立nginx-ingress-controller后，查看是否成功开启UDP 53端口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -lnup | grep 53</span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/821127b3b80f30705b270f49349d209f.png"></p><p>集群完整的域名如何获取？</p><p>我们使用一个pod去ping另一个pod的service名称，kube-dns会解析出完整的域名，就可以修改其中的service名和对应的namespace即可</p><p><img src="/2020/08/26/k8s/media/82e7d16fd70b1d760463183eeafd14c6.png"></p><p>验证效果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nslookup -qt=A kubernetes.default.svc.cluster.local 192.168.137.13</span><br><span class="line">nslookup -qt=A kube-dns.kube-system.svc.cluster.local 192.168.137.13</span><br><span class="line">nslookup -qt=A prometheus-k8s.monitoring.svc.cluster.local 192.168.137.13</span><br></pre></td></tr></table></figure><p>windows CMD验证</p><p><img src="/2020/08/26/k8s/media/7671a9d3a5beb90673783537c32d0cf7.png"></p><p>linux 验证</p><p><img src="/2020/08/26/k8s/media/e08988c2ffa569bce75b7972fcb2fac8.png"></p><h1 id="Metrics-server"><a href="#Metrics-server" class="headerlink" title="Metrics-server"></a>Metrics-server</h1><p>kubernetes Metrics Server是资源使用数据的集群范围聚合器，是Heapster的后继者。度量服务器通过汇集来自kubernetes.summary_api的数据来收集节点和pod的CPU和内存使用情况。摘要API是一种内存高效的API，用于将数据从Kubelet / cAdvisor传递到度量服务器。</p><p>从 v1.8 开始，资源使用情况的度量（如容器的 CPU 和内存使用）可以通过 Metrics API获取。注意：</p><ul><li>  Metrics API 只可以查询当前的度量数据，并不保存历史数据</li><li>  Metrics API URI 为 /apis/metrics.k8s.io/，在 k8s.io/metrics 维护</li><li>  必须部署 metrics-server 才能使用该 API，metrics-server 通过调用 Kubelet Summary API 获取数据</li><li>  在新版本的kubernetes中 Pod CPU使用率不在来源于heapster,而是来自于metrics-server</li><li>  支持metrics-server必须在api-server中添加如下参数</li></ul><p>设置apiserver相关参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">--requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem </span><br><span class="line">--proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.pem </span><br><span class="line">--proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client-key.pem </span><br><span class="line">--requestheader-allowed-names=aggregator </span><br><span class="line">--requestheader-group-headers=X-Remote-Group </span><br><span class="line">--requestheader-extra-headers-prefix=X-Remote-Extra- </span><br><span class="line">--requestheader-username-headers=X-Remote-User </span><br></pre></td></tr></table></figure><h2 id="部署metrics-server"><a href="#部署metrics-server" class="headerlink" title="部署metrics-server"></a>部署metrics-server</h2><p>此处将metrics-server放到prometheus-operator后面是因为在prometheus-operator建立的时候就建立了一个以prometheus-adapter为支持的<strong>api：metrics.k8s.io</strong>，这个api建立后就可以使用kubectl top命令了，但是prometheus-operator方式建立的<strong>api：metrics.k8s.io</strong>仅仅支持kubectltop pod，并不支持kubectl top node。<br>为了后续的<strong>api：custom.metrics.k8s.io</strong>使用prometheus-adapter来作服务支撑和使用metrics-server来建立<strong>api：metrics.k8s.io</strong>，我们需要移除prometheus-operator以prometheus-adapter为支持的<strong>api：metrics.k8s.io</strong></p><p>将prometheus-adapter的所有yaml文件归档到一个文件目录中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir prometheus-adapter</span><br><span class="line">mv prometheus-adapter-* prometheus-adapter/</span><br><span class="line">kubectl delete -f prometheus-adapter/</span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/1eb6a6f5a131ff6287126ff61259575d.png"></p><p>然后部署metrics-server，具体部署yaml参考地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Metrics-server1.12/Metrics-server1.12.yaml</span><br></pre></td></tr></table></figure><h2 id="验证效果-4"><a href="#验证效果-4" class="headerlink" title="验证效果"></a>验证效果</h2><p>获取命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl api-versions | grep metrics.k8s.io</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl top pod --all-namespaces</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl top node</span></span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/12031d891104780e561b9742041e8b3f.png"></p><p>查看nodes metrics：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get --raw &quot;/apis/metrics.k8s.io/v1beta1/nodes&quot; | jq .</span><br></pre></td></tr></table></figure><p><img src="/2020/08/26/k8s/media/d213f882d29ff203408d0021dcbc860c.png"></p><h1 id="HPA"><a href="#HPA" class="headerlink" title="HPA"></a>HPA</h1><p>此栏目已经迁移到<a href="https://huisebug.github.io/2019/08/28/k8s-hpa/">k8s-HPA</a> </p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这是一个k8s集群，从零开始搭建服务器环境，keepalived+haproxy实现集群高可用VIP；glusterfs搭建实现可持续存储；k8s1.12版本集群；efk日志系统；prometheus-operator告警系统；HPA v2横向pod扩容；k8s1.11+以上的版本部署基本没什么区别，无非就是优化了一些参数的配置和增加一些功能，舍弃一些api，此文档适用后续发布的其他版本的部署，提供一些部署思路。&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes" scheme="https://huisebug.github.io/categories/Kubernetes/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="hpav2" scheme="https://huisebug.github.io/tags/hpav2/"/>
    
    <category term="hpa" scheme="https://huisebug.github.io/tags/hpa/"/>
    
  </entry>
  
  <entry>
    <title>Jenkins持续集成到Kubernetes集群</title>
    <link href="https://huisebug.github.io/2020/06/03/jenkins-CICD-k8s/"/>
    <id>https://huisebug.github.io/2020/06/03/jenkins-CICD-k8s/</id>
    <published>2020-06-03T09:04:01.000Z</published>
    <updated>2023-04-20T05:41:46.718Z</updated>
    
    <content type="html"><![CDATA[<p>这是一个k8s集群结合jenkins做持续集成，以及jenkins流水线在k8s集群中运行。java编译后、nodejs编译后docker镜像制作。</p><span id="more"></span><h1 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h1><p>K8s集群1.15.10</p><p>Docker-ce 19.03</p><p>nfs卷</p><p>服务器信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">192.168.137.100:k8s-master/k8s-node</span><br><span class="line">192.168.137.101:k8s-master/k8s-node</span><br><span class="line">192.168.137.102:k8s-master/k8s-node</span><br><span class="line">192.168.137.5:nfs服务、docker私有镜像仓库</span><br></pre></td></tr></table></figure><h1 id="nfs服务部署"><a href="#nfs服务部署" class="headerlink" title="nfs服务部署"></a>nfs服务部署</h1><p>nfsinstall.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">docker run -d --restart=always --name nfs_server --privileged -p 2049:2049 \</span><br><span class="line">-v /data/nfs:/nfsroot \</span><br><span class="line">-h nfsserver \</span><br><span class="line">-e SHARED_DIRECTORY=/nfsroot \</span><br><span class="line">huisebug/nfs-server:latest</span><br></pre></td></tr></table></figure><p>执行脚本即可在服务器上部署一个nfs服务</p><h1 id="docker私有镜像仓库"><a href="#docker私有镜像仓库" class="headerlink" title="docker私有镜像仓库"></a>docker私有镜像仓库</h1><p>创建私有仓库带有简单的密码认证,用户名admin，密码huisebug</p><p>dockerregistryinstall.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">dockerregistryinstall()&#123;</span><br><span class="line">mkdir -p /data/dockerregistry/auth</span><br><span class="line">docker run --entrypoint htpasswd registry:2 -Bbn admin huisebug  &gt; /data/dockerregistry/auth/htpasswd</span><br><span class="line"></span><br><span class="line">docker run -d -p 80:5000 --restart=always --name registry \</span><br><span class="line">   -v /data/dockerregistry/auth/:/auth \</span><br><span class="line">   -e &quot;REGISTRY_AUTH=htpasswd&quot; \</span><br><span class="line">   -e &quot;REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm&quot; \</span><br><span class="line">   -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \</span><br><span class="line">   -v /data/dockerregistry/data:/var/lib/registry \</span><br><span class="line">   registry:2</span><br><span class="line">&#125;</span><br><span class="line">dockerregistryinstall</span><br></pre></td></tr></table></figure><h1 id="Jenkins部署"><a href="#Jenkins部署" class="headerlink" title="Jenkins部署"></a>Jenkins部署</h1><h2 id="新建jenkins命名空间"><a href="#新建jenkins命名空间" class="headerlink" title="新建jenkins命名空间"></a>新建jenkins命名空间</h2><p>jenkins-ns.yaml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">   name: jenkins</span><br><span class="line">   labels:</span><br><span class="line">     name: jenkins</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="持久化存储jenkins和maven"><a href="#持久化存储jenkins和maven" class="headerlink" title="持久化存储jenkins和maven"></a>持久化存储jenkins和maven</h2><p>准备好2个nfs数据卷用于jenkins持久化和maven仓库持久化（此处我是使用1个，采用不同目录进行区分）</p><p>因为jenkins的kubernetes插件功能不是很完善，所以这里的pvc在pod中是不支持subpath的，所以需要在pv的path下写好对应的路径</p><p>登录到nfs所在机器192.168.137.5,建立pv所需的路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /data/nfs</span><br><span class="line">mkdir -p jenkins/home</span><br><span class="line">mkdir -p jenkins/maven</span><br></pre></td></tr></table></figure><h3 id="jenkins-pv-pvc"><a href="#jenkins-pv-pvc" class="headerlink" title="jenkins-pv-pvc"></a>jenkins-pv-pvc</h3><p>jenkins-pv-pvc.yaml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins</span><br><span class="line">  labels:</span><br><span class="line">    app: jenkins</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 20Gi</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  persistentVolumeReclaimPolicy: Retain</span><br><span class="line">  volumeMode: Filesystem</span><br><span class="line">  nfs:</span><br><span class="line">    server: 192.168.137.5</span><br><span class="line">    path: /jenkins/home</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: jenkins</span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: jenkins</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 20Gi</span><br><span class="line">  storageClassName: &quot;&quot;</span><br><span class="line">  volumeMode: Filesystem</span><br><span class="line">  volumeName: jenkins</span><br></pre></td></tr></table></figure><h3 id="maven-pv-pvc"><a href="#maven-pv-pvc" class="headerlink" title="maven-pv-pvc"></a>maven-pv-pvc</h3><p>maven-pv-pvc.yaml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: maven</span><br><span class="line">  labels:</span><br><span class="line">    app: maven</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 20Gi</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  nfs:</span><br><span class="line">    server: 192.168.137.5</span><br><span class="line">    path: /jenkins/maven</span><br><span class="line">  persistentVolumeReclaimPolicy: Retain</span><br><span class="line">  volumeMode: Filesystem</span><br><span class="line">---</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: maven</span><br><span class="line">  name: maven</span><br><span class="line">  namespace: jenkins</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 20Gi</span><br><span class="line">  storageClassName: &quot;&quot;</span><br><span class="line">  volumeMode: Filesystem</span><br><span class="line">  volumeName: maven</span><br></pre></td></tr></table></figure><h2 id="Jenkins-rbac"><a href="#Jenkins-rbac" class="headerlink" title="Jenkins-rbac"></a>Jenkins-rbac</h2><p>jenkins需要建立pod权限以调用k8s集群建立jenkins构建节点pod</p><p>jenkins-rbac.yaml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: jenkins</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: jenkins</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;extensions&quot;, &quot;apps&quot;]</span><br><span class="line">    resources: [&quot;deployments&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;, &quot;delete&quot;, &quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;patch&quot;, &quot;update&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;services&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;, &quot;delete&quot;, &quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;patch&quot;, &quot;update&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;pods&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;pods/exec&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;pods/log&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;,&quot;list&quot;,&quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;secrets&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;]</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: jenkins</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: jenkins</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: jenkins</span><br></pre></td></tr></table></figure><h2 id="Jenkins"><a href="#Jenkins" class="headerlink" title="Jenkins"></a>Jenkins</h2><p>jenkins.yaml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: jenkins</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: jenkins</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 10</span><br><span class="line">      serviceAccount: jenkins</span><br><span class="line">      containers:</span><br><span class="line">      - name: jenkins</span><br><span class="line">        image: jenkins/jenkins:lts</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">          name: web</span><br><span class="line">          protocol: TCP</span><br><span class="line">        - containerPort: 50000</span><br><span class="line">          name: agent</span><br><span class="line">          protocol: TCP</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1000m</span><br><span class="line">            memory: 1Gi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 500m</span><br><span class="line">            memory: 512Mi</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /login</span><br><span class="line">            port: 8080</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">          failureThreshold: 12 </span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /login</span><br><span class="line">            port: 8080</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">          failureThreshold: 12</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: jenkinshome</span><br><span class="line">          subPath: jenkins</span><br><span class="line">          mountPath: /var/jenkins_home</span><br><span class="line">        env:</span><br><span class="line">        - name: LIMITS_MEMORY</span><br><span class="line">          valueFrom:</span><br><span class="line">            resourceFieldRef:</span><br><span class="line">              resource: limits.memory</span><br><span class="line">              divisor: 1Mi</span><br><span class="line">        - name: JAVA_OPTS</span><br><span class="line">          value: -Xmx$(LIMITS_MEMORY)m -XshowSettings:vm -Dhudson.slaves.NodeProvisioner.initialDelay=0 -Dhudson.slaves.NodeProvisioner.MARGIN=50 -Dhudson.slaves.NodeProvisioner.MARGIN0=0.85 -Duser.timezone=Asia/Shanghai</span><br><span class="line">      securityContext:</span><br><span class="line">        fsGroup: 1000</span><br><span class="line">      volumes:</span><br><span class="line">      - name: jenkinshome</span><br><span class="line">        persistentVolumeClaim:</span><br><span class="line">          claimName: jenkins</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: jenkins</span><br><span class="line">  labels:</span><br><span class="line">    app: jenkins</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    app: jenkins</span><br><span class="line">  ports:</span><br><span class="line">  - name: web</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: web</span><br><span class="line">    nodePort: 30000</span><br><span class="line">  - name: agent</span><br><span class="line">    port: 50000</span><br><span class="line">    targetPort: agent</span><br></pre></td></tr></table></figure><p>注意事项：</p><p>如果jenkins提示权限问题</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/5662511e2ac0f43bd196847ced4cf757.png"></p><p>因为jenkins镜像中使用的账户是jenkins，而非root用户，所以需要将jenkins使用的pv下的path路径加上jenkins中提交的subpath重新授权</p><p>登录到nfs所在机器192.168.137.5</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /data/nfs/jenkins/home</span><br><span class="line">chown -R 1000 jenkins/</span><br></pre></td></tr></table></figure><p><img src="/2020/06/03/jenkins-CICD-k8s/media/c4d5c6811ce0bf8b6cf9ee70bc3b69b0.png"></p><p>授权后重新部署jenkins</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -f Jenkins.yaml &amp;&amp; kubectl apply -f Jenkins.yaml</span><br></pre></td></tr></table></figure><h2 id="get-k8s-admin-cert"><a href="#get-k8s-admin-cert" class="headerlink" title="get_k8s_admin_cert"></a>get_k8s_admin_cert</h2><p>jenkins构建节点的docker镜像内置了kubectl客户端，用于job完成打包到部署的流程，所以需要执行get_k8s_admin_cert.sh脚本获取k8s管理员证书建立secret</p><p>因为jenkins的kubernetes插件功能不是很完善，所以这里的secret的key不能单独指定</p><p>get_k8s_admin_cert.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">建立jenkins构建节点pod中kubectl客户端所需的k8s管理员证书</span></span><br><span class="line">kubectl -n jenkins create secret generic  k8sadmin --from-file=/root/.kube/config</span><br></pre></td></tr></table></figure><h1 id="验证访问"><a href="#验证访问" class="headerlink" title="验证访问"></a>验证访问</h1><p>访问<a href="http://192.168.137.101:30000/">http://192.168.137.101:30000</a></p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/9a868befb4f79a4e93891051dfcaeabc.png"></p><p>准备完毕</p><h1 id="Jenkins配置"><a href="#Jenkins配置" class="headerlink" title="Jenkins配置"></a>Jenkins配置</h1><h2 id="安装kubernetes插件"><a href="#安装kubernetes插件" class="headerlink" title="安装kubernetes插件"></a>安装kubernetes插件</h2><p>因为网络原因需要将jenkins代理设置位置：<a href="http://jenkins访问url/pluginManager/advanced">http://jenkins访问url/pluginManager/advanced</a></p><p>将升级站点处的url地址修改为：<a href="http://mirror.xmission.com/jenkins/updates/current/update-center.json">http://mirror.xmission.com/jenkins/updates/current/update-center.json</a></p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/0c4b674450b636a558782580d1a9c3f9.png"></p><p>然后点击提交</p><p>切换到插件管理进行插件安装</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/20d9bd3e6f8985401213b8ab9b108451.png"></p><p>等待安装完毕即可</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/5b77afb8002eb93c0e995894a2fee144.png"></p><h2 id="配置kubernetes节点信息"><a href="#配置kubernetes节点信息" class="headerlink" title="配置kubernetes节点信息"></a>配置kubernetes节点信息</h2><p>系统管理–节点管理—Configure Clouds</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/534ec5d1523e40cb87661456ac900d86.png"></p><p>安装了kubernetes插件这里就会出现kubernetes可选项</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/8cbab7e483c0b48ccc1da1ceada128d8.png"></p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/3aebc8174620eaa633a1aadcef5f6b1d.png"></p><h2 id="Kubernetes-Cloud-details配置"><a href="#Kubernetes-Cloud-details配置" class="headerlink" title="Kubernetes Cloud details配置"></a>Kubernetes Cloud details配置</h2><p>点击Kubernetes Cloud details按钮即可开始配置</p><p>名称：kubernetes</p><p>kubernetes地址：<a href="https://kubernetes.default.svc.cluster.local/">https://kubernetes.default.svc.cluster.local</a></p><p>kubernetes命名空间：jenkins 即 jenkins服务所在的命名空间namespace</p><p>配置完毕后点击连接测试，查看是否可以连接成功</p><p>jenkins地址：<a href="http://jenkins.jenkins.svc.cluster.local:8080/">http://jenkins.jenkins.svc.cluster.local:8080</a><br>即jenkins服务在k8s集群中使用完整的svc名称的访问地址</p><p>其他项默认即可</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/05c0f00a2a1fdae7abb1fe0d5ad64ac2.png"></p><h2 id="Pod-Templates配置"><a href="#Pod-Templates配置" class="headerlink" title="Pod Templates配置"></a>Pod Templates配置</h2><p>因为我们需要定义一些功能，所以需要自定义jenkins构建节点pod的配置</p><p>点击添加pod模板后再点击Pod Template details按钮即可开始配置</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/255b9d0e92403bcaa35bf6c2d7762565.png"></p><p>注意事项：</p><ul><li><p>  命令空间填写和jenkins同一命名空间</p></li><li><p>  标签列表就是后续job调用这个pod运行的标签，后续需要写到job中去</p></li><li><p>  容器列表名称的pod模板中的容器名必须为jnlp，这样就只会有一个容器</p></li><li><p>  容器镜像必须是以jenkins/jnlp-slave镜像为基础的镜像，可以以此为基础镜像增加服务，此处我的镜像增加了mvn、nodejs、docker、kubectl</p></li><li><p>  如果pod模板中容器名称不是jnlp，那么就会认为是一个额外的容器，就会默认建立一个容器名为jnlp使用镜像jenkins/jnlp-slave的容器。然后将模板中的容器名称作为pod中的第2个容器的容器名，镜像使用你填写的镜像</p></li><li><p>  pipline工作中调用的容器永远是容器名为jnlp的容器</p></li></ul><h2 id="卷挂载（非必须的）"><a href="#卷挂载（非必须的）" class="headerlink" title="卷挂载（非必须的）"></a>卷挂载（非必须的）</h2><p><img src="/2020/06/03/jenkins-CICD-k8s/media/2546fa4d2ca83fd0f937f9c0f872ea68.png"></p><p>注意事项：</p><ul><li>  docker打包的时候需要将调度的k8s-node的docker.sock挂载到容器中，便于容器中的docker服务进行打包，然后推送镜像到私有仓库，推送镜像需要认证私有仓库信息。认证的时候是根据挂载的docker.sock的，所以k8s-node的docker服务需要将私有仓库信息添加到insecure-registries中，比如此处我的私有镜像仓库为172.168.137.5</li></ul><p><img src="/2020/06/03/jenkins-CICD-k8s/media/86d752d8beaf8302ba796150af848ece.png"></p><ul><li><p>  java项目打包的时候会下载很多依赖插件，所以此处需要将maven的仓库持久化存储，加快打包效率，注意我的jenkinsbuild镜像中使用的是root用户运行的，所以maven的仓库数据是放在/root/.m2的。</p></li><li><p>  secret认证是用于kubectl客户端连接k8s集群，便于k8s镜像发布，secret的创建在之前的get_k8s_admin_cert.sh已经建立</p></li><li><p>  其他的配置默认即可，例如：代理的空闲存活时间（分）代表jenkins构建节点pod的存活时间，一般默认即可</p></li></ul><h1 id="Jenkins自由风格Job验证"><a href="#Jenkins自由风格Job验证" class="headerlink" title="Jenkins自由风格Job验证"></a>Jenkins自由风格Job验证</h1><p><img src="/2020/06/03/jenkins-CICD-k8s/media/4ac012a55e42313cad69db7cd10d06ab.png"></p><h2 id="限制项目的运行节点"><a href="#限制项目的运行节点" class="headerlink" title="限制项目的运行节点"></a>限制项目的运行节点</h2><p><img src="/2020/06/03/jenkins-CICD-k8s/media/ea3186c54da301f079b0e4a46b8cd0cd.png"></p><p>此处就对应之前pod template中的标签列表</p><h2 id="构建操作，执行一个shell"><a href="#构建操作，执行一个shell" class="headerlink" title="构建操作，执行一个shell"></a>构建操作，执行一个shell</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br><span class="line">mvn -v</span><br><span class="line">nodejs -v</span><br><span class="line">docker info</span><br><span class="line">kubectl get pod -n jenkins</span><br></pre></td></tr></table></figure><p><img src="/2020/06/03/jenkins-CICD-k8s/media/f694753eb556140c75e4247ea4e8efb5.png"></p><p>保存后点击构建</p><p>会出现下面的状态，正在创建pod以获取输出，如果一直没有变化，那么就是pod建立失败，需要查看一下kubernetes节点配置是否正确，大多数原因是因为pod的配置</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/37357509031ed3b1d088ed8ed26a049a.png"></p><p>等待建立构建的pod，可以在终端查看到</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/0b077fc9098d1f93860c632af3ab0fb0.png"></p><p>pod建立成功将会在jenkins控制台进行输出，也会输出pod的建立yaml，可以根据来排查问题</p><p>成功获取到了上述shell执行结果</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/2d3e57dca1b2d82746ea73672cacdc20.png"></p><h1 id="Jenkins认证凭证"><a href="#Jenkins认证凭证" class="headerlink" title="Jenkins认证凭证"></a>Jenkins认证凭证</h1><p><img src="/2020/06/03/jenkins-CICD-k8s/media/b8b0051d9be94b8ee731ba1d50353e8b.png"></p><p>添加好jenkins认证凭证，可以用于后续的job拉取git仓库代码、pipline中使用git函数利用凭证的ID字段进行认证，上面的ID是可以手动定义的，后续将会用到</p><h1 id="Jenkins-Pipline"><a href="#Jenkins-Pipline" class="headerlink" title="Jenkins Pipline"></a>Jenkins Pipline</h1><p>构建定制化的持续集成是必须的工作，pipline的使用就不过多介绍,此处介绍本次使用的pipline过程</p><p>参考github地址：<a href="https://github.com/huisebug/jenk8s-pipline.git">https://github.com/huisebug/jenk8s-pipline.git</a> 和 <a href="https://github.com/huisebug/jenk8s-install.git">https://github.com/huisebug/jenk8s-install.git</a></p><ul><li><p>  build.groovy：流水线pipline文件</p></li><li><p>  dockerfile：java环境Dockerfile，nodejs项目Dockerfile，jenkins构建节点的Dockerfile</p></li><li><p>  jenkins：jenkins部署到k8s中的yaml</p></li><li><p>  projects：存放需要打包的项目的信息，文件目录名区分项目，project_info文件中写入：项目名称#服务名称=服务的git代码地址</p></li><li><p>  scripts：存放docker镜像制作的脚本</p></li></ul><h2 id="build-groovy"><a href="#build-groovy" class="headerlink" title="build.groovy"></a>build.groovy</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line">//当前的jenkins管理节点</span><br><span class="line">node &#123;</span><br><span class="line">//配置全局变量</span><br><span class="line">env.project = env.JOB_BASE_NAME.split(&quot;\\+&quot;)[0]</span><br><span class="line">env.app_name = env.JOB_BASE_NAME.split(&quot;\\+&quot;)[1]</span><br><span class="line">env.branch = env.JOB_BASE_NAME.split(&quot;\\+&quot;)[2]</span><br><span class="line">println(env.project)</span><br><span class="line">println(env.app_name)</span><br><span class="line">println(env.branch)</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line">因为jenkins的版本原因，使用git方式拉取了jenkinsfile的项目所有源代码文件</span><br><span class="line">在系统workspace目录下建立了一个job名+@script的文件目录来存放jenkinsfile源代码所有文件</span><br><span class="line">pipline执行的时候是以job名称为文件目录名称下</span><br><span class="line">类似于</span><br><span class="line">[root@localhost workspace]# tree</span><br><span class="line">.</span><br><span class="line">├── jenkins+java+master</span><br><span class="line">│   └── java-ci</span><br><span class="line">├── jenkins+java+master@script</span><br><span class="line">│   ├── build.groovy</span><br><span class="line">│   └── projects</span><br><span class="line">│       └── jenkins</span><br><span class="line">│           └── project_info</span><br><span class="line">*/</span><br><span class="line"></span><br><span class="line">//jenkinsfile的源代码存放目录名称,此处是:job名称+@script</span><br><span class="line">env.tempsuffix = &#x27;@script&#x27;</span><br><span class="line">env.JenkinsfileREPO = env.JOB_BASE_NAME + env.tempsuffix</span><br><span class="line"></span><br><span class="line">//获取代码仓库地址,作为全局变量提供给打包k8s节点使用</span><br><span class="line">if (env.app_name == &#x27;&#x27;) &#123;</span><br><span class="line">env.app_name = &#x27;none&#x27;</span><br><span class="line">env.git_repository = sh returnStdout: true, script: &#x27;cat ../&#x27;+env.JenkinsfileREPO+&#x27;/projects/&#x27;+env.project+&#x27;project_info|grep &#x27;+env.project+&#x27;_repo|awk -F &quot;=&quot; \&#x27;&#123;print $2&#125;\&#x27;&#x27;</span><br><span class="line">env.ci_dir =  env.project+&#x27;-ci&#x27;</span><br><span class="line"></span><br><span class="line">&#125; else &#123;</span><br><span class="line">env.git_repository = sh returnStdout: true, script: &#x27;cat ../&#x27;+env.JenkinsfileREPO+&#x27;/projects/&#x27;+env.project+&#x27;/project_info|grep &#x27;+env.project+&#x27;#&#x27;+env.app_name+&#x27;_repo|awk -F &quot;=&quot; \&#x27;&#123;print $2&#125;\&#x27;&#x27;    </span><br><span class="line">env.ci_dir =  env.app_name+&#x27;-ci&#x27;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//CI打包k8s节点，这里就是匹配之前的pod template中的标签列表</span><br><span class="line">node(&#x27;jenkinsbuildnode&#x27;) &#123;</span><br><span class="line">echo &quot;本次build的项目的源代码地址: &quot;+env.git_repository</span><br><span class="line">        </span><br><span class="line">stage(&#x27;CI打包&#x27;) &#123;</span><br><span class="line">    /*tools &#123;</span><br><span class="line">                maven &#x27;3.6.2&#x27;</span><br><span class="line">                jdk &#x27;1.8.0_242&#x27;</span><br><span class="line">                nodejs &#x27;10.19.0&#x27;</span><br><span class="line">npm &#x27;5.8.0&#x27;</span><br><span class="line">            &#125;*/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">script &#123;       </span><br><span class="line">                    /*判断是否推送到生产仓库</span><br><span class="line">                    try &#123;</span><br><span class="line">                        timeout(time: 70, unit: &#x27;SECONDS&#x27;) &#123;</span><br><span class="line">                            def userInput = input(id: &#x27;userInput&#x27;, ok: &#x27;确定&#x27;, message: &#x27;是否推送镜像到生产环境&#x27;, parameters: [booleanParam(defaultValue:  false, description: &#x27;&#x27;, name: &#x27;发布到master仓库&#x27;)])</span><br><span class="line">                            //println userInput</span><br><span class="line">                            println userInput.getClass()</span><br><span class="line">                            if(userInput==true)&#123;</span><br><span class="line">                                env.to_master_registry = &quot;go&quot;</span><br><span class="line">                            &#125;else &#123;</span><br><span class="line">                                env.to_master_registry = &quot;&quot;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">catch(Exception ex) &#123;</span><br><span class="line">                        println(&quot;Catching  exception&quot;)</span><br><span class="line">                        echo &#x27;do nothing, continue.......&#x27;</span><br><span class="line">                        env.to_master_registry = &quot;&quot;</span><br><span class="line">                    &#125;*/</span><br><span class="line">dir(env.ci_dir) &#123;</span><br><span class="line">                        echo &quot;拉取git代码&quot;</span><br><span class="line">                        checkout([$class: &#x27;GitSCM&#x27;, branches: [[name: branch]], doGenerateSubmoduleConfigurations: false, extensions: [[$class: &#x27;CleanBeforeCheckout&#x27;]], submoduleCfg: [], userRemoteConfigs: [[credentialsId: &#x27;huisebug&#x27;, url: git_repository]]])                    </span><br><span class="line">                        git_commit_hash = sh (script: &quot;git log -n 1 --pretty=format:&#x27;%H&#x27;&quot;, returnStdout: true)</span><br><span class="line">                        env.git_commit_hash = &#x27;-&#x27;+git_commit_hash[0..4]                        </span><br><span class="line"></span><br><span class="line">                        project = sh returnStdout: true, script: &#x27;cat app.info|grep devlang|awk -F &quot;:&quot; \&#x27;&#123;print $2&#125;\&#x27;&#x27;</span><br><span class="line">                        project = project.replace(&quot;\r&quot;,&quot;&quot;).replace(&quot; &quot;, &quot;&quot;).replace(&#x27;\n&#x27;, &#x27;&#x27;)</span><br><span class="line">                        echo &quot;项目类型是&quot;+project</span><br><span class="line">                        println project.class</span><br><span class="line"></span><br><span class="line">                        def version = sh returnStdout: true, script: &#x27;&#x27;&#x27; cat app.info |grep version|awk -F &quot;:&quot; &#x27;&#123;print $2&#125;&#x27; &#x27;&#x27;&#x27;</span><br><span class="line">                        env.ver = version.replace(&quot;\r&quot;, &quot;&quot;).replace(&quot;\n&quot;, &quot;&quot;).replace(&quot; &quot;, &quot;&quot;)</span><br><span class="line">                        echo &#x27;打包版本&#x27;+env.ver</span><br><span class="line">                        </span><br><span class="line">if (project == &#x27;NodeJs&#x27;||project == &#x27;nodejs&#x27;) &#123;</span><br><span class="line">                            sh &#x27;mkdir -pv target&#x27;</span><br><span class="line">                            //sh &#x27;find ./ -type f -exec dos2unix -q &#123;&#125; \\;&#x27;</span><br><span class="line">                            sh &#x27;npm cache clean --force&#x27;</span><br><span class="line">                            sh &#x27;npm config set registry http://registry.npm.taobao.org/&#x27;</span><br><span class="line">                            sh &#x27;cnpm install&#x27;</span><br><span class="line">                            if (env.JOB_BASE_NAME.tokenize(&#x27;+&#x27;)[3] == &#x27;dev&#x27;) &#123;</span><br><span class="line">                                sh &#x27;npm run build:dev&#x27;</span><br><span class="line">                            &#125; else if (env.JOB_BASE_NAME.tokenize(&#x27;+&#x27;)[3] == &#x27;test&#x27;) &#123;</span><br><span class="line">                                sh &#x27;npm run build:test&#x27;</span><br><span class="line">                            &#125; else if (env.JOB_BASE_NAME.tokenize(&#x27;+&#x27;)[3] == &#x27;sg&#x27;) &#123;</span><br><span class="line">                                sh &#x27;npm run build:sg&#x27;                               </span><br><span class="line">                            &#125; else &#123;</span><br><span class="line">                                sh &#x27;npm run build&#x27;</span><br><span class="line">                            &#125;</span><br><span class="line">                            sh &#x27;mv dist target/&#x27;</span><br><span class="line">                            echo project+&#x27;nodejs container Preparing....&#x27;</span><br><span class="line"></span><br><span class="line">                        &#125; else if (project == &#x27;Java&#x27;||project == &#x27;java&#x27;) &#123;</span><br><span class="line">                            echo project+&#x27; java container Preparing....&#x27;</span><br><span class="line">                            sh &#x27;mvn -Dmaven.test.skip=true clean package&#x27;   </span><br><span class="line"></span><br><span class="line">                        &#125; else if (project == &#x27;Python&#x27;||project == &#x27;python&#x27;) &#123;</span><br><span class="line">                            sh &#x27;mkdir -pv target&#x27;</span><br><span class="line">                            //sh &#x27;find ./ -type f -exec dos2unix -q &#123;&#125; \\;&#x27;</span><br><span class="line">                            sh &#x27;cp -rf `ls|grep -v app.info|grep -v target|xargs` target/&#x27;</span><br><span class="line">                            echo project+&#x27; python container Preparing....&#x27;</span><br><span class="line"></span><br><span class="line">                        &#125; else if (project == &#x27;HTML&#x27;||project == &#x27;html&#x27;) &#123;</span><br><span class="line">                            sh &#x27;mkdir -pv target/dist&#x27;</span><br><span class="line">                            //sh &#x27;find ./ -type f -exec dos2unix -q &#123;&#125; \\;&#x27;</span><br><span class="line">                            sh &#x27;cp -rf `ls|grep -v app.info|grep -v target|xargs` target/dist&#x27;</span><br><span class="line">                            echo project+&#x27; html container Preparing....&#x27;</span><br><span class="line"></span><br><span class="line">                        &#125; else &#123;</span><br><span class="line">                            sh &#x27;mkdir -pv target/dist&#x27;</span><br><span class="line">                            //sh &#x27;find ./ -type f -exec dos2unix -q &#123;&#125; \\;&#x27;</span><br><span class="line">                            sh &#x27;cp -rf `ls|grep -v app.info|grep -v target|xargs` target/dist&#x27;</span><br><span class="line">                            echo project+&#x27; html container Preparing....&#x27;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">stage(&#x27;docker镜像制作&#x27;) &#123;</span><br><span class="line">            script &#123;</span><br><span class="line">                sh &#x27;mkdir -pv dockerbuild&#x27;</span><br><span class="line">                dir(&#x27;dockerbuild&#x27;) &#123;</span><br><span class="line">                    deleteDir()</span><br><span class="line">                    echo &quot;拉取docker镜像制作脚本，代码分支一般默认是master&quot;,凭证信息credentialsId: &#x27;huisebug&#x27; ，直接使用jenkins存放的凭证的ID</span><br><span class="line">env.git_repo = &#x27;https://github.com/huisebug/jenk8s-pipline.git&#x27;</span><br><span class="line">                    checkout([$class: &#x27;GitSCM&#x27;, branches: [[name: &#x27;master&#x27;]], doGenerateSubmoduleConfigurations: false, extensions: [[$class: &#x27;CleanBeforeCheckout&#x27;]], submoduleCfg: [], userRemoteConfigs: [[credentialsId: &#x27;huisebug&#x27;, url: git_repo]]])                    </span><br><span class="line"></span><br><span class="line">                    println(&#x27;需要执行打包的容器名称有:&#x27;)</span><br><span class="line">                    sh &#x27;bash scripts/dockerbuild.sh&#x27;+&#x27; &#x27;+env.WORKSPACE+&#x27;/&#x27;+env.ci_dir+&#x27; &#x27;+env.app_name+&#x27; &#x27;+env.ver+&#x27; &#x27;+env.git_commit_hash+&#x27; &#x27;+env.project+&#x27; &#x27;+project+&#x27; &#x27;+env.to_master_registry</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>  pipline会首先在jenkins本地node执行build.groovy，根据当前的job名称来获取项目名称+服务名称+代码分支，例如：jenkins+java+master，然后去匹配projects文件目录中的信息，最终得到此次job的运行的服务名，git地址，分支信息传递到整个pipline的全局变量中</p></li><li><p>  然后运行在jenkins构建节点pod中运行拉取项目服务代码、项目服务编译、docker镜像制作</p></li></ul><h2 id="scripts-dockerbuild-sh"><a href="#scripts-dockerbuild-sh" class="headerlink" title="scripts/ dockerbuild.sh"></a>scripts/ dockerbuild.sh</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">脚本参数，<span class="variable">$1</span>:ci路径   <span class="variable">$2</span>:服务名称   <span class="variable">$3</span>:打包版本   <span class="variable">$4</span>:代码哈希  <span class="variable">$5</span> 项目名称  <span class="variable">$6</span>:项目类型(Html  or  Java  or  Nodejs)  <span class="variable">$7</span>:生产仓库(可选)</span></span><br><span class="line"><span class="meta">#</span><span class="bash">私库地址</span></span><br><span class="line">REGISTRY=&#x27;192.168.137.5&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">私库存放服务的仓库名</span></span><br><span class="line">WAREHOUSE_NAME=&#x27;huisebug&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">项目的CI目录名称</span></span><br><span class="line">if [ x$2 != xnone ];then</span><br><span class="line">app_name=$5-$2</span><br><span class="line">ci_dir=$2-ci</span><br><span class="line">else</span><br><span class="line">app_name=$5</span><br><span class="line">ci_dir=$5-ci</span><br><span class="line">fi</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">java项目镜像制作</span></span><br><span class="line">java()</span><br><span class="line">&#123;</span><br><span class="line">BASE_IMAGE=$&#123;REGISTRY&#125;/base/openjdk:8</span><br><span class="line">APP_HOME=/data/projects/$&#123;app_name&#125;</span><br><span class="line">EXE_CMD=&quot;java -jar&quot;</span><br><span class="line">EXE_BIN=$&#123;APP_HOME&#125;/bin/$&#123;app_name&#125;.jar</span><br><span class="line">EXE_CONF=&#x27;-Dlog_host=$&#123;log_host&#125;&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">jvm内存参数，临时设置，后续可以传参自定义</span></span><br><span class="line">EXE_LEVEL=&quot;-Xms1680M -Xmx1680M -Xmn1260M -Xss1M&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash">jvm其他参数调优</span></span><br><span class="line">EXE_OPTION=&quot;-server -XX:SurvivorRatio=8 -XX:+UseConcMarkSweepGC -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=512m -XX:MaxDirectMemorySize=1g -XX:+ExplicitGCInvokesConcurrent -XX:CMSInitiatingOccupancyFraction=80 -XX:-UseCMSInitiatingOccupancyOnly -Dsun.rmi.dgc.server.gcInterval=2592000000 -Dsun.rmi.dgc.client.gcInterval=2592000000 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=$&#123;APP_HOME&#125;/log/java.hprof -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cat &gt; run.sh &lt;&lt; \EOF</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">环境变量当前主机名</span></span><br><span class="line">log_host=`hostname`</span><br><span class="line"><span class="meta">#</span><span class="bash">jvm内存参数设置</span></span><br><span class="line">java_mem=`env|grep java_mem|awk -F &#x27;=&#x27; &#x27;&#123;print $2&#125;&#x27;`</span><br><span class="line"><span class="meta">#</span><span class="bash">服务日志是否在控制台输出，即容器即生命</span></span><br><span class="line">log_echo=`env|grep log_echo|awk -F &#x27;=&#x27; &#x27;&#123;print $2&#125;&#x27;`</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">判断变量java_mem是否为空，不存在即为空</span></span><br><span class="line">if [[ x$java_mem == x ]];then</span><br><span class="line">EXE_LEVEL=&quot;&quot;</span><br><span class="line">else</span><br><span class="line">EXE_LEVEL=$java_mem</span><br><span class="line">fi</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">额外的jvm参数</span></span><br><span class="line">if [[ x$general_para != x ]];then</span><br><span class="line">EXT_OPTION=$general_para</span><br><span class="line">else</span><br><span class="line">EXT_OPTION=&quot;&quot;</span><br><span class="line">fi</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">程序的参数</span></span><br><span class="line">if [[ x$application_para != x ]];then</span><br><span class="line">APP_OPTION=$application_para</span><br><span class="line">else</span><br><span class="line">APP_OPTION=&quot;&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">echo &quot;if [[ x\$&#123;log_echo&#125; != x ]];then&quot; &gt;&gt; run.sh</span><br><span class="line">echo $&#123;EXE_CMD&#125; \$&#123;EXE_LEVEL&#125; $&#123;EXE_OPTION&#125;  \$&#123;EXT_OPTION&#125; $&#123;EXE_CONF&#125; $&#123;EXE_BIN&#125; \$&#123;APP_OPTION&#125; &gt;&gt; run.sh</span><br><span class="line">echo &quot;else&quot; &gt;&gt; run.sh</span><br><span class="line">EXE_CMD=&quot;nohup java -jar&quot; </span><br><span class="line">echo $&#123;EXE_CMD&#125; \$&#123;EXE_LEVEL&#125; $&#123;EXE_OPTION&#125;  \$&#123;EXT_OPTION&#125; $&#123;EXE_CONF&#125; $&#123;EXE_BIN&#125; \$&#123;APP_OPTION&#125; &gt;&gt; run.sh</span><br><span class="line">echo &quot;tail -f /dev/null&quot; &gt;&gt; run.sh</span><br><span class="line">echo &quot;fi&quot; &gt;&gt; run.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">准备好将要复制到镜像中的服务文件</span></span><br><span class="line">rm -rf source</span><br><span class="line">mkdir -p source</span><br><span class="line">mv ../$&#123;ci_dir&#125;/target/* source/</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">生成Dockerfile</span></span><br><span class="line">cat &gt; Dockerfile &lt;&lt; EOF</span><br><span class="line"></span><br><span class="line">FROM $&#123;BASE_IMAGE&#125; </span><br><span class="line">RUN for dir in data bin log conf; do mkdir -p $&#123;APP_HOME&#125;/\$dir; done </span><br><span class="line">ADD source $&#123;APP_HOME&#125;/bin</span><br><span class="line">COPY run.sh /root/run.sh</span><br><span class="line">CMD [&quot;/bin/bash&quot;,&quot;/root/run.sh&quot;]</span><br><span class="line"></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">web项目镜像制作，包含nodejs和html</span></span><br><span class="line">web()</span><br><span class="line">&#123;</span><br><span class="line"><span class="meta">#</span><span class="bash">此处只做拷贝文件，启动方式按照<span class="variable">$BASE_IMAGE</span>的说明进行</span></span><br><span class="line">BASE_IMAGE=$&#123;REGISTRY&#125;/base/nginx:1.19</span><br><span class="line">APP_HOME=/usr/share/nginx/html</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">准备好将要复制到镜像中的服务文件，nodejs打包后的项目html全部放在dist目录下，在前面的pipline中已经将dist目录移动到了target目录中</span></span><br><span class="line">rm -rf source</span><br><span class="line">mkdir -p source</span><br><span class="line">mv ../$&#123;ci_dir&#125;/target/dist/* source/</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">生成dockerfile</span></span><br><span class="line">cat &gt; Dockerfile &lt;&lt; EOF</span><br><span class="line">FROM $&#123;BASE_IMAGE&#125;</span><br><span class="line">ADD source $&#123;APP_HOME&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if [[ x$6 == xJava || x$6 == xjava  ]];then</span><br><span class="line">java $1 $2 $3 $4 $5 $6 $7 $8</span><br><span class="line">else</span><br><span class="line">web $1 $2 $3 $4 $5 $6 $7 $8</span><br><span class="line">fi</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">####################main()###################</span></span></span><br><span class="line">cat Dockerfile</span><br><span class="line">container_name=$&#123;app_name&#125;</span><br><span class="line">time_char=`date &quot;+%y%m%d%H%M&quot;`</span><br><span class="line">container_version=$3-$&#123;time_char&#125;$4</span><br><span class="line"><span class="meta">#</span><span class="bash">私有仓库的认证</span></span><br><span class="line">docker login 192.168.137.5 -u admin -p huisebug &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">docker build -t $&#123;REGISTRY&#125;/$&#123;WAREHOUSE_NAME&#125;/$&#123;container_name&#125;:$&#123;container_version&#125; .</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">生成镜像文件<span class="comment">#推送镜像文件</span></span></span><br><span class="line">docker push  $&#123;REGISTRY&#125;/$&#123;WAREHOUSE_NAME&#125;/$&#123;container_name&#125;:$&#123;container_version&#125;</span><br><span class="line"></span><br><span class="line">if [[ x$7 == xgo ]];then</span><br><span class="line">echo &#x27;开始推送到生产镜像仓库&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash">生产镜像仓库地址</span></span><br><span class="line">proimagerepo=192.168.137.5</span><br><span class="line"><span class="meta">#</span><span class="bash">生产镜像仓库认证</span></span><br><span class="line">docker login $&#123;proimagerepo&#125; -u admin -p huisebug &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">docker tag $&#123;REGISTRY&#125;/$&#123;WAREHOUSE_NAME&#125;/$&#123;container_name&#125;:$&#123;container_version&#125; $&#123;proimagerepo&#125;/$&#123;WAREHOUSE_NAME&#125;/$&#123;container_name&#125;:$&#123;container_version&#125;</span><br><span class="line">docker push $&#123;proimagerepo&#125;/$&#123;WAREHOUSE_NAME&#125;/$&#123;container_name&#125;:$&#123;container_version&#125;</span><br><span class="line">echo &#x27;清除多余的标签&#x27;</span><br><span class="line">docker rmi $&#123;proimagerepo&#125;/$&#123;WAREHOUSE_NAME&#125;/$&#123;container_name&#125;:$&#123;container_version&#125;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><ul><li><p>  java的镜像制作过程中会需要项目的pom.xml中定义构建后的jar包名称，比如此处我的项目jar名称就应该为jenkins-java.jar。</p></li><li><p>  java的docker镜像制作是可以后续在建立时使用docker的变量方式进行java传参和jar包传参；变量general_para是java参数传递，变量application_para是jar参数传递。</p></li><li><p>  nodejs编译后就是web页面，所以nodejs和web直接copy到nginx容器即可</p></li><li><p>  使用的基础docker镜像是存放在私有镜像仓库的，所以需要推送镜像到私有镜像仓库，参考java和nginx基础镜像Dockerfile：<a href="https://github.com/huisebug/jenk8s-install.git">https://github.com/huisebug/jenk8s-install.git</a></p></li></ul><h2 id="projects"><a href="#projects" class="headerlink" title="projects"></a>projects</h2><p>此处我准备的项目名称为：jenkins 所以建立了一个jenkins文件目录</p><p>jenkins文件目录下存放了project_info文件<br>project_info</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">jenkins#</span><span class="bash">java_repo=https://github.com/huisebug/jenk8s-java.git</span></span><br><span class="line"><span class="meta">jenkins#</span><span class="bash">nodejs_repo=https://github.com/huisebug/jenk8s-nodejs.git</span></span><br><span class="line"><span class="meta">jenkins#</span><span class="bash">html_repo=https://github.com/huisebug/jenk8s-html.git</span></span><br></pre></td></tr></table></figure><ul><li><p>  前面的jenkins代表项目名称，要求这个名称和文件目录名称相同</p></li><li><p>  java_repo/nodejs_repo/html_repo将会去掉repo取名作为服务名称</p></li><li><p>  git地址是对应的服务的git地址</p></li></ul><h2 id="调试项目介绍"><a href="#调试项目介绍" class="headerlink" title="调试项目介绍"></a>调试项目介绍</h2><p>此处我准备了三个不同类型的项目</p><p>java：<a href="https://github.com/huisebug/jenk8s-java.git">https://github.com/huisebug/jenk8s-java.git</a></p><p>nodejs：<a href="https://github.com/huisebug/jenk8s-nodejs.git">https://github.com/huisebug/jenk8s-nodejs.git</a></p><p>html：<a href="https://github.com/huisebug/jenk8s-html.git">https://github.com/huisebug/jenk8s-html.git</a></p><p>三个项目中都会有一个特殊的配置文件app.info</p><h3 id="项目中特殊的配置文件app-info"><a href="#项目中特殊的配置文件app-info" class="headerlink" title="项目中特殊的配置文件app.info"></a>项目中特殊的配置文件app.info</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">devlang:java </span><br><span class="line">version:1.1 </span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">devlang:html </span><br><span class="line">version:1.1</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">devlang:nodejs </span><br><span class="line">version:1.1</span><br></pre></td></tr></table></figure><ul><li><p>  需要给项目中的代码根目录建立一个app.info文件，便于流水线识别这是什么类型的项目而调用对应的方法进行打包和制作docker镜像</p></li><li><p>  devlang ：开发语言</p></li><li><p>  version ：版本号</p></li></ul><h1 id="Jenkins流水线Job验证"><a href="#Jenkins流水线Job验证" class="headerlink" title="Jenkins流水线Job验证"></a>Jenkins流水线Job验证</h1><h2 id="job名称格式"><a href="#job名称格式" class="headerlink" title="job名称格式"></a>job名称格式</h2><p><img src="/2020/06/03/jenkins-CICD-k8s/media/629a853f5ea978a15ce182f1ca228828.png"></p><p>注意job名称，以”+”号分割，分别是项目名称+服务名称+代码分支</p><h2 id="流水线配置"><a href="#流水线配置" class="headerlink" title="流水线配置"></a>流水线配置</h2><p><img src="/2020/06/03/jenkins-CICD-k8s/media/7351b532308f352de91be0b66d46ea4d.png"></p><p>使用git的方式拉取jenkinsfile（即build.groovy），轻量级检出的选项不勾选，勾选就会直接读取build.groovy的内容，不会clone整个项目代码，就无法获取到其他的项目代码</p><p>后续的项目我们只需要在建立job的时候复制然后建立就行了</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/62effa702063273bed79898bece3b9ee.png"></p><p>java项目执行结果</p><p><img src="/2020/06/03/jenkins-CICD-k8s/media/xiaoguo.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这是一个k8s集群结合jenkins做持续集成，以及jenkins流水线在k8s集群中运行。java编译后、nodejs编译后docker镜像制作。&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes" scheme="https://huisebug.github.io/categories/Kubernetes/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="jenkins" scheme="https://huisebug.github.io/tags/jenkins/"/>
    
    <category term="devops" scheme="https://huisebug.github.io/tags/devops/"/>
    
    <category term="pipline" scheme="https://huisebug.github.io/tags/pipline/"/>
    
    <category term="java" scheme="https://huisebug.github.io/tags/java/"/>
    
    <category term="nodejs" scheme="https://huisebug.github.io/tags/nodejs/"/>
    
    <category term="dockerfile" scheme="https://huisebug.github.io/tags/dockerfile/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes-hpav2横向自动扩容</title>
    <link href="https://huisebug.github.io/2019/08/28/k8s-hpa/"/>
    <id>https://huisebug.github.io/2019/08/28/k8s-hpa/</id>
    <published>2019-08-28T02:04:01.000Z</published>
    <updated>2021-07-07T08:50:30.212Z</updated>
    
    <content type="html"><![CDATA[<p>这是一个k8s集群验证hpav2的功能</p><span id="more"></span><h1 id="HPA"><a href="#HPA" class="headerlink" title="HPA"></a>HPA</h1><ul><li>  Horizo​​ntal Pod Autoscaler根据观察到的CPU利用率自动调整复制控制器，部署或副本集中的pod数量（或者，使用自定义度量标准支持，根据其他一些应用程序提供的度量标准）。请注意，Horizo​​ntal PodAutoscaling不适用于无法缩放的对象，例如DaemonSet。</li><li>  Horizo​​ntal Pod Autoscaler实现为Kubernetes API资源和控制器。资源确定控制器的行为。控制器会定期调整复制控制器或部署中的副本数，以使观察到的平均CPU利用率与用户指定的目标相匹配</li></ul><h2 id="Horizo​​ntal-Pod-Autoscaler如何工作？"><a href="#Horizo​​ntal-Pod-Autoscaler如何工作？" class="headerlink" title="Horizo​​ntal Pod Autoscaler如何工作？"></a>Horizo​​ntal Pod Autoscaler如何工作？</h2><p><img src="/2019/08/28/k8s-hpa/media/7f0680b00596a3a68743a5b0b041da00.png"></p><p>Horizo​​ntal Pod<br>Autoscaler实现为控制循环，其周期由控制器管理器的–horizontal-pod-autoscaler-sync-period标志控制（默认值为15秒）。<br>在每个期间，控制器管理器根据每个Horizo​​ntalPodAutoscaler定义中指定的度量查询资源利用率。控制器管理器从资源指标API（针对每个窗格资源指标）或自定义指标API（针对所有其他指标）获取指标。</p><ul><li>  对于每个pod资源指标（如CPU），控制器从Horizo​​ntalPodAutoscaler所针对的每个pod获取资源指标API中的指标。然后，如果设置了目标利用率值，则控制器将利用率值计算为每个容器中容器上的等效资源请求的百分比。如果设置了目标原始值，则直接使用原始度量标准值。然后，控制器在所有目标pod中获取利用率的平均值或原始值（取决于指定的目标类型），并产生用于缩放所需副本数量的比率。<br>请注意，如果某些pod的容器没有设置相关的资源请求，则不会定义pod的CPU利用率，并且autoscaler不会对该度量标准采取任何操作。有关自动调节算法如何工作的更多信息，请参阅下面的算法详细信息部分。</li><li>  对于每个pod自定义指标，控制器的功能与每个pod资源指标类似，不同之处在于它适用于原始值，而不是使用值。</li><li>  对于对象度量和外部度量，将获取单个度量，该度量描述相关对象。将该度量与目标值进行比较，以产生如上所述的比率。在autoscaling/v2beta2API版本中，可以选择在进行比较之前将此值除以pod的数量。<br>所述Horizo​​ntalPodAutoscaler通常由一系列的API聚集（的获取度量metrics.k8s.io，custom.metrics.k8s.io和external.metrics.k8s.io）。该metrics.k8s.ioAPI通常是通过度量服务器，其需要单独启动提供。有关说明，请参阅<br>metrics-server。Horizo​​ntalPodAutoscaler还可以直接从Heapster获取指标。</li></ul><p><font color="red" size="5">从Kubernetes 1.11开始，不推荐从Heapster获取指标。</font></p><h2 id="算法细节"><a href="#算法细节" class="headerlink" title="算法细节"></a>算法细节</h2><p>从最基本的角度来看，Horizo​​ntal Pod Autoscaler控制器根据所需度量值与当前度量值之间的比率进行操作：<br>desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]</p><ul><li>  例如，如果当前度量标准值是200m，并且期望值是100m，则副本的数量将加倍，因为200.0 / 100.0 == 2.0如果当前值是50m，则我们将副本的数量减半，因为50.0 / 100.0 == 0.5。如果比率足够接近1.0（在全局可配置的容差范围内，从–horizontal-pod-autoscaler-tolerance标志，默认为0.1），我们将跳过缩放。</li><li>  当指定a targetAverageValue或时targetAverageUtilization，currentMetricValue通过在Horizo​​ntalPodAutoscaler的比例目标中获取所有Pod的给定度量的平均值来计算。在检查容差和决定最终值之前，我们考虑了容量准备和缺失指标。</li><li>  设置了删除时间戳的所有Pod（即正在关闭的Pod）和所有失败的Pod都将被丢弃。</li><li>  如果特定的Pod缺少指标，则将其留待以后使用;具有缺失指标的窗格将用于调整最终缩放量。</li><li>  在CPU上进行扩展时，如果任何pod尚未准备好（即它仍在初始化），或者pod的最新度量标准点在它准备就绪之前，那么该pod也会被搁置。</li><li>  由于技术限制，在确定是否预留某些CPU指标时，Horizo​​ntalPodAutoscaler控制器无法准确确定容器第一次准备就绪。相反，它认为Pod尚未准备就绪，如果它尚未准备就绪，并且在它启动后的一个简短，可配置的时间窗口内转换为未准备好。此值使用–horizontal-pod-autoscaler-initial-readiness-delay标志配置，默认值为30秒。一旦pod准备就绪，它会认为任何转换都准备好成为第一个，如果它在启动后的较长的可配置时间内发生的话。此值使用–horizontal-pod-autoscaler-cpu-initialization-period标志配置，默认值为5分钟。</li><li>  所述currentMetricValue / desiredMetricValue然后碱比例是利用剩余的pod没有预留或从上方丢弃计算。</li><li>  如果有任何缺失的指标，我们会更加保守地重新计算平均值，假设这些容量在缩小的情况下消耗100％的期望值，并且在放大的情况下消耗0％。这可以抑制任何潜在规模的大小。</li><li>  此外，如果存在任何尚未准备好的播客，并且我们会扩大规模而不考虑丢失的指标或尚未准备好的播客，我们保守地假设尚未准备好的播客正在消耗所需指标的0％，进一步抑制了规模扩大的程度。</li><li>  考虑到尚未准备好的广告连播和缺少指标后，我们会重新计算使用率。如果新比率反转了比例方向，或者在公差范围内，我们会跳过缩放比例。否则，我们使用新的比例进行扩展。</li><li>  请注意，即使使用新的使用率，也会通过Horizo​​ntalPodAutoscaler状态报告平均利用率的原始值，而不考虑尚未准备好的容器或缺少指标。</li><li>  如果在Horizo​​ntalPodAutoscaler中指定了多个度量标准，则对每个度量标准进行此计算，然后选择所需的最大副本计数。如果任何这些度量标准无法转换为所需的副本计数（例如，由于从度量标准API获取度量标准时出错），则会跳过缩放。</li><li>  最后，在HPA扩展目标之前，记录比例建议。控制器会在可配置窗口中考虑所有建议，从该窗口中选择最高建议。可以使用–horizontal-pod-autoscaler-downscale-stabilization-window标志配置此值，默认为5分钟。这意味着缩放将逐渐发生，平滑快速波动的度量值的影响。</li></ul><p>使用Horizo​​ntal Pod自动缩放器管理一组副本的比例时，由于所评估的度量标准的动态特性，副本数量可能会不断波动。这有时被称为颠簸。<br>从v1.6开始，集群运营商可以通过调整作为kube-controller-manager组件标志公开的全局HPA设置来缓解此问题：<br>从v1.12开始，新的算法更新消除了对高级延迟的需求。<br>–horizontal-pod-autoscaler-downscale-delay：此选项的值是一个持续时间，指定自动缩放器必须等待多长时间才能在当前完成后执行另一个缩减操作。默认值为5分钟（5m0s）。<br>注意：调整这些参数值时，集群操作员应了解可能的后果。如果延迟（冷却）值设置得太长，可能会有人抱怨Horizo​​ntal Pod Autoscaler没有响应工作负载变化。但是，如果延迟值设置得太短，副本集的比例可能会像往常一样保持颠簸。</p><h2 id="支持的API"><a href="#支持的API" class="headerlink" title="支持的API"></a>支持的API</h2><p>默认情况下，Horizo​​ntalPodAutoscaler控制器从一系列API中检索指标。为了使其能够访问这些API，集群管理员必须确保：<br>1.该API汇聚层启用。<br>2.相应的API已注册：</p><ul><li>  对于资源指标，这是metrics.k8s.ioAPI，通常由metrics-server提供。它可以作为群集插件启动。</li><li>  对于自定义指标，这是custom.metrics.k8s.ioAPI。它由度量标准解决方案供应商提供的“适配器”API服务器提供。检查您的指标管道或已知解决方案列表。如果您想自己编写，请查看样板文件以开始使用。</li><li>  对于外部指标，这是external.metrics.k8s.ioAPI。它可能由上面提供的自定义指标适配器提供。</li></ul><p>3.–horizontal-pod-autoscaler-use-rest-clients=true已经在1.12+版本取消。将此设置为false会切换到基于Heapster的自动缩放，不推荐使用。</p><p>官方参考文档链接地址：<br><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/</a></p><h1 id="HPA-V1"><a href="#HPA-V1" class="headerlink" title="HPA V1"></a>HPA V1</h1><p>v2已经集成v1的功能，所以这里就不演示v1了。<br>最简易的v1，例如下面命令或者yaml示例</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=103</span><br><span class="line"></span><br><span class="line">或</span><br><span class="line"></span><br><span class="line">apiVersion: autoscaling/v1</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: php-apache</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: apps/v1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: php-apache</span><br><span class="line">  minReplicas: 1</span><br><span class="line">  maxReplicas: 10</span><br><span class="line">  targetCPUUtilizationPercentage: 50</span><br></pre></td></tr></table></figure><h1 id="HPA-V2"><a href="#HPA-V2" class="headerlink" title="HPA V2 "></a>HPA V2 </h1><p>下面是之前提到的版本问题HPA无法获取内存情况</p><p>参考地址：<br><a href="https://github.com/kubernetes/kubernetes/issues/74704">https://github.com/kubernetes/kubernetes/issues/74704</a></p><p>k8s从v1.7版本开始，对支持自定义指标的HPA架构进行了重新设计，引入了api server aggregation层、<strong>custom metrics server</strong>等组件来实现自定义业务指标的采集、保存和查询、再提供给HPA控制器进行扩缩容决策，称为HPA V2版本；</p><p>首先我们需要给kube-controller-manager服务增加三个参数：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--horizontal-pod-autoscaler-sync-period=30s \</span><br><span class="line">--horizontal-pod-autoscaler-downscale-delay=3m0s \</span><br><span class="line">--horizontal-pod-autoscaler-upscale-delay=3m0s</span><br></pre></td></tr></table></figure><p>分别是同步时间，缩容时间，扩容时间</p><p>参考修改后的kube-controller-manager服务service文件地址：<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/kube-controller-manager">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/kube-controller-manager</a></p><h2 id="概念解析"><a href="#概念解析" class="headerlink" title="概念解析"></a>概念解析</h2><ul><li>  cluster-autoscaler：kubernetes社区中负责节点水平伸缩的组件，目前处在GA阶段（General Availability，即正式发布的版本）。</li><li>  HPA：kubernetes社区中负责Pod水平伸缩的组件，是所有伸缩组件中历史最悠久的，目前支持autoscaling/v1、autoscaling/v2beta1与autoscaling/v2beta2，其中autoscaling/v1只支持CPU一种伸缩指标，在autoscaling/v2beta1中增加支持custommetrics，在autoscaling/v2beta2中增加支持external metrics。（获取命令kubectlapi-versions）</li><li>  cluster-proportional-autoscaler：根据集群的节点数目，水平调整Pod数目的组件，目前处在GA阶段。</li><li>  vetical-pod-autoscaler：根据Pod的资源利用率、历史数据、异常事件，来动态调整负载的Request值的组件，主要关注在有状态服务、单体应用的资源伸缩场景，目前处在beta阶段。</li><li>  addon-resizer：根据集群中节点的数目，纵向调整负载的Request的组件，目前处在beta阶段。</li></ul><h1 id="Resouce类型（CPU、内存）"><a href="#Resouce类型（CPU、内存）" class="headerlink" title="Resouce类型（CPU、内存）"></a>Resouce类型（CPU、内存）</h1><h2 id="运行原理简析"><a href="#运行原理简析" class="headerlink" title="运行原理简析"></a>运行原理简析</h2><h3 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h3><p>spec.template.spec.containers.resources.requests来为基数，</p><p>根据pod消耗的cpu数量相加求出平均值，然后除以基数，即可得到当前百分比（current），超出设置的目标百分比（target）就进行扩容，低于就缩容。</p><p><img src="/2019/08/28/k8s-hpa/media/e0767d3a988919bd6df3cf47df01c173.png"></p><p>内存根据pod消耗的内存值相加求出平均值就是当前值（current），与目标值（target）相比较，高于就扩容。</p><h3 id="缩容"><a href="#缩容" class="headerlink" title="缩容"></a>缩容</h3><p>如果是CPU的原因进行了扩容后，CPU消耗百分比低于目标百分比将会进行缩容，此时平均消耗内存除以目标值，如果接近于1就不进行完全的缩容。</p><h2 id="实践探究"><a href="#实践探究" class="headerlink" title="实践探究"></a>实践探究</h2><h3 id="部署mysql-rc服务"><a href="#部署mysql-rc服务" class="headerlink" title="部署mysql-rc服务"></a>部署mysql-rc服务</h3><p>yaml文件参考地址：<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/mysql-rc">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/mysql-rc</a></p><p>注意，使用hpav2的type: Resource，必须在pod中定义spec.template.spec.containers.resource，否则没有具体的基数。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ReplicationController</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql-rc</span><br><span class="line">  labels:</span><br><span class="line">    name: mysql-rc</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    name: mysql-pod</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels: </span><br><span class="line">        name: mysql-pod</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: mysql</span><br><span class="line">        image: mysql</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 3306</span><br><span class="line">        env:</span><br><span class="line">        - name: MYSQL_ROOT_PASSWORD</span><br><span class="line">          value: &quot;mysql&quot;</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 200m</span><br><span class="line">            memory: 500Mi</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 400m</span><br><span class="line">            memory: 1000Mi</span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: autoscaling/v2beta1</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql-rc</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: v1</span><br><span class="line">    kind: ReplicationController</span><br><span class="line">    name: mysql-rc</span><br><span class="line">  minReplicas: 1</span><br><span class="line">  maxReplicas: 3</span><br><span class="line">  metrics:</span><br><span class="line">  - type: Resource</span><br><span class="line">    resource:</span><br><span class="line">      name: cpu</span><br><span class="line">      targetAverageUtilization: 80</span><br><span class="line">  - type: Resource</span><br><span class="line">    resource:</span><br><span class="line">      name: memory</span><br><span class="line">      targetAverageValue: 1300Mi</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>| <strong>指示</strong>                                                                                                                                                                                                                                                                                                                                                                                                                 | <strong>描述</strong>                                                                                             |<br>| apiVersion: autoscaling/v2beta1                                                                                                                                                                                                                                                                                                                                                                                          | autoscaling正在使用的Kubernetes API组的版本。此示例清单使用beta版本，因此启用了按CPU和内存进行扩展。 |<br>| name: mysql-rc                                                                                                                                                                                                                                                                                                                                                                                                           | 表示HPA正在为mysql-rc部署执行自动扩展。                                                              |<br>| minReplicas: 1                                                                                                                                                                                                                                                                                                                                                                                                           | 表示运行的最小副本数不能低于1。                                                                      |<br>| maxReplicas: 3                                                                                                                                                                                                                                                                                                                                                                                                           | 表示部署中最大副本数不能超过3。                                                                      |<br>| targetAverageUtilization: 80                                                                                                                                                                                                                                                                                                                                                                                             | 表示当平均运行pod使用超过其请求CPU的80％时，部署将扩展pod。                                          |<br>| targetAverageValue: 1300Mi                                                                                                                                                                                                                                                                                                                                                                                               | 表示当平均运行pod使用超过1300Mi的内存时，部署将扩展pod。   </p><h3 id="验证效果"><a href="#验证效果" class="headerlink" title="验证效果"></a>验证效果</h3><h4 id="部署结果"><a href="#部署结果" class="headerlink" title="部署结果"></a>部署结果</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod -l name=mysql-pod</span><br></pre></td></tr></table></figure><p><img src="/2019/08/28/k8s-hpa/media/e84b6ee5dfdb84f117bd0030afcaeb5f.png"></p><h5 id="CPU扩容"><a href="#CPU扩容" class="headerlink" title="CPU扩容"></a>CPU扩容</h5><p>进入pod中的容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -it mysql-rc-9ngzn bash</span><br></pre></td></tr></table></figure><p>使CPU满载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i in `seq 1 $(cat /proc/cpuinfo |grep &quot;physical id&quot; |wc -l)`; do dd if=/dev/zero of=/dev/null &amp; done</span><br></pre></td></tr></table></figure><p><img src="/2019/08/28/k8s-hpa/media/f3cfd616d9da177d177ee2c9fc2ae2a3.png"></p><p>查看扩容情况，之前设定了扩容间隔时间（–horizontal-pod-autoscaler-upscale-delay=3m0s）为3m0s。所以我们需要等待大约3分钟<br><img src="/2019/08/28/k8s-hpa/media/6a37a76bf2228f35abf66c662b09611d.png"></p><h4 id="CPU缩容"><a href="#CPU缩容" class="headerlink" title="CPU缩容"></a>CPU缩容</h4><p>杀死容器进程1，即会重启容器，然后增压CPU的循环将会停止，以达到CPU的负载就会将下来，然后进行缩容。</p><h5 id="内存不影响的情况下"><a href="#内存不影响的情况下" class="headerlink" title="内存不影响的情况下"></a>内存不影响的情况下</h5><p><img src="/2019/08/28/k8s-hpa/media/7a6bbf5f1793439be51cd86d098d4c93.png"></p><h5 id="内存影响情况下"><a href="#内存影响情况下" class="headerlink" title="内存影响情况下"></a>内存影响情况下</h5><p>此处的mysql稳定运行消耗的内存为500Mi左右，为了让其缩容比例接近于1，以达到扩容后，就算CPU消耗降下来，因为内存的current/target接近于1，不会完全的缩容到最小副本数。</p><p>参考yaml文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete hpa mysql-rc</span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/hpav2/mysql-rc/mysql-rc-hpa-memory.yaml</span><br></pre></td></tr></table></figure><p>测试步骤和上面一样</p><p><img src="/2019/08/28/k8s-hpa/media/cbfc06e94c4af731f1f9f7207ba61625.png"><br><img src="/2019/08/28/k8s-hpa/media/89b5f8e71844f2be704b4a02db5cc63f.png"><br><img src="/2019/08/28/k8s-hpa/media/7f6102e660fd454b4c74346e6564f26c.png"><br><img src="/2019/08/28/k8s-hpa/media/361a0e5c157f9d9b7588e62b5183451c.png"></p><p>此时pod数量将会缩减为2，所以目标值的设定可以经过实际场景数据进行分析。</p><p><img src="/2019/08/28/k8s-hpa/media/4ac56712cd4cdd18505e63a4d45a776b.png"></p><h1 id="建立custom-metrics-k8s-io"><a href="#建立custom-metrics-k8s-io" class="headerlink" title="建立custom.metrics.k8s.io"></a>建立custom.metrics.k8s.io</h1><p>使用HPA v2的type为Resource时，使用的api是metrics.k8s.io，这个是由metrics-server提供的，使用type为Pods和Object时就需要使用的api是custom.metrics.k8s.io；</p><p>其中的Custom Metrics Server由prometheus-adapter服务实现，这个服务在之前我们安装prometheus-operator服务的时候已经安装。但是其安装的是api：metrics.k8s.io，并且其功能并不满足。所以在metrics-server1.12已经将api：metrics.k8s.io移除并使用metrics-server来提供支持。并且移除了prometheus-adapter服务。</p><p>github上有位大神的方法可以成功，参考地址如下，并且里面也有metrics-server的安装和验证</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/stefanprodan/k8s-prom-hpa</span><br></pre></td></tr></table></figure><p>需要修改镜像地址为可拉取的，可参考我修改了的</p><p><a href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/k8s-prom-hpa">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/k8s-prom-hpa</a></p><h2 id="部署Prometheus和Prometheus-adapter"><a href="#部署Prometheus和Prometheus-adapter" class="headerlink" title="部署Prometheus和Prometheus-adapter"></a>部署Prometheus和Prometheus-adapter</h2><p>创建monitoring命名空间：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f ./namespaces.yaml</span><br></pre></td></tr></table></figure><p>在monitoring命名空间中部署Prometheus v2 ：</p><p>如果要部署到GKE，可能会收到错误消息：<em>Error from server (Forbidden): error when<br>creating</em> 这将帮助您解决该问题：<a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/troubleshooting.md">GKE上的RBAC</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f ./prometheus</span><br></pre></td></tr></table></figure><p>生成Prometheus适配器所需的TLS证书：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make certs</span><br></pre></td></tr></table></figure><p>部署Prometheus自定义指标API适配器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f ./custom-metrics-api</span><br></pre></td></tr></table></figure><h2 id="验证并访问api"><a href="#验证并访问api" class="headerlink" title="验证并访问api"></a>验证并访问api</h2><p>查看pod是否成功建立</p><p><img src="/2019/08/28/k8s-hpa/media/e2613a17ecea2aff0b5b982038589066.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl api-versions | grep custom.metrics.k8s.io</span><br></pre></td></tr></table></figure><p><img src="/2019/08/28/k8s-hpa/media/b5822b1159f9adaf1733c956398a013a.png"></p><p>安装jQuery插件，并将请求值转换为json</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum install epel-release</span><br><span class="line">yum list jq</span><br><span class="line">yum -y install jq</span><br></pre></td></tr></table></figure><p>获取monitoring命名空间中所有pod的FS使用情况：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/monitoring/pods/*/fs_usage_bytes&quot; | jq .</span><br></pre></td></tr></table></figure><p><img src="/2019/08/28/k8s-hpa/media/7b630add30ef6652a3943add8fced092.png"></p><h1 id="Pods类型"><a href="#Pods类型" class="headerlink" title="Pods类型"></a>Pods类型</h1><h2 id="创建Pod"><a href="#创建Pod" class="headerlink" title="创建Pod"></a>创建Pod</h2><p>依照项目k8s-prom-hpa中的podinfo文件目录下来验证。</p><p>podinfo在default命名空间中创建NodePort服务和部署</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f ./podinfo/podinfo-svc.yaml ./podinfo/podinfo-dep.yaml</span><br></pre></td></tr></table></figure><p>该podinfo应用程序公开名为的自定义指标http_requests_total。Prometheus适配器删除_total后缀并将度量标记为计数器度量标准。</p><p>从自定义指标API获取每秒的总请求数：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests&quot; | jq .</span><br></pre></td></tr></table></figure><p><img src="/2019/08/28/k8s-hpa/media/534de9c48f8f9c01da152a780cdc9368.png"></p><p>注意：如果想让prometheus从pod抓取数据，需要在template声明annotations:中添加 prometheus.io/scrape: ‘true’</p><p><img src="/2019/08/28/k8s-hpa/media/a12b92db5f89feb0b1cbe01263e182bf.png"></p><h2 id="创建HPA"><a href="#创建HPA" class="headerlink" title="创建HPA"></a>创建HPA</h2><p>podinfo如果请求数超过每秒10个，将扩展部署</p><p>podinfo在default命名空间中部署HPA</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f ./podinfo/podinfo-hpa-custom.yaml</span><br></pre></td></tr></table></figure><p><img src="/2019/08/28/k8s-hpa/media/e27e33e324df09caaaf3748ccd82891c.png"></p><p>注意：不加m的时候，单位的默认是n<em>1000m，即上面是10</em>1000m=10000m</p><h2 id="安装-hey-压力测试工具"><a href="#安装-hey-压力测试工具" class="headerlink" title="安装 hey 压力测试工具"></a>安装 hey 压力测试工具</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run -dit -v /usr/local/bin:/go/bin golang:1.8 go get github.com/rakyll/hey</span><br><span class="line">export APP_ENDPOINT=$(kubectl get svc podinfo -o template --template &#123;&#123;.spec.clusterIP&#125;&#125;); echo $&#123;APP_ENDPOINT&#125;</span><br><span class="line">export APP_ENDPOINT_PORT=$(kubectl get svc podinfo -o yaml | grep port: | awk -F &#x27;: &#x27; &#x27;&#123;print $2&#125;&#x27;); </span><br><span class="line">echo $&#123;APP_ENDPOINT_PORT&#125;</span><br><span class="line">hey -n 10000 -q 5 -c 5</span><br></pre></td></tr></table></figure><p>执行请求测试后，可以看到pod数量变为了3</p><p><img src="/2019/08/28/k8s-hpa/media/721ddf000a950a306e7419edc8437ea4.png"></p><p>增大请求，pod数量变成4，停止后等待大约3分钟就可以缩容到最小副本数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hey -n 50000 -q 5 -c 5</span><br></pre></td></tr></table></figure><p><img src="/2019/08/28/k8s-hpa/media/77b90e208755d72445e6ac4a5f952679.png"><br><img src="/2019/08/28/k8s-hpa/media/f859ae2a4ddc72caba3a0fede4f0c88a.png"></p><h1 id="Object类型"><a href="#Object类型" class="headerlink" title="Object类型"></a>Object类型</h1><p>可支持的数据来源可以是service、endpoint等。</p><p>暂时无法实现，分析原因是需要在prometheus-adapter的配置文件中定义service，因为prometheus可以在service中声明annotations:中添加<br>prometheus.io/scrape: ‘true’</p><p>是可以获取到的metrics的，可是在api就无法访问到。</p><p>参考yaml：<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/sample-metrics">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/sample-metrics</a></p><h1 id="失败"><a href="#失败" class="headerlink" title="失败"></a>失败</h1><h2 id="部署prometheus"><a href="#部署prometheus" class="headerlink" title="部署prometheus"></a>部署prometheus</h2><p>参考我的values-hpav2.yaml文件地址：<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/prometheus">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/prometheus</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> helm delete prometheushpa --purge</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> helm install --name prometheushpa --namespace kube-system \</span></span><br><span class="line"><span class="bash">/root/charts/stable/prometheus \</span></span><br><span class="line"><span class="bash">-f /root/charts/stable/prometheus/values-hpav2.yaml</span></span><br></pre></td></tr></table></figure><p>部署的服务如下：</p><ul><li>  关闭告警插件alertmanager</li><li>  关闭节点收集器nodeexporter</li><li>  关闭网关pushgateway</li><li>  开启deployment收集器kubestatemetrics</li><li>  开启prometheus</li></ul><p>注意：我为方便与prometheus-operator安装prometheus区分，将服务命名为prometheushpa</p><h2 id="部署prometheus-adapter"><a href="#部署prometheus-adapter" class="headerlink" title="部署prometheus-adapter"></a>部署prometheus-adapter</h2><p>参考我的values-hpav2.yaml文件地址：<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/prometheus-adapter">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/hpav2/prometheus-adapter</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> helm delete prometheus-adapter --purge</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> helm install --name prometheus-adapter --namespace kube-system \</span></span><br><span class="line"><span class="bash">/root/charts/stable/prometheus-adapter \</span></span><br><span class="line"><span class="bash">-f /root/charts/stable/prometheus-adapter/values-hpav2.yaml</span></span><br></pre></td></tr></table></figure><p>注意：prometheus-adapter去连接prometheushpa时，helm方式安装的prometheus的service port是80，不是9090；当然你可以修改为9090</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这是一个k8s集群验证hpav2的功能&lt;/p&gt;</summary>
    
    
    
    <category term="Kubernetes" scheme="https://huisebug.github.io/categories/Kubernetes/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="hpav2" scheme="https://huisebug.github.io/tags/hpav2/"/>
    
    <category term="hpa" scheme="https://huisebug.github.io/tags/hpa/"/>
    
    <category term="custom.metrics" scheme="https://huisebug.github.io/tags/custom-metrics/"/>
    
    <category term="custom-metrics-api" scheme="https://huisebug.github.io/tags/custom-metrics-api/"/>
    
  </entry>
  
  <entry>
    <title>Prometheus-Operator监控k8s</title>
    <link href="https://huisebug.github.io/2019/08/27/Prometheus-Operator/"/>
    <id>https://huisebug.github.io/2019/08/27/Prometheus-Operator/</id>
    <published>2019-08-27T05:53:02.000Z</published>
    <updated>2021-07-09T02:57:20.670Z</updated>
    
    <content type="html"><![CDATA[<p>这是一篇Prometheus-Operator监控k8s服务的部署、监控原理讲解、监控配置、告警模板配置。</p><span id="more"></span><p>初始条件：</p><ul><li>  K8s1.12+集群</li></ul><h1 id="Prometheus-operator"><a href="#Prometheus-operator" class="headerlink" title="Prometheus-operator "></a>Prometheus-operator </h1><h2 id="安装monitoring-coreos-com-v1-api（prometheus-operator）"><a href="#安装monitoring-coreos-com-v1-api（prometheus-operator）" class="headerlink" title="安装monitoring.coreos.com/v1 api（prometheus-operator）"></a>安装monitoring.coreos.com/v1 api（prometheus-operator）</h2><p>此处我们直接使用yaml文件方式安装，不使用helm安装，helm安装会缺少一些服务，不方便我们更深了解</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/coreos/prometheus-operator.git</span><br></pre></td></tr></table></figure><p>已迁移到如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/coreos/kube-prometheus.git</span><br><span class="line">cd prometheus-operator/contrib/kube-prometheus</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/a75024b7d92536cdc10f54c8545a1c3b.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod --all-namespaces</span><br><span class="line">kubectl api-versions</span><br></pre></td></tr></table></figure><p>如果整个集群是否设置tain，解除即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i in api.huisebug.com node1.huisebug.com node2.huisebug.com; do kubectl taint nodes $i node-role.kubernetes.io/master:NoSchedule-; done</span><br></pre></td></tr></table></figure><h2 id="集群状态"><a href="#集群状态" class="headerlink" title="集群状态"></a>集群状态</h2><p>查看是否建立并启动好所有容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod --all-namespaces -o wide 或者 kubectl get pod -n monitoring</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/7fa22fa29b2998022851543d251dae94.png"></p><h2 id="问题汇总"><a href="#问题汇总" class="headerlink" title="问题汇总"></a>问题汇总</h2><ul><li>  <strong>Prometheus can’t access node-exporter and kube-state-metrics</strong></li></ul><p>解决地址</p><p><a href="https://github.com/coreos/prometheus-operator/issues/2330">https://github.com/coreos/prometheus-operator/issues/2330</a></p><ul><li>  alertmanager无法建立pod，主要是服务器环境硬件跟不上</li></ul><p>解决地址</p><p><a href="https://github.com/coreos/prometheus-operator/issues/1902">https://github.com/coreos/prometheus-operator/issues/1902</a></p><p><a href="https://github.com/coreos/prometheus-operator/issues/965">https://github.com/coreos/prometheus-operator/issues/965</a></p><ul><li>  prometheus的storage.local.retention（数据存储时间）设置更长时间</li></ul><p>解决地址</p><p><a href="https://github.com/coreos/prometheus-operator/issues/732">https://github.com/coreos/prometheus-operator/issues/732</a></p><p>只需在prometheus-prometheus.yaml文件中增加一行配置</p><p><img src="/2019/08/27/Prometheus-Operator/media/2fb7331e54c312463c5817ed4293e280.png"></p><p>主要介绍一下alertmanager无法建立pod的问题的解决方法，默认的alertmanager重启嗅探如下：</p><p><img src="/2019/08/27/Prometheus-Operator/media/5e37cd97ee9a0ad417dc44df9fbb2f64.png"></p><p>即嗅探10*10=100秒以后就会重启，显然模拟环境是无法在100秒内成功启动alertmanager，所以这里我们需要给配置文件alertmanager-alertmanager.yaml添加paused:true参数，添加步骤如下：</p><ol><li> 首先已经建立所有的prometheus-operator服务（kubectl apply -f manifests/ ）</li><li> 然后给alertmanager-alertmanager.yaml添加paused: true参数; paused: true参数解释参考地址：<a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerspec">https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerspec</a><br><img src="/2019/08/27/Prometheus-Operator/media/629d37574ccd3a5996ba69138c53299e.png"></li><li> 重载配置文件alertmanager-alertmanager.yaml （kubectl apply -f alertmanager-alertmanager.yaml）</li><li> 将配置文件alertmanager-alertmanager.yaml建立后生成的statefulset转储到文件中（kubectl get statefulsets alertmanager-main -n monitoring -o yaml &gt; alertmanager-main-statefulsets.yaml）</li><li> 在集群中删除alertmanager（kubectl delete -f alertmanager-main-statefulsets.yaml）</li><li> 修改文件alertmanager-main-statefulsets.yaml的嗅探失败次数，修改为30次，即30*10=300秒，也可根据你的实际环境来调节<br><img src="/2019/08/27/Prometheus-Operator/media/a5f43f5ed4d781c892b25e579d5eea8f.png"></li><li> 再次创建，即可成功建立alertmanager</li></ol><h1 id="解析prometheus-operator原理"><a href="#解析prometheus-operator原理" class="headerlink" title="解析prometheus-operator原理"></a>解析prometheus-operator原理</h1><p>新加了两个apiversion，获取命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl api-versions</span><br></pre></td></tr></table></figure><p>metrics.k8s.io/v1beta1<br>monitoring.coreos.com/v1</p><ul><li>  <strong>monitoring.coreos.com/v1是作为四个资源对象的组而添加到api中</strong></li><li>  <strong>metrics.k8s.io/v1beta1是使用资源对象APIService进行建立的，这个apiversion需关联到service对应的pod prometheus-adapter建立成功才会成功建立，所以记住需要使用kubectl api-versions命令关注是否建立成功</strong></li></ul><p>使用自定义资源定义CustomResourceDefinitions（CRD）新增4个kind：</p><ol><li> Prometheus，它定义了所需的Prometheus部署。运营商始终确保正在运行与资源定义匹配的部署。</li><li> ServiceMonitor，以声明方式指定应如何监控服务组。操作员根据定义自动生成Prometheus刮削配置。</li><li> PrometheusRule，它定义了一个所需的Prometheus规则文件，该文件可以由包含Prometheus警报和记录规则的Prometheus实例加载。</li><li> Alertmanager，它定义了所需的Alertmanager部署。运营商始终确保正在运行与资源定义匹配的部署。</li></ol><p><strong>获取命令如下</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get Servicemonitor --all-namespaces</span><br><span class="line">kubectl get Prometheus --all-namespaces</span><br></pre></td></tr></table></figure><h2 id="yaml类型"><a href="#yaml类型" class="headerlink" title="yaml类型"></a>yaml类型</h2><ol><li> 0prometheus-operator*：建立整个prometheus-operator的CRD、提供服务支持的pod运行</li><li> alertmanager*：prometheus所需的告警处理，alertmanager告警的pod建立</li><li> grafana*：监控页面展示</li><li> kube-state-metrics*：增加对deployment建立的pod、statefulset建立的pod等资源对象的metrics数据，参考地址：<a href="https://github.com/kubernetes/kube-state-metrics">https://github.com/kubernetes/kube-state-metrics</a></li><li> node-exporter*：运行k8s集群的node的服务器数据抓取</li><li> prometheus-adapter：为<strong>metrics.k8s.io/v1beta1</strong>提供metrics数据支持，获取集群的node和pod的CPU、内存使用情况，但是在这里只能获取到pod的，无法获取node的，后续将会在<strong>HPA V2</strong>章节讲解如何解决。</li><li> prometheus*：prometheus监控系统的pod建立，为需要添加到监控列表的服务增加metrics</li></ol><h2 id="简易原理"><a href="#简易原理" class="headerlink" title="简易原理"></a>简易原理</h2><p>数据源是从prometheus获取的，那么prometheus是如何获取k8s中这些数据的呢？</p><ol><li> 资源类型ServiceMonitor来定义添加的要被监控的k8s的服务service，然后使用operator工具来定义一个资源类型Prometheus进行筛选ServiceMonitor进行服务监控。PrometheusRule和Alertmanager进行规制和告警设置。</li></ol><h2 id="资源类型Prometheus"><a href="#资源类型Prometheus" class="headerlink" title="资源类型Prometheus"></a>资源类型Prometheus</h2><p>这里我们来深度解析下这个资源类型为Prometheus</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get Prometheus --all-namespaces -o yaml &gt; k8s-prometheus.yaml</span><br></pre></td></tr></table></figure><p>去除一些不必要的信息后，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">items:</span><br><span class="line">- apiVersion: monitoring.coreos.com/v1</span><br><span class="line">  kind: Prometheus</span><br><span class="line">  metadata:</span><br><span class="line">    labels:</span><br><span class="line">      prometheus: k8s</span><br><span class="line">    name: k8s</span><br><span class="line">    namespace: monitoring</span><br><span class="line">  spec:</span><br><span class="line">    alerting:</span><br><span class="line">      alertmanagers:</span><br><span class="line">      - name: alertmanager-main</span><br><span class="line">        namespace: monitoring</span><br><span class="line">        port: web</span><br><span class="line">    baseImage: quay.io/prometheus/prometheus</span><br><span class="line">    nodeSelector:</span><br><span class="line">      beta.kubernetes.io/os: linux</span><br><span class="line">    replicas: 2</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: 400Mi</span><br><span class="line">    ruleSelector:</span><br><span class="line">      matchLabels:</span><br><span class="line">        prometheus: k8s</span><br><span class="line">        role: alert-rules</span><br><span class="line">    securityContext:</span><br><span class="line">      fsGroup: 2000</span><br><span class="line">      runAsNonRoot: true</span><br><span class="line">      runAsUser: 1000</span><br><span class="line">    serviceAccountName: prometheus-k8s</span><br><span class="line">    serviceMonitorNamespaceSelector: &#123;&#125;</span><br><span class="line">    serviceMonitorSelector: &#123;&#125;</span><br><span class="line">    version: v2.5.0</span><br><span class="line">kind: List</span><br><span class="line">metadata:</span><br><span class="line">  resourceVersion: &quot;&quot;</span><br><span class="line">  selfLink: &quot;&quot;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">kind: List</span><br><span class="line">metadata:</span><br><span class="line">  resourceVersion: &quot;&quot;</span><br><span class="line">  selfLink: &quot;&quot;</span><br><span class="line">这一段是定义多个，不必在意。</span><br><span class="line"></span><br><span class="line">- apiVersion: monitoring.coreos.com/v1</span><br><span class="line">  kind: Prometheus</span><br><span class="line">  metadata:</span><br><span class="line">    labels:</span><br><span class="line">      prometheus: k8s</span><br><span class="line">    name: k8s</span><br><span class="line">    namespace: monitoring</span><br><span class="line">这是基础的头定义，定义了api类型，资源对象类型，标签，名称，命名空间。</span><br><span class="line"></span><br><span class="line">alerting:</span><br><span class="line">      alertmanagers:</span><br><span class="line">      - name: alertmanager-main</span><br><span class="line">        namespace: monitoring</span><br><span class="line">        port: web</span><br><span class="line">这是定义告警方式的alertmanager的k8s服务service名称（kubectl get svc –all-namespaces）获取查看，web是定义service时spec.ports下的一个名称name（kubectl get svc alertmanager-main -n monitoring -o yaml）获取查看。</span><br><span class="line"></span><br><span class="line">baseImage: quay.io/prometheus/prometheus</span><br><span class="line">    nodeSelector:</span><br><span class="line">      beta.kubernetes.io/os: linux</span><br><span class="line">    replicas: 2</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: 400Mi</span><br><span class="line">设置prometheus建立pod时的参数，基础镜像，节点选择，副本因子数，资源配额。</span><br><span class="line"></span><br><span class="line">ruleSelector:</span><br><span class="line">      matchLabels:</span><br><span class="line">        prometheus: k8s</span><br><span class="line">        role: alert-rules</span><br><span class="line">规则选择器，依照标签来选择规则列表，获取规则列表可以（kubectl get prometheusrule prometheus-k8s-rules -n monitoring），prometheus-operator添加api时候定义了一个新的资源类型PrometheusRule，如何查看添加哪些新的kind呢，我是依照安装prometheus-operator时候会建立资源对象推断出的</span><br><span class="line"></span><br><span class="line">    securityContext:</span><br><span class="line">      fsGroup: 2000</span><br><span class="line">      runAsNonRoot: true</span><br><span class="line">      runAsUser: 1000</span><br><span class="line">安全上下文配置，这里我猜测应该是基于容器container的安全上下文配置。具体参考官方</span><br><span class="line">https://www.kubernetes.org.cn/security-context-psp</span><br><span class="line"></span><br><span class="line">serviceAccountName: prometheus-k8s</span><br><span class="line">RBAC认证账户</span><br><span class="line"></span><br><span class="line">serviceMonitorNamespaceSelector: &#123;&#125;</span><br><span class="line">服务监控（ServiceMonitor）命名空间选择，这里是匹配所有；</span><br><span class="line"></span><br><span class="line">serviceMonitorSelector: &#123;&#125;</span><br><span class="line">对（ServiceMonitor）进行筛选，这里也是匹配所有</span><br><span class="line"></span><br><span class="line">version: v2.5.0</span><br><span class="line">镜像的版本号，即prometheus的版本号</span><br><span class="line"></span><br><span class="line">建立的pod是使用statefulsets部署的</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="资源类型PrometheusRule"><a href="#资源类型PrometheusRule" class="headerlink" title="资源类型PrometheusRule"></a>资源类型PrometheusRule</h2><p>专门用于prometheus的rules配置,执行以下命令来参照修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get prometheusrule prometheus-k8s-rules -n monitoring -o yaml</span><br></pre></td></tr></table></figure><p>后续将会在企业微信告警配置中讲解</p><h2 id="资源类型ServiceMonitor"><a href="#资源类型ServiceMonitor" class="headerlink" title="资源类型ServiceMonitor"></a>资源类型ServiceMonitor</h2><p>kubectl get Servicemonitor –all-namespaces<br>以下举例三个不同namespace下的service进行说明</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">kubectl get Servicemonitor kube-apiserver -n monitoring -o yaml &gt; kube-apiserver-servicemonitor.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: monitoring.coreos.com/v1</span><br><span class="line">kind: ServiceMonitor</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: apiserver</span><br><span class="line">  name: kube-apiserver</span><br><span class="line">  namespace: monitoring</span><br><span class="line">spec:</span><br><span class="line">  endpoints:</span><br><span class="line">  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token</span><br><span class="line">    interval: 30s</span><br><span class="line">    metricRelabelings:</span><br><span class="line">    - action: drop</span><br><span class="line">      regex: etcd_(debugging|disk|request|server).*</span><br><span class="line">      sourceLabels:</span><br><span class="line">      - __name__</span><br><span class="line">    port: https</span><br><span class="line">    scheme: https</span><br><span class="line">    tlsConfig:</span><br><span class="line">      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt</span><br><span class="line">      serverName: kubernetes</span><br><span class="line">  jobLabel: component</span><br><span class="line">  namespaceSelector:</span><br><span class="line">    matchNames:</span><br><span class="line">    - default</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      component: apiserver</span><br><span class="line">      provider: kubernetes</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">kubectl get Servicemonitor node-exporter -n monitoring -o yaml &gt; node-exporter-servicemonitor.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: monitoring.coreos.com/v1</span><br><span class="line">kind: ServiceMonitor</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: node-exporter</span><br><span class="line">  name: node-exporter</span><br><span class="line">  namespace: monitoring</span><br><span class="line">spec:</span><br><span class="line">  endpoints:</span><br><span class="line">  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token</span><br><span class="line">    interval: 30s</span><br><span class="line">    port: https</span><br><span class="line">    scheme: https</span><br><span class="line">    tlsConfig:</span><br><span class="line">      insecureSkipVerify: true</span><br><span class="line">  jobLabel: k8s-app</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: node-exporter</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">kubectl get Servicemonitor coredns -n monitoring -o yaml &gt; coredns-servicemonitor.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: monitoring.coreos.com/v1</span><br><span class="line">kind: ServiceMonitor</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: coredns</span><br><span class="line">  name: coredns</span><br><span class="line">  namespace: monitoring</span><br><span class="line">spec:</span><br><span class="line">  endpoints:</span><br><span class="line">  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token</span><br><span class="line">    interval: 15s</span><br><span class="line">    port: metrics</span><br><span class="line">  namespaceSelector:</span><br><span class="line">    matchNames:</span><br><span class="line">    - kube-system</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: kube-dns</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">endpoints:   #端点配置</span><br><span class="line">  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token  #serviceaccount自动建立的token</span><br><span class="line">    interval: 30s       #间隔时间</span><br><span class="line">    metricRelabelings:    </span><br><span class="line">    - action: drop</span><br><span class="line">      regex: etcd_(debugging|disk|request|server).*</span><br><span class="line">      sourceLabels:</span><br><span class="line">      - __name__</span><br><span class="line">    port: https        #端点配置中的port定义名称，使用kubectl get ep ep名称 -o yaml 可以查看到 </span><br><span class="line">scheme: https       #访问metrics的方式，一般是http和https，但是源代码是没有固定参数，可以任意传字符串，所以这里还是写http或者https</span><br><span class="line">tlsConfig: #在抓取端点时使用的TLS配置</span><br><span class="line"></span><br><span class="line">namespaceSelector:        #命名空间选择器</span><br><span class="line">    matchNames:     #当监控的pod与servicemonitor不在同一个命名空间就需要进行筛选，使用any： true匹配所有命名空间，不然就写matchNames来匹配</span><br><span class="line">    - default</span><br><span class="line"></span><br><span class="line">selector:        #service标签选择器，kubectl get svc kubernetes -o yaml进行查看，注意svc必须在metadata.label下声明才能进行匹配。</span><br><span class="line">    matchLabels:</span><br><span class="line">      component: apiserver</span><br><span class="line">      provider: kubernetes</span><br></pre></td></tr></table></figure><p>官方解析地址如下：<br><a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md">https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md</a></p><h2 id="资源对象Alertmanagers"><a href="#资源对象Alertmanagers" class="headerlink" title="资源对象Alertmanagers"></a>资源对象Alertmanagers</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">kubectl get alertmanagers main -n monitoring -o yaml &gt; main-alertmanagers.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">items:</span><br><span class="line">- apiVersion: monitoring.coreos.com/v1</span><br><span class="line">  kind: Alertmanager</span><br><span class="line">  metadata:</span><br><span class="line">    labels:</span><br><span class="line">      alertmanager: main</span><br><span class="line">    name: main</span><br><span class="line">    namespace: monitoring</span><br><span class="line">  spec:</span><br><span class="line">    baseImage: quay.io/prometheus/alertmanager</span><br><span class="line">    nodeSelector:</span><br><span class="line">      beta.kubernetes.io/os: linux</span><br><span class="line">    replicas: 3</span><br><span class="line">    securityContext:</span><br><span class="line">      fsGroup: 2000</span><br><span class="line">      runAsNonRoot: true</span><br><span class="line">      runAsUser: 1000</span><br><span class="line">    serviceAccountName: alertmanager-main</span><br><span class="line">    version: v0.15.3</span><br><span class="line">kind: List</span><br><span class="line">metadata:</span><br><span class="line">  resourceVersion: &quot;&quot;</span><br><span class="line">  selfLink: &quot;&quot;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">- apiVersion: monitoring.coreos.com/v1</span><br><span class="line">  kind: Alertmanager</span><br><span class="line">  metadata:</span><br><span class="line">    labels:</span><br><span class="line">      alertmanager: main</span><br><span class="line">    name: main</span><br><span class="line">    namespace: monitoring</span><br><span class="line">这是基础的头定义，定义了api类型，资源对象类型，标签，名称，命名空间。</span><br><span class="line">spec:</span><br><span class="line">baseImage: quay.io/prometheus/alertmanager</span><br><span class="line">    nodeSelector:</span><br><span class="line">      beta.kubernetes.io/os: linux</span><br><span class="line">    replicas: 3</span><br><span class="line">    securityContext:</span><br><span class="line">      fsGroup: 2000</span><br><span class="line">      runAsNonRoot: true</span><br><span class="line">      runAsUser: 1000</span><br><span class="line">        </span><br><span class="line">设置prometheus建立pod时的参数，基础镜像，节点选择，副本因子数，安全上下文。</span><br><span class="line">安全上下文配置，这里我猜测应该是基于容器container的安全上下文配置。具体参考官方https://www.kubernetes.org.cn/security-context-psp</span><br><span class="line"></span><br><span class="line">serviceAccountName: alertmanager-main</span><br><span class="line">RBAC认证账户</span><br><span class="line"></span><br><span class="line">version: v0.15.3</span><br><span class="line">镜像的版本号，即alertmanager的版本号</span><br><span class="line"></span><br><span class="line">建立的pod是使用statefulsets部署的，会建立一个无头服务service：</span><br><span class="line">serviceName: alertmanager-operated</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>使用命令kubectl get statefulsets alertmanager-main -n monitoring -o yaml可以查看到；</strong><br><img src="/2019/08/27/Prometheus-Operator/media/d169d095774698249244fe235ae40837.png"></p><h1 id="验证效果"><a href="#验证效果" class="headerlink" title="验证效果"></a>验证效果</h1><p>查看建立的service，我们需要访问prometheus数据源和grafana的监控展示web-UI界面，如何访问呢，可以使用service的Nodeport方式，显然这是非常lose的，这里我使用traefik代理方式（traefik安装就不赘述了），建立ingress</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">第一步，查看service</span><br><span class="line">kubectl get svc --all-namespaces</span><br><span class="line"></span><br><span class="line">第二步，查看ingress</span><br><span class="line">kubectl get ingress --all-namespaces</span><br><span class="line"></span><br><span class="line">新建一个ingress在命名空间monitoring下的</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus-k8s-grafana</span><br><span class="line">  namespace: monitoring</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">    - host: prometheus.huisebug.com</span><br><span class="line">      http:</span><br><span class="line">        paths:</span><br><span class="line">          - backend:</span><br><span class="line">              serviceName: prometheus-k8s</span><br><span class="line">              servicePort: 9090</span><br><span class="line">            path: /</span><br><span class="line">    - host: grafana.huisebug.com</span><br><span class="line">      http:</span><br><span class="line">        paths:</span><br><span class="line">          - backend:</span><br><span class="line">              serviceName: grafana</span><br><span class="line">              servicePort: 3000</span><br><span class="line">            path: /</span><br></pre></td></tr></table></figure><p>给客户机的hosts添加本地解析记录<br>192.168.137.13 prometheus.huisebug.com grafana.huisebug.com</p><h2 id="grafana-huisebug-com"><a href="#grafana-huisebug-com" class="headerlink" title="grafana.huisebug.com"></a>grafana.huisebug.com</h2><p>grafana.huisebug.com需要密码验证用户名admin 密码admin并提示修改初始密码</p><p>grafana已经添加了k8s集群的监控</p><p><img src="/2019/08/27/Prometheus-Operator/media/c94d7f6a843e9df190914a2abca78c2e.png"></p><h2 id="prometheus-huisebug-com"><a href="#prometheus-huisebug-com" class="headerlink" title="prometheus.huisebug.com"></a>prometheus.huisebug.com</h2><p><img src="/2019/08/27/Prometheus-Operator/media/650ae2d1f247399562f4fb6ce90f9515.png"></p><p>从上面可以看到没有获取到kube-controller-manager、kube-scheduler的metric，接下来就解决这个问题，因为这2个服务不是使用kubeadm方式部署的是使用二进制部署的，所以无法获取到。使用kubeadm方式部署的服务都是存在于namespace:kube-system中，例如：</p><p><img src="/2019/08/27/Prometheus-Operator/media/d37b4268d6fb43051831ca9581da35bd.png"></p><h1 id="metrics"><a href="#metrics" class="headerlink" title="metrics"></a>metrics</h1><h2 id="解决scheduler和controller-manager的metric"><a href="#解决scheduler和controller-manager的metric" class="headerlink" title="解决scheduler和controller-manager的metric"></a>解决scheduler和controller-manager的metric</h2><p>想要解决上述问题，先来查看整个集群的service和endpoint<br><img src="/2019/08/27/Prometheus-Operator/media/9e6546ab25a4f4d1cce61a3413baa3db.png"><br>可以看到：</p><ul><li>  service中不存在kube-controller-manager、kube-scheduler</li><li>  endpoint中都有</li></ul><p>所以我们需要建立对应的service和修改endpoint的参数</p><p>注意新建立service的port名称要与serviceMonitor配置文件对应的上<br><img src="/2019/08/27/Prometheus-Operator/media/b3309bb846a64600ecc3293dfeaba2ab.png"></p><p>参考地址,记得修改其中的地址为你的服务器node_ip地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Prometheus-operator/metrics/scheduler_controller-manager/metics_services.yaml</span><br></pre></td></tr></table></figure><p>如果建立后还是无法访问记得使用命令netstat -lntp查看端口是否设置是所有网卡可以访问，如果是127.0.0.1，就修改服务的启动参数，设置为0.0.0.0</p><p>上述修改完毕后，就可以再次查看是否成功获取到<br><img src="/2019/08/27/Prometheus-Operator/media/d79e172a8e29f7deddbe41ea5223e99f.png"></p><h2 id="mysql-metrics"><a href="#mysql-metrics" class="headerlink" title="mysql-metrics"></a>mysql-metrics</h2><p>此处我们建立一个mysql服务的metric验证</p><h3 id="首先建立一个mysql服务"><a href="#首先建立一个mysql服务" class="headerlink" title="首先建立一个mysql服务"></a>首先建立一个mysql服务</h3><p>参考地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Prometheus-operator/metrics/mysql-metrics/mysql-metric.yaml</span><br></pre></td></tr></table></figure><p>测试是否成功建立mysql服务命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it mysql sh -c &#x27;exec mysql -h192.168.137.13 -P32306 -pmysql -uroot&#x27;</span><br></pre></td></tr></table></figure><p>如果能登录，说明mysql服务已经建立<br><img src="/2019/08/27/Prometheus-Operator/media/79bbd7c67dd266a4e97e04147ac8aa07.png"></p><h3 id="使用helm方式安装mysql-exporter"><a href="#使用helm方式安装mysql-exporter" class="headerlink" title="使用helm方式安装mysql-exporter"></a>使用helm方式安装mysql-exporter</h3><p>找到到之前安装traefik的官方charts目录；<br>进入到stable/prometheus-mysql-exporter目录下就是mysql-exporter的chart了；<br>values.yaml文件中要指定mysql服务的用户和密码。然后再安装<br><img src="/2019/08/27/Prometheus-Operator/media/ffd56c5b5235fb6b8e27c715f6d36dd5.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm install -f values.yaml --name mysql-exporter .</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/99ce34a2797daee5e18254382881bf5d.png"></p><h3 id="访问mysql-exporter是否有metric生成"><a href="#访问mysql-exporter是否有metric生成" class="headerlink" title="访问mysql-exporter是否有metric生成"></a>访问mysql-exporter是否有metric生成</h3><p>这里同样采用traefik代理来访问，建立ingress，上述两个pod都是建立在default命名空间的。</p><p>ingress yaml文件参考地址：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/Prometheus-operator/metrics/mysql-metrics/mysql-metric.yaml</span><br></pre></td></tr></table></figure><p>本地解析添加<br>192.168.137.13 mysqlexporter.huisebug.com</p><p>验证访问<a href="http://mysqlexporter.huisebug.com/metrics">http://mysqlexporter.huisebug.com/metrics</a><br><img src="/2019/08/27/Prometheus-Operator/media/1e0514c33f9c540cdfc6880f42a3d971.png"></p><h3 id="ServiceMonitor资源对象建立"><a href="#ServiceMonitor资源对象建立" class="headerlink" title="ServiceMonitor资源对象建立"></a>ServiceMonitor资源对象建立</h3><p>查看service的对应标签，用于servicemonitor</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc mysql-exporter-prometheus-mysql-exporter -o yaml| grep -1 labels</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/20f49c74e35290151302f7657fac6d16.png"></p><p>参考yaml地址<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/Prometheus-operator/metrics/mysql-metrics/mysql-metric.yaml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/Prometheus-operator/metrics/mysql-metrics/mysql-metric.yaml</a></p><p><font color="red" size="6">记住servicemonitor必须建立在monitor命名空间下</font></p><h3 id="Prometheus资源对象建立"><a href="#Prometheus资源对象建立" class="headerlink" title="Prometheus资源对象建立"></a>Prometheus资源对象建立</h3><p>promethes资源对象在安装<strong>monitoring.coreos.com/v1</strong> api的时候建立了一个匹配所有servicemonitot的prometheus资源对象，所以这里我就不建立了，可以参考以下地址：<br><a href="https://github.com/coreos/prometheus-operator/blob/master/contrib/kube-prometheus/manifests/prometheus-prometheus.yaml">https://github.com/coreos/prometheus-operator/blob/master/contrib/kube-prometheus/manifests/prometheus-prometheus.yaml</a></p><h3 id="验证效果-1"><a href="#验证效果-1" class="headerlink" title="验证效果"></a>验证效果</h3><p>再次访问prometheus的target页面查看是否成功获取</p><p><img src="/2019/08/27/Prometheus-Operator/media/0ce7ae823851da215e533b770df42b93.png"></p><p>成功建立mysql-metric</p><h2 id="traefik-metrics"><a href="#traefik-metrics" class="headerlink" title="traefik-metrics"></a>traefik-metrics</h2><h3 id="metrics建立"><a href="#metrics建立" class="headerlink" title="metrics建立"></a>metrics建立</h3><p>之前建立traefik的时候开启dashboard.enabled=true,dashboard.domain=traefik.huisebug.com,metrics.prometheus.enabled=true<br>这样我们就可以访问到traefik的metrics</p><p>访问地址：<a href="http://traefik.huisebug.com/metrics">http://traefik.huisebug.com/metrics</a></p><p><img src="/2019/08/27/Prometheus-Operator/media/45a04fd447062832133a6db5fa9f7135.png"></p><h3 id="ServiceMonitor资源对象建立-1"><a href="#ServiceMonitor资源对象建立-1" class="headerlink" title="ServiceMonitor资源对象建立"></a>ServiceMonitor资源对象建立</h3><p>参考yaml文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Prometheus-operator/metrics/traefik-metrics/traefik-metrics.yaml</span><br></pre></td></tr></table></figure><h3 id="验证效果-2"><a href="#验证效果-2" class="headerlink" title="验证效果"></a>验证效果</h3><p><img src="/2019/08/27/Prometheus-Operator/media/6412727c854d8dec8afa4048d4372318.png"></p><h1 id="alertmanager告警"><a href="#alertmanager告警" class="headerlink" title="alertmanager告警"></a>alertmanager告警</h1><p>alertmanager也是提供了web访问控制台的，先使用ingress进行访问</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/huisebug/k8s1.12-Ecosphere/master/Prometheus-operator/alertmanager/alertmanager-ingress.yaml</span><br></pre></td></tr></table></figure><p>记得本地添加解析记录</p><p>访问效果</p><p><img src="/2019/08/27/Prometheus-Operator/media/83a70cb03cfb322a21960c1fe68311b4.png"></p><p>可以看到告警信息这么多，无法找到接收者，所以堆积了。</p><p>查看alertmanager的配置参数</p><p><img src="/2019/08/27/Prometheus-Operator/media/e4b8bd6bf0c5a44e1066f84b7e374a75.png"></p><p>下面演示如何用钉钉群组来接收告警信息</p><h2 id="钉钉webhook报警2-0版本8060"><a href="#钉钉webhook报警2-0版本8060" class="headerlink" title="钉钉webhook报警2.0版本8060"></a>钉钉webhook报警2.0版本8060</h2><h3 id="安装钉钉插件"><a href="#安装钉钉插件" class="headerlink" title="安装钉钉插件"></a>安装钉钉插件</h3><h4 id="物理机安装"><a href="#物理机安装" class="headerlink" title="物理机安装"></a>物理机安装</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/timonwong/prometheus-webhook-dingtalk/releases/download/v0.3.0/prometheus-webhook-dingtalk-0.3.0.linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">tar解压后启动</span><br><span class="line"></span><br><span class="line">nohup ./prometheus-webhook-dingtalk --ding.profile=&quot;ops_dingding=钉钉机器人处复制webhook&quot; 2&gt;&amp;1 1&gt;dingding.log &amp;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>物理方式安装就不过多介绍</p><h4 id="Docker单机容器部署"><a href="#Docker单机容器部署" class="headerlink" title="Docker单机容器部署"></a>Docker单机容器部署</h4><h5 id="钉钉webhook-容器建立"><a href="#钉钉webhook-容器建立" class="headerlink" title="钉钉webhook 容器建立"></a>钉钉webhook 容器建立</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --restart=always -p 8060:8060 --name=prometheus-webhook-dingtalk timonwong/prometheus-webhook-dingtalk --ding.profile=&quot;ops_dingding=钉钉机器人处复制webhook&quot;</span><br></pre></td></tr></table></figure><h5 id="修改alertmanager-yaml内容"><a href="#修改alertmanager-yaml内容" class="headerlink" title="修改alertmanager.yaml内容"></a>修改alertmanager.yaml内容</h5><p>此处需要修改/root/prometheus-operator-history/contrib/kube-prometheus/manifests/alertmanager-secret.yaml文件，其中的data数据是经过base64加密的，复制出来随便去一个base64解码即可看到。或者运行下面的命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat alertmanager-secret.yaml | grep alertmanager.yaml | cut -f 2- -d &quot;:&quot; | tr -d &quot; &quot; | base64 –d</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/ce16fd528480ca5dc6accbe565813805.png"></p><p>我们需要将secret中的alertmanager.yaml修改为如下的内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">  resolve_timeout: 1h</span><br><span class="line">route:</span><br><span class="line">  receiver: webhook</span><br><span class="line">  group_wait: 30s</span><br><span class="line">  group_interval: 30s</span><br><span class="line">  repeat_interval: 10m</span><br><span class="line">  group_by: [alertname]</span><br><span class="line">  routes:</span><br><span class="line">  - receiver: webhook</span><br><span class="line">    group_wait: 30s</span><br><span class="line">    match:</span><br><span class="line">      team: node</span><br><span class="line">receivers:</span><br><span class="line">- name: webhook</span><br><span class="line">  webhook_configs:</span><br><span class="line">  - url: http://192.168.137.13:8060/dingtalk/ops_dingding/send</span><br><span class="line">    send_resolved: true</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>上述的地址记得修改为你的服务器IP地址</p><p>首先在同一级目录下建立一个alertmanager.yaml文件，内容就是如上</p><p>运行如下命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ALT_S=$(cat alertmanager-secret.yaml | grep alertmanager.yaml | cut -f 2- -d &quot;:&quot; | tr -d &quot; &quot;)</span><br><span class="line">ALT_D=$(cat alertmanager.yaml | base64 | tr -d &quot;n&quot; )</span><br><span class="line">sed -i &quot;s/$ALT_S/$ALT_D/&quot; ./alertmanager-secret.yaml</span><br></pre></td></tr></table></figure><p>即可完成替换，然后重新建立secret，从之前的alertmanager-main-statefulsets.yaml文件中可以看出secret是挂载到卷的，不是环境变量的方式，这样是支持热更新的</p><p><img src="/2019/08/27/Prometheus-Operator/media/669cdd83d0ce476a8c71ed066323249f.png"></p><h1 id="执行修改后的secret文件"><a href="#执行修改后的secret文件" class="headerlink" title="执行修改后的secret文件"></a>执行修改后的secret文件</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f alertmanager-secret.yaml</span><br></pre></td></tr></table></figure><p>查看是否修改成功</p><p><img src="/2019/08/27/Prometheus-Operator/media/7ee31846b6d89f96cc4906984556d0e6.png"></p><h4 id="集群方式部署"><a href="#集群方式部署" class="headerlink" title="集群方式部署"></a>集群方式部署</h4><p>都失败了，暂时放弃这种想法了。</p><h5 id="hostnetwork方式访问"><a href="#hostnetwork方式访问" class="headerlink" title="hostnetwork方式访问"></a>hostnetwork方式访问</h5><p>参考yaml地址<br><a href="https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/Prometheus-operator/alertmanager/hostnetwork%E6%96%B9%E5%BC%8F">https://github.com/huisebug/k8s1.12-Ecosphere/tree/master/Prometheus-operator/alertmanager/hostnetwork%E6%96%B9%E5%BC%8F</a></p><h5 id="service方式访问"><a href="#service方式访问" class="headerlink" title="service方式访问"></a>service方式访问</h5><p>这种方式会存在问题</p><p>alertmanager的日志</p><p><img src="/2019/08/27/Prometheus-Operator/media/7bd2e8ce4126f01a2e22c66c7409552c.png"></p><p>钉钉插件日志</p><p><img src="/2019/08/27/Prometheus-Operator/media/e4c36138aa9877ebd76c385982790997.png"></p><p>参考yaml文件</p><p><a href="https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/Prometheus-operator/alertmanager/dd_webhook-deployment-svc.yaml">https://github.com/huisebug/k8s1.12-Ecosphere/blob/master/Prometheus-operator/alertmanager/dd_webhook-deployment-svc.yaml</a></p><p>需要将其中的webhook设置成你的</p><h6 id="修改alertmanager-yaml内容-1"><a href="#修改alertmanager-yaml内容-1" class="headerlink" title="修改alertmanager.yaml内容"></a>修改alertmanager.yaml内容</h6><p>此处需要修改/root/prometheus-operator-history/contrib/kube-prometheus/manifests/alertmanager-secret.yaml文件，其中的data数据是经过base64加密的，为了不与上面混淆，将其复制并命名为alertmanager-secret-k8sdd.yaml</p><p>我们需要将secret中的alertmanager.yaml修改为如下的内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">  resolve_timeout: 1h</span><br><span class="line">route:</span><br><span class="line">  receiver: webhook</span><br><span class="line">  group_wait: 30s</span><br><span class="line">  group_interval: 30s</span><br><span class="line">  repeat_interval: 10m</span><br><span class="line">  group_by: [alertname]</span><br><span class="line">  routes:</span><br><span class="line">  - receiver: webhook</span><br><span class="line">    group_wait: 30s</span><br><span class="line">    match:</span><br><span class="line">      team: node</span><br><span class="line">receivers:</span><br><span class="line">- name: webhook</span><br><span class="line">  webhook_configs:</span><br><span class="line">  - url: http://dd-webhook:8060/dingtalk/ops_dingding/send</span><br><span class="line">    send_resolved: true</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>首先在同一级目录下建立一个alertmanager-k8sdd.yaml文件，内容就是如上，修改了地址为service名称<br>运行如下命令，记住alertmanager-k8sdd.yaml不能存在注释符号</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ALT_S=$(cat alertmanager-secret-k8sdd.yaml | grep alertmanager.yaml | cut -f 2- -d &quot;:&quot; | tr -d &quot; &quot;)</span><br><span class="line">ALT_D=$(cat alertmanager-k8sdd.yaml | base64 | tr -d &quot;n&quot; )</span><br><span class="line">sed -i &quot;s/$ALT_S/$ALT_D/&quot; ./alertmanager-secret-k8sdd.yaml</span><br></pre></td></tr></table></figure><p>上述方法我感觉是钉钉插件镜像的代码问题，镜像作者也很久未更新了，所以暂时放弃service方式访问</p><h2 id="企业微信报警"><a href="#企业微信报警" class="headerlink" title="企业微信报警"></a>企业微信报警</h2><p>后续的所有操作都可在以下GitHub地址找到<br><a href="https://github.com/huisebug/Prometheus-Operator">https://github.com/huisebug/Prometheus-Operator</a><br>后续演示的还是基于官方的github来操作</p><h3 id="建立企业微信账户"><a href="#建立企业微信账户" class="headerlink" title="建立企业微信账户"></a>建立企业微信账户</h3><p>step 1: 访问网站<a href="https://work.weixin.qq.com/">https://work.weixin.qq.com/</a> 注册企业微信账号（不需要企业认证）。<br>step 2: 访问apps 创建应用，点击 创建应用按钮 -&gt; 填写应用信息：</p><p><img src="/2019/08/27/Prometheus-Operator/media/2167460f7929e9f8d6b804602d053162.png"></p><p>如果有则忽略</p><h3 id="修改alertmanager-yaml内容-2"><a href="#修改alertmanager-yaml内容-2" class="headerlink" title="修改alertmanager.yaml内容"></a>修改alertmanager.yaml内容</h3><p>此处需要修改kube-prometheus/manifests/alertmanager-secret.yaml文件，其中的data数据是经过base64加密的，复制出来随便去一个base64解码即可看到。或者运行下面的命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat alertmanager-secret.yaml | grep alertmanager.yaml | cut -f 2- -d &quot;:&quot; | tr -d &quot; &quot; | base64 –d</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/ce16fd528480ca5dc6accbe565813805.png"></p><p>我们需要将secret中的alertmanager.yaml修改为如下的内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">  resolve_timeout: 1h</span><br><span class="line">route:</span><br><span class="line">  group_by: [&#x27;alertname&#x27;]</span><br><span class="line">  receiver: &#x27;wechat&#x27;</span><br><span class="line">  group_wait: 30s</span><br><span class="line">  group_interval: 30s</span><br><span class="line">  repeat_interval: 10m</span><br><span class="line"></span><br><span class="line">receivers:</span><br><span class="line">- name: &#x27;wechat&#x27;</span><br><span class="line">  wechat_configs:</span><br><span class="line">  - corp_id: &#x27;企业ID&#x27;</span><br><span class="line">    to_party: &#x27;组ID&#x27;</span><br><span class="line">    agent_id: &#x27;应用ID&#x27;</span><br><span class="line">    api_secret: &#x27;应用密钥&#x27;</span><br><span class="line">    #是否发送恢复信息</span><br><span class="line">    send_resolved: true</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>  corp_id: 企业微信账号唯一 ID， 可以在”我的企业”中查看。</li><li>  to_party: 需要发送的组(部门ID)。即<br><img src="/2019/08/27/Prometheus-Operator/media/a5971d9b5aec90857f23a49be18077d8.png"></li><li>  agent_id: 第三方企业应用的 ID，可以在自己创建的第三方企业应用详情页面查看。</li><li>  api_secret:第三方企业应用的密钥，可以在自己创建的第三方企业应用详情页面查看。<br><img src="/2019/08/27/Prometheus-Operator/media/09f014e63904cbd46b50e91e05cf679b.png"><br>将上面修改好的内容进行base64加密，然后copy到alertmanager-secret.yaml中，然后在alertmanager web页面进行查看</li></ul><p><img src="/2019/08/27/Prometheus-Operator/media/53c9c0f213c0f6922474fae21b5d5048.png"></p><h3 id="建立资源类型PrometheusRule"><a href="#建立资源类型PrometheusRule" class="headerlink" title="建立资源类型PrometheusRule"></a>建立资源类型PrometheusRule</h3><p>默认的在建立<a href="https://github.com/huisebug/Prometheus-Operator">Prometheus-Operator</a>时候会建立官方推荐的一些告警规则，为便于测试，可暂时删除prometheus-rules.yaml中的资源类型PrometheusRule规则</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl --kubeconfig=./kubeconfig/sre.kubeconfig delete -f kube-prometheus/manifests/prometheus-rules.yaml</span><br></pre></td></tr></table></figure><p>然后执行我新建的一些简单告警规则资源类型PrometheusRule（服务状态规则，mysql服务状态规则）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl --kubeconfig=./kubeconfig/sre.kubeconfig apply -f kube-prometheus/manifests/new/mysql-rules.yaml</span><br><span class="line">kubectl --kubeconfig=./kubeconfig/sre.kubeconfig apply -f kube-prometheus/manifests/new/state-rules.yaml</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/c601e3e34c2264ac2a41bb0ce001479f.png"></p><h3 id="建立对应的告警目标服务"><a href="#建立对应的告警目标服务" class="headerlink" title="建立对应的告警目标服务"></a>建立对应的告警目标服务</h3><p>根据上面我们建立的告警规则，我们需要建立mysql服务，mysql监控服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl --kubeconfig=./kubeconfig/sre.kubeconfig apply -f kube-prometheus/manifests/new/mysql-rc.yaml</span><br><span class="line">kubectl --kubeconfig=./kubeconfig/sre.kubeconfig apply -fkube-prometheus/manifests/new/mysql-exporter.yaml</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/fa3d85ccd71b489a0321dde1c4f61e60.png"></p><p>在prometheus web页面查看</p><p><img src="/2019/08/27/Prometheus-Operator/media/eeb84e617080aab07e887b6019149aa5.png"></p><h3 id="验证告警效果"><a href="#验证告警效果" class="headerlink" title="验证告警效果"></a>验证告警效果</h3><p>注意验证方法，MySQL服务自身是没有提供metrics，是依靠mysql-exporter这个服务去收集的，所以当MySQL服务无法访问的时候才能实现mysql告警，MySQL-exporter服务停止是服务停止告警，此处我需要将mysql服务的service配置删除，已达到服务无法访问的效果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl.exe --kubeconfig=./kubeconfig/sre.kubeconfig get svc</span><br><span class="line">kubectl.exe --kubeconfig=./kubeconfig/sre.kubeconfig delete svc mysql-svc</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/859214249b15ec19e6978a18afffb9cd.png"></p><p>30秒后就可以在两个控制台分别看到如下</p><p><img src="/2019/08/27/Prometheus-Operator/media/5d43c62772602e2c81ec114cb7f27e73.png"></p><p><img src="/2019/08/27/Prometheus-Operator/media/475bccb776141ea2327393e1b47eeea0.png"></p><p>企业微信客户端收到错误告警如下</p><p><img src="/2019/08/27/Prometheus-Operator/media/cb724f3a8955de683842e386b32611a3.png"></p><p>重新建立svc，验证恢复告警</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl --kubeconfig=./kubeconfig/sre.kubeconfig apply -f kube-prometheus/manifests/new/mysql-rc.yaml</span><br></pre></td></tr></table></figure><p><img src="/2019/08/27/Prometheus-Operator/media/0a9bb4b8c8f002f2129e10f3de548ac6.png"></p><p>总结：</p><ul><li>  告警信息太多，太杂乱</li><li>  错误告警和恢复告警基本内容差不多，无法区别</li><li>  需要自定义告警模板来便于快速定位</li></ul><h3 id="建立自定义告警模板"><a href="#建立自定义告警模板" class="headerlink" title="建立自定义告警模板"></a>建立自定义告警模板</h3><h4 id="alertmanager-temp-configmap-yaml"><a href="#alertmanager-temp-configmap-yaml" class="headerlink" title="alertmanager-temp-configmap.yaml"></a>alertmanager-temp-configmap.yaml</h4><p><img src="/2019/08/27/Prometheus-Operator/media/9faa737a345f90cacdad094721bb64a0.png"></p><p>上面的模板是使用的gotemplate语法编写，其中包含了错误告警和恢复告警，具体编写请参考以下链接<br><a href="https://github.com/songjiayang/prometheus_practice/issues/12">https://github.com/songjiayang/prometheus_practice/issues/12</a></p><h4 id="修改alertmanager-yaml内容-3"><a href="#修改alertmanager-yaml内容-3" class="headerlink" title="修改alertmanager.yaml内容"></a>修改alertmanager.yaml内容</h4><p>我们需要将secret中的alertmanager.yaml修改为如下的内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">  resolve_timeout: 1h</span><br><span class="line">route:</span><br><span class="line">  group_by: [&#x27;alertname&#x27;]</span><br><span class="line">  receiver: &#x27;wechat&#x27;</span><br><span class="line">  group_wait: 30s</span><br><span class="line">  group_interval: 30s</span><br><span class="line">  repeat_interval: 10m</span><br><span class="line"></span><br><span class="line">receivers:</span><br><span class="line">- name: &#x27;wechat&#x27;</span><br><span class="line">  wechat_configs:</span><br><span class="line">  - corp_id: &#x27;企业ID&#x27;</span><br><span class="line">    to_party: &#x27;组ID&#x27;</span><br><span class="line">    agent_id: &#x27;应用ID&#x27;</span><br><span class="line">    api_secret: &#x27;应用密钥&#x27;</span><br><span class="line">    #是否发送恢复信息</span><br><span class="line">    send_resolved: true</span><br><span class="line"><span class="meta">#</span><span class="bash">告警模板</span></span><br><span class="line">templates: </span><br><span class="line">- &#x27;/etc/alertmanager/configmaps/alertmanager-temp/temp.yaml&#x27;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>  templates：指定告警模板的存放位置，注意此处的路径是固定格式的/etc/alertmanager/configmaps/+configmap名称+configmap中的键名，即我在new目录下的alertmanager-temp-configmap.yaml，如下：</li></ul><p><img src="/2019/08/27/Prometheus-Operator/media/1fee93d395693c7b04f1a47da3bea884.png"></p><p><img src="/2019/08/27/Prometheus-Operator/media/dd755c752ef27bee10ea902010d5999e.png"></p><p>将上面修改好的内容进行base64加密，然后copy到alertmanager-secret.yaml中，然后在alertmanager web页面进行查看</p><p><img src="/2019/08/27/Prometheus-Operator/media/4f1cddad3f49bcfa1aa08e5381026785.png"></p><h4 id="alertmanager服务添加configmap挂载"><a href="#alertmanager服务添加configmap挂载" class="headerlink" title="alertmanager服务添加configmap挂载"></a>alertmanager服务添加configmap挂载</h4><p>alertmanager服务的建立是statefulset建立的，是依据alertmanager-alertmanager.yaml文件建立的，需要增加自定义模板的configmap挂载，并且默认是挂载到/etc/alertmanager/configmaps/下的，可参照官方介绍<a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerspec">https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerspec</a></p><p>修改如下：</p><p><img src="/2019/08/27/Prometheus-Operator/media/a47fde5295cd130d94772a0223d818cc.png"></p><p>所以需要重新建立alertmanager服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl --kubeconfig=./kubeconfig/sre.kubeconfig apply -f kube-prometheus/manifests/alertmanager-alertmanager.yaml</span><br></pre></td></tr></table></figure><p>再次验证告警效果，同样的删除mysql的svc。企业微信效果如下</p><p><img src="/2019/08/27/Prometheus-Operator/media/8d5ab49247e0cc6611fe0f1dda7ba020.png"></p><p><img src="/2019/08/27/Prometheus-Operator/media/d02e70173936e3774b30232d105fc185.png"></p><h2 id="企业微信-QQmail同时报警"><a href="#企业微信-QQmail同时报警" class="headerlink" title="企业微信/QQmail同时报警"></a>企业微信/QQmail同时报警</h2><p>配置文件如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">  resolve_timeout: 1h</span><br><span class="line">  smtp_smarthost: &#x27;smtp.qq.com:587&#x27;</span><br><span class="line">  smtp_from: &#x27;发件人&#x27;     </span><br><span class="line">  smtp_auth_username: &#x27;发件人&#x27;   </span><br><span class="line">  smtp_auth_password: &#x27;授权码&#x27;  </span><br><span class="line"></span><br><span class="line">route:</span><br><span class="line">  group_by: [&#x27;alertname&#x27;] </span><br><span class="line">  group_wait: 30s</span><br><span class="line">  group_interval: 30s</span><br><span class="line">  repeat_interval: 10m</span><br><span class="line">  receiver: &#x27;huisebug&#x27;</span><br><span class="line">  routes:</span><br><span class="line">  - receiver: &#x27;wechat&#x27;</span><br><span class="line">    continue: true</span><br><span class="line">  - receiver: &#x27;huisebug&#x27;</span><br><span class="line">    continue: true</span><br><span class="line"></span><br><span class="line">receivers:</span><br><span class="line">- name: &#x27;huisebug&#x27;  </span><br><span class="line">  email_configs:</span><br><span class="line">  - to: &#x27;收件人&#x27;</span><br><span class="line">    send_resolved: true</span><br><span class="line">- name: &#x27;wechat&#x27;</span><br><span class="line">  wechat_configs:</span><br><span class="line">  - corp_id: &#x27;&#x27;</span><br><span class="line">    to_party: &#x27;8&#x27;</span><br><span class="line">    agent_id: &#x27;1000009&#x27;</span><br><span class="line">    api_secret: &#x27;&#x27;</span><br><span class="line">    send_resolved: true</span><br><span class="line">    </span><br><span class="line">templates: </span><br><span class="line">- &#x27;/etc/alertmanager/configmaps/alertmanager-temp/temp.yaml&#x27;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;这是一篇Prometheus-Operator监控k8s服务的部署、监控原理讲解、监控配置、告警模板配置。&lt;/p&gt;</summary>
    
    
    
    <category term="Prometheus" scheme="https://huisebug.github.io/categories/Prometheus/"/>
    
    
    <category term="k8s" scheme="https://huisebug.github.io/tags/k8s/"/>
    
    <category term="prometheus" scheme="https://huisebug.github.io/tags/prometheus/"/>
    
    <category term="alertmanager" scheme="https://huisebug.github.io/tags/alertmanager/"/>
    
    <category term="grafana" scheme="https://huisebug.github.io/tags/grafana/"/>
    
    <category term="mysql-export" scheme="https://huisebug.github.io/tags/mysql-export/"/>
    
    <category term="钉钉告警" scheme="https://huisebug.github.io/tags/%E9%92%89%E9%92%89%E5%91%8A%E8%AD%A6/"/>
    
    <category term="企业微信告警" scheme="https://huisebug.github.io/tags/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E5%91%8A%E8%AD%A6/"/>
    
  </entry>
  
</feed>
